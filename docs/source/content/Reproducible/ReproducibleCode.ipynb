{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13166dc2",
   "metadata": {},
   "source": [
    "# Reproducible code for the associated paper\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Here is the reproducible Python code used to run the experiments in the paper and output the presented plots. We test the performance of various anomaly detection strategies::\n",
    "\n",
    "- LODA (Pevny 2015)\n",
    "- LODA with updated linear combination weights using active learning (Das et al. 2016)\n",
    "- GLAD (Islam et al. 2018)\n",
    "- Our AAA method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb83c8-8152-46be-ba0a-b1509da3616d",
   "metadata": {},
   "source": [
    "## All neccesary background loads\n",
    "\n",
    "Importing our __acanag__ library already imports a number of other functions/libraries. A few others to import for this reproducible codes are also below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07354a9-650e-4729-b84f-1c25eca73b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acanag import *\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81caf328-c8b6-4d5b-9632-bcfb81763cd1",
   "metadata": {},
   "source": [
    "# Two-dimensional Gaussian trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a8a99",
   "metadata": {},
   "source": [
    "## Simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ad69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of \"old\" data points we start with:\n",
    "n_old = 1000\n",
    "\n",
    "# The number of data points in each future batch:\n",
    "B = 500\n",
    "\n",
    "# The number of future batches:\n",
    "n_loops = 200\n",
    "\n",
    "# The mixture parameter:\n",
    "tau = 0.01\n",
    "\n",
    "#Minimum number of data points to use to calibrate the number of LODA projections, unless the \n",
    "#today number of data points is less than n_min. The choice between the two happens elsewhere\n",
    "#so this can be left here.\n",
    "n_min = 1000\n",
    "\n",
    "#Set an upper bound for the number of LODA projections:\n",
    "M_max = 15\n",
    "\n",
    "#Parameter set to default 0.01 in Pevny (2015). However, in small-dimensional settings (e.g., d=2) this\n",
    "#may not be a good idea?\n",
    "tau_M = 0.1\n",
    "\n",
    "#We also need to decide how many data-points to feed to the expert in each loop IN TOTAL. In\n",
    "#Das et al. (2016) they feed the top one. Here there are different algorithms, and not all of the\n",
    "#have an active learning step. What we do is define the total number of items the expert can see in\n",
    "#each pass = n_send. However, for instance with our method, not all of these will be sent because\n",
    "#they had high scores (= possible anomaly). Some of them will be kept aside to be used in an active\n",
    "#learning strategy (e.g., uncertainty sampling)\n",
    "n_send = 5\n",
    "\n",
    "#Proportion of initial data we suppose we know the true anomaly status of:\n",
    "u = 0.1\n",
    "\n",
    "#Number of simulation trials for each setting:\n",
    "n_trials = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a7d4ee-f6fc-4c60-8f75-40a4e252ce76",
   "metadata": {},
   "source": [
    "In our simulations we have the nominal data as $N((0,0),I_2)$ and the anomaly data as $N((c,c),0.1*I_2)$ for four choices of $c$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df9515-8cf0-44f6-b547-d840503bbeeb",
   "metadata": {},
   "source": [
    "## $c=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a8b29-d4d4-4a8f-8dbb-05bba138e63f",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82aae9-e960-4a1c-a9d6-970c810986e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666ec19-1076-4c48-a422-0e6549720331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Two-dimensional Gaussian nominals with one two-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "c = 2\n",
    "\n",
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    # Specific arguments:\n",
    "    a_list = [[c,c]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "\n",
    "\n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "\n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'LogisticRegression'\n",
    "\n",
    "    #Initialization\n",
    "\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "    # Run the initialization function InitActiveAGG:\n",
    "    X_lab, Y_lab, all_labeled_scores = InitActiveAGG(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models)\n",
    "\n",
    "\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 1,min_n_nom=5,min_n_anom=1,tau_exp=tau)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        \n",
    "        \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_AAA_2 = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_2 = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_2_onlyAAA.npz\",\n",
    "         avg_AAA_2 = avg_AAA_2,\n",
    "         avg_nY1_AAA_2 = avg_nY1_AAA_2,\n",
    "         Y_AUC_2_onlyAAA = Y_AUC #The AUC for the last loop\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_2_onlyAAA.npz\",\n",
    "         new_preds_2 = new_preds\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e6664-4bdd-45fa-8418-98f8b73b6899",
   "metadata": {},
   "source": [
    "### The other three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248fce6-6994-433f-a271-e80fd027647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da14ed-0528-4247-9c8f-bea96459dc09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Two-dimensional Gaussian nominals with one two-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "c = 2\n",
    "\n",
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    # Specific arguments:\n",
    "    a_list = [[c,c]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "      \n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # LODA TRIALS\n",
    "\n",
    "    print('LODA trial begins')\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "       \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # ACTIVE-LODA trials:\n",
    "\n",
    "    print('active-LODA trial begins')\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau\n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "\n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    #print(np.shape(w_old))\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "                    \n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "            \n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "\n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "\n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "\n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "                \n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "\n",
    "        nY0 = Y_lab.count(0)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        #print('q tau:',q_tau)\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # GLAD trials:\n",
    "\n",
    "    print('GLAD trial begins')\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_old)\n",
    "                y_score = model.score_samples(X_old)\n",
    "                y_score.dtype = np.float64\n",
    "                all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "    \n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "\n",
    "\n",
    "        \n",
    "        #print('top k indices:',top_k_indices)\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]\n",
    "\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "\n",
    "        #model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss(X_lab, Y_lab, q_tau_tm1, all_labeled_scores,model_GLAD, X_so_far, mylambda, b), metrics=['accuracy'])\n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        #model_GLAD.fit(X_lab, np.array(Y_lab), epochs=100, batch_size=32,verbose=0)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "        \n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "\n",
    "    \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "\n",
    "# Calculate column averages for each array\n",
    "avg_LODA_2 = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_2 = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_2 = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_2 = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_2 = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_2 = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_2.npz\",\n",
    "         avg_LODA_2 = avg_LODA_2,\n",
    "         avg_active_LODA_2 = avg_active_LODA_2,\n",
    "         avg_GLAD_2 = avg_GLAD_2,\n",
    "         avg_nY1_LODA_2 = avg_nY1_LODA_2,\n",
    "         avg_nY1_active_LODA_2 = avg_nY1_active_LODA_2,\n",
    "         avg_nY1_GLAD_2 = avg_nY1_GLAD_2,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_2.npz\",\n",
    "         weighted_scores_2 = weighted_scores,\n",
    "         weighted_validation_scores_2 = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_2 = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d12f0-ad91-48fe-90bc-5106d3e7d986",
   "metadata": {},
   "source": [
    "### Load the AUC and anomaly count data, and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905d9fb-4f39-4694-abd6-bfaeaf78c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_2_onlyAAA = np.load(\"AUC_2_onlyAAA.npz\")\n",
    "avg_AAA_2 = data_2_onlyAAA[\"avg_AAA_2\"]\n",
    "avg_nY1_AAA_2 = data_2_onlyAAA[\"avg_nY1_AAA_2\"]  \n",
    "Y_AUC_2 = data_2_onlyAAA[\"Y_AUC_2_onlyAAA\"] \n",
    "\n",
    "# The others\n",
    "data_2 = np.load(\"AUC_2.npz\")\n",
    "avg_LODA_2 = data_2[\"avg_LODA_2\"]\n",
    "avg_active_LODA_2 = data_2[\"avg_active_LODA_2\"]\n",
    "avg_GLAD_2 = data_2[\"avg_GLAD_2\"]\n",
    "avg_nY1_LODA_2 = data_2[\"avg_nY1_LODA_2\"]\n",
    "avg_nY1_active_LODA_2 = data_2[\"avg_nY1_active_LODA_2\"]\n",
    "avg_nY1_GLAD_2 = data_2[\"avg_nY1_GLAD_2\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e7fad-63c4-429f-bdf1-1029356a7068",
   "metadata": {},
   "source": [
    "### Plot the AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e4d5b-02c2-4e98-b9d3-b1c619f89749",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_2, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_2, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_2, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_2, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9adf706-8cc3-494b-915f-caa5e7885e4a",
   "metadata": {},
   "source": [
    "### Plot the cumulative number of anomalies detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2efb413-4e39-46c7-8231-dc09b13c2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_2, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_2, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_2, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_2, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83252462-10a5-4990-a707-00d25545609a",
   "metadata": {},
   "source": [
    "### Plot the score densities for each method after the final batch\n",
    "\n",
    "Here we want to see the effect of each method on the score density after running through all of the batches.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc87f31-f6fa-476c-8012-d53f7c554ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "scores_2_onlyAAA = np.load(\"scores_2_onlyAAA.npz\")\n",
    "new_preds_2 = scores_2_onlyAAA[\"new_preds_2\"]\n",
    "\n",
    "# The others\n",
    "scores_2 = np.load(\"scores_2.npz\")\n",
    "weighted_scores_2 = scores_2[\"weighted_scores_2\"]\n",
    "weighted_validation_scores_2 = scores_2[\"weighted_validation_scores_2\"]\n",
    "X_new_final_scores_ext_2 = scores_2[\"X_new_final_scores_ext_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0346d-c96b-495f-9ddc-c1d8e8b747a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_arrays = [weighted_scores_2, weighted_validation_scores_2, X_new_final_scores_ext_2, new_preds_2]\n",
    "titles = [\"LODA\", \"Active-LODA\", \"GLAD\", \"AAA\"]\n",
    "\n",
    "# Set font sizes\n",
    "title_fontsize = 16  \n",
    "label_fontsize = 13  \n",
    "legend_fontsize = 14 \n",
    "tick_fontsize = 12\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))  # 2x2 grid\n",
    "axes = axes.flatten()  # Flatten for easy iteration\n",
    "\n",
    "for i, scores in enumerate(score_arrays):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Define x-axis range\n",
    "    x_min, x_max = np.min(scores), np.max(scores)\n",
    "    x_vals = np.linspace(x_min, x_max, 200)\n",
    "\n",
    "    # Split into nominal and anomaly\n",
    "    nominal_scores = scores[Y_AUC_2 == 0]\n",
    "    anomaly_scores = scores[Y_AUC_2 == 1]\n",
    "\n",
    "    # KDE with proper scaling\n",
    "    if len(nominal_scores) > 1:\n",
    "        kde_nominal = gaussian_kde(nominal_scores)\n",
    "        nominal_density = kde_nominal(x_vals) * (1 - tau)\n",
    "        ax.plot(x_vals, nominal_density, label=\"Nominal (Y=0)\", color=\"blue\")\n",
    "\n",
    "    if len(anomaly_scores) > 1:\n",
    "        kde_anomaly = gaussian_kde(anomaly_scores)\n",
    "        anomaly_density = 10 * kde_anomaly(x_vals) * tau\n",
    "        ax.plot(x_vals, anomaly_density, label=\"Anomaly (Y=1)\", color=\"red\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"{titles[i]} (c=2)\", fontsize=title_fontsize)\n",
    "    ax.set_xlabel(\"Score\", fontsize=label_fontsize)\n",
    "    ax.set_ylabel(\"Aggregated score 'density'\", fontsize=label_fontsize)\n",
    "    ax.legend(fontsize=legend_fontsize)\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='both', labelsize=tick_fontsize)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_ylim(0, 1.55)\n",
    "    if i == 1:\n",
    "        ax.set_ylim(0, 0.48)\n",
    "    if i == 2:\n",
    "        ax.set_ylim(0, 0.17)\n",
    "    if i == 3:\n",
    "        ax.set_ylim(0, 16.5)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"ScoreDensities_2.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ba86c-fd6b-4548-9b8e-84ecbad24724",
   "metadata": {},
   "source": [
    "## $c = 1.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a99089-05d5-4324-ac44-2b51c893735b",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96bc8f-f982-40d9-a743-4e65920f2ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82e445-c2a3-4d1a-8d30-73c6597de609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Two-dimensional Gaussian nominals with one two-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "#Choose 0.5, 1, 1.5 or 2 for the centre of the anomaly distribution\n",
    "c = 1.5\n",
    "\n",
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    # Specific arguments:\n",
    "    a_list = [[c,c]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "\n",
    "    #####################################################################################################################\n",
    "    # Our trials: \n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'LogisticRegression'\n",
    "\n",
    "    #Initialization\n",
    "\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "    # Run the initialization function InitActiveAGG:\n",
    "    X_lab, Y_lab, all_labeled_scores = InitActiveAGG(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models)\n",
    "\n",
    "\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.6,min_n_nom=5,min_n_anom=1,tau_exp=tau)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        #new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:, 1]\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        \n",
    "        \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "\n",
    "# Calculate column averages for each array\n",
    "avg_AAA_1p5 = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_1p5 = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_1p5_onlyAAA.npz\",\n",
    "         avg_AAA_1p5 = avg_AAA_1p5,\n",
    "         avg_nY1_AAA_1p5 = avg_nY1_AAA_1p5,\n",
    "        )\n",
    "\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_1p5_onlyAAA.npz\",\n",
    "         new_preds_1p5 = new_preds\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f4327-3574-47da-a76f-7a271376936c",
   "metadata": {},
   "source": [
    "### The other three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82298c8-2eb2-4975-aa36-279729dd3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1befdebb-224b-4f6e-be78-88214f4e5556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Two-dimensional Gaussian nominals with one two-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "#Choose 0.5, 1, 1.5 or 2 for the centre of the anomaly distribution\n",
    "c = 1.5\n",
    "\n",
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    # Specific arguments:\n",
    "    a_list = [[c,c]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # LODA TRIALS\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # ACTIVE-LODA trials:\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau\n",
    "    \n",
    "    #There are hyperparameters that need to be set in advance for this algorithm. However, \n",
    "    #for simplicity we assume they tal the default values in the function optimize_w.\n",
    "    #C_A = 100  #default in their article\n",
    "    #C_eta = 1000. #default in their article\n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "    \n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "                    \n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "    \n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        #We actually have to, at this point, attach the current versions of H_A\n",
    "        #and H_N to new_unweighted_scores, since in this batch framework, we do\n",
    "        #not have a fixed number of data points from the start to the finish, like\n",
    "        #they do in Das et al. (2016). If we do not do this, it will affect the\n",
    "        #calculation of q_tau over time (a kind of bias will be introduced, maybe\n",
    "        #not the end of the world, but still.)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "\n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "    \n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "                \n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "                \n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # GLAD trials:\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_old)\n",
    "                y_score = model.score_samples(X_old)\n",
    "                y_score.dtype = np.float64\n",
    "                all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "    \n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]\n",
    "\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "        \n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "\n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "        \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_LODA_1p5 = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_1p5 = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_1p5 = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_1p5 = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_1p5 = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_1p5 = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_1p5.npz\",\n",
    "         avg_LODA_1p5 = avg_LODA_1p5,\n",
    "         avg_active_LODA_1p5 = avg_active_LODA_1p5,\n",
    "         avg_GLAD_1p5 = avg_GLAD_1p5,\n",
    "         avg_nY1_LODA_1p5 = avg_nY1_LODA_1p5,\n",
    "         avg_nY1_active_LODA_1p5 = avg_nY1_active_LODA_1p5,\n",
    "         avg_nY1_GLAD_1p5 = avg_nY1_GLAD_1p5,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_1p5.npz\",\n",
    "         weighted_scores_1p5 = weighted_scores,\n",
    "         weighted_validation_scores_1p5 = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_1p5 = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dafdabe-6966-4cbe-8074-baa74319b660",
   "metadata": {},
   "source": [
    "### Load AUC and anomaly count data and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabdaa29-e9c6-4b44-9417-c4c9daed9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_1p5_onlyAAA = np.load(\"AUC_1p5_onlyAAA.npz\")\n",
    "avg_AAA_1p5 = data_1p5_onlyAAA[\"avg_AAA_1p5\"]\n",
    "avg_nY1_AAA_1p5 = data_1p5_onlyAAA[\"avg_nY1_AAA_1p5\"]\n",
    "\n",
    "\n",
    "# The others\n",
    "data_1p5 = np.load(\"AUC_1p5.npz\")\n",
    "avg_LODA_1p5 = data_1p5[\"avg_LODA_1p5\"]\n",
    "avg_active_LODA_1p5 = data_1p5[\"avg_active_LODA_1p5\"]\n",
    "avg_GLAD_1p5 = data_1p5[\"avg_GLAD_1p5\"]\n",
    "avg_nY1_LODA_1p5 = data_1p5[\"avg_nY1_LODA_1p5\"]\n",
    "avg_nY1_active_LODA_1p5 = data_1p5[\"avg_nY1_active_LODA_1p5\"]\n",
    "avg_nY1_GLAD_1p5 = data_1p5[\"avg_nY1_GLAD_1p5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10cb2b2-1ae9-4984-b14d-d6d53285b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_1p5, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_1p5, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_1p5, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_1p5, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963dd684-2ef5-4767-9ff3-c274f0af88ed",
   "metadata": {},
   "source": [
    "### Plot of cumulative number of anomalies detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888616d5-60e8-40ab-90ed-1efec062f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_1p5, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_1p5, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_1p5, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_1p5, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a83e3-c6b0-4843-bdee-dcf3d10b12a5",
   "metadata": {},
   "source": [
    "### Cumulative percentage of queries which are true anomalies over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38afd45c-ac8d-47ae-ae2b-3c77a7faba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold the new transformed values\n",
    "transformed_values = []\n",
    "\n",
    "# Loop over the values in avg_nY1_AAA_1p5 and apply the transformation\n",
    "for i in range(n_loops):\n",
    "    transformed_value = avg_nY1_AAA_1p5[i] / (100 + 5 * i)\n",
    "    transformed_values.append(transformed_value)\n",
    "\n",
    "# Define the x-axis (batches/loops)\n",
    "xx = list(range(1, n_loops + 1))\n",
    "\n",
    "# Plot the original values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the transformed values\n",
    "plt.plot(xx, transformed_values, marker='o', linestyle='-', color='red')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Fraction of labeled data that is an anomaly\", fontsize=14)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15dd330-63d5-4ea6-9502-22a6feb42125",
   "metadata": {},
   "source": [
    "### Fitting functions to this curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e2ecd-a835-47e6-92e3-5f94fb14d793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Your existing code for transformation\n",
    "transformed_values = []\n",
    "\n",
    "for i in range(n_loops):\n",
    "    transformed_value = avg_nY1_AAA_1p5[i] / (100 + 5 * i)\n",
    "    transformed_values.append(transformed_value)\n",
    "\n",
    "xx = list(range(1, n_loops + 1))\n",
    "\n",
    "# Logarithmic fit function\n",
    "def log_func(x, a, b):\n",
    "    return a * np.log(x) + b\n",
    "\n",
    "# Hyperbolic fit function\n",
    "def hyperbola_func(x, a, b, c):\n",
    "    return a / (x + b) + c\n",
    "\n",
    "# Fit the transformed values using the logarithmic function\n",
    "popt_log, _ = curve_fit(log_func, xx, transformed_values, maxfev=10000)\n",
    "\n",
    "# Fit the transformed values using the hyperbolic function\n",
    "popt_hyper, _ = curve_fit(hyperbola_func, xx, transformed_values, maxfev=10000)\n",
    "\n",
    "# Generate fitted curves for plotting\n",
    "log_fit_values = log_func(np.array(xx), *popt_log)\n",
    "hyper_fit_values = hyperbola_func(np.array(xx), *popt_hyper)\n",
    "\n",
    "# Plotting the transformed values and fitted curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the transformed values\n",
    "plt.plot(xx, transformed_values, marker='o', linestyle='-', color='red', label=\"Transformed Values\")\n",
    "\n",
    "# Plot the logarithmic fit\n",
    "plt.plot(xx, log_fit_values, linestyle='--', color='blue', label=f\"Logarithmic Fit: y = {popt_log[0]:.2f}*ln(x) + {popt_log[1]:.2f}\")\n",
    "\n",
    "# Plot the hyperbolic fit\n",
    "plt.plot(xx, hyper_fit_values, linestyle='--', color='green', label=f\"Hyperbolic Fit: y = {popt_hyper[0]:.2f}/(x + {popt_hyper[1]:.2f}) + {popt_hyper[2]:.2f}\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Fraction of labeled data that is an anomaly\", fontsize=14)\n",
    "plt.title(\"Fitting Transformed Values with Logarithmic and Hyperbolic Functions\", fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e3785d-05f1-4fd0-beb2-0c32f5912f89",
   "metadata": {},
   "source": [
    "## $c=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf36de4-36f6-4436-a646-eb637a6c9e07",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfd6be-8f25-41bb-b362-2341290735dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a69e1-6a36-421c-9b5c-2a4da6096115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Two-dimensional Gaussian nominals with one two-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "c = 1\n",
    "\n",
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    # Specific arguments:\n",
    "    a_list = [[c,c]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # Our trials: \n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'LogisticRegression'\n",
    "\n",
    "    #Initialization\n",
    "\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "    # Run the initialization function InitActiveAGG:\n",
    "    X_lab, Y_lab, all_labeled_scores = InitActiveAGG(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models)\n",
    "\n",
    "\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 1,min_n_nom=5,min_n_anom=1,tau_exp=tau)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        #new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:, 1]\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        \n",
    "        \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "\n",
    "# Calculate column averages for each array\n",
    "avg_AAA_1 = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_1 = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_1_onlyAAA.npz\",\n",
    "         avg_AAA_1 = avg_AAA_1,\n",
    "         avg_nY1_AAA_1 = avg_nY1_AAA_1, \n",
    "         Y_AUC_1_onlyAAA = Y_AUC #The AUC for the last loop\n",
    "        )\n",
    "\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_1_onlyAAA.npz\",\n",
    "         new_preds_1 = new_preds\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54118db2-6d44-4c84-87da-882b70cdbc80",
   "metadata": {},
   "source": [
    "### The other three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e80baa-e644-49af-a2e3-a90003aaac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798a2c5-1ca6-48eb-aa50-4247bb18dcc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Two-dimensional Gaussian nominals with one two-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "c = 1\n",
    "\n",
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    # Specific arguments:\n",
    "    a_list = [[c,c]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # LODA TRIALS\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # ACTIVE-LODA trials:\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau\n",
    "    \n",
    "    #There are hyperparameters that need to be set in advance for this algorithm. However, \n",
    "    #for simplicity we assume they tal the default values in the function optimize_w.\n",
    "    #C_A = 100  #default in their article\n",
    "    #C_eta = 1000. #default in their article\n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "    \n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            print('There were no labeled anomalies in the old data')\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "\n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "    \n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        #We actually have to, at this point, attach the current versions of H_A\n",
    "        #and H_N to new_unweighted_scores, since in this batch framework, we do\n",
    "        #not have a fixed number of data points from the start to the finish, like\n",
    "        #they do in Das et al. (2016). If we do not do this, it will affect the\n",
    "        #calculation of q_tau over time (a kind of bias will be introduced, maybe\n",
    "        #not the end of the world, but still.)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "\n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "        \n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "    \n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            #print('next top index:',next_top_index)\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "                \n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "                \n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # GLAD trials:\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_old)\n",
    "                y_score = model.score_samples(X_old)\n",
    "                y_score.dtype = np.float64\n",
    "                all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "    \n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]\n",
    "\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "        \n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "\n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "        \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_LODA_1 = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_1 = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_1 = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_1 = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_1 = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_1 = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_1.npz\",\n",
    "         avg_LODA_1 = avg_LODA_1,\n",
    "         avg_active_LODA_1 = avg_active_LODA_1,\n",
    "         avg_GLAD_1 = avg_GLAD_1,\n",
    "         avg_nY1_LODA_1 = avg_nY1_LODA_1,\n",
    "         avg_nY1_active_LODA_1 = avg_nY1_active_LODA_1,\n",
    "         avg_nY1_GLAD_1 = avg_nY1_GLAD_1,     \n",
    "        )\n",
    "\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_1.npz\",\n",
    "         weighted_scores_1 = weighted_scores,\n",
    "         weighted_validation_scores_1 = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_1 = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2d67b-425f-4b07-ae26-25831533b399",
   "metadata": {},
   "source": [
    "### Load the AUC and anomaly count data and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f832735-15fb-4812-ad0c-0a6b801552cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_1_onlyAAA = np.load(\"AUC_1_onlyAAA.npz\")\n",
    "avg_AAA_1 = data_1_onlyAAA[\"avg_AAA_1\"]\n",
    "avg_nY1_AAA_1 = data_1_onlyAAA[\"avg_nY1_AAA_1\"]\n",
    "Y_AUC_1 = data_1_onlyAAA[\"Y_AUC_1_onlyAAA\"] \n",
    "\n",
    "\n",
    "# The others\n",
    "data_1 = np.load(\"AUC_1.npz\")\n",
    "avg_LODA_1 = data_1[\"avg_LODA_1\"]\n",
    "avg_active_LODA_1 = data_1[\"avg_active_LODA_1\"]\n",
    "avg_GLAD_1 = data_1[\"avg_GLAD_1\"]\n",
    "avg_nY1_LODA_1 = data_1[\"avg_nY1_LODA_1\"]\n",
    "avg_nY1_active_LODA_1 = data_1[\"avg_nY1_active_LODA_1\"]\n",
    "avg_nY1_GLAD_1 = data_1[\"avg_nY1_GLAD_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879ff5d-5d5e-44f7-9723-4017258ea50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_1, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_1, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_1, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_1, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d04eec4-0693-418f-9237-176337458c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_1, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_1, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_1, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_1, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308f077-3a4d-4c3e-abac-9aa03a5ce88c",
   "metadata": {},
   "source": [
    "### Plot the densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a7e23-6082-4855-acf9-ea67fe9d0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "scores_1_onlyAAA = np.load(\"scores_1_onlyAAA.npz\")\n",
    "new_preds_1 = scores_1_onlyAAA[\"new_preds_1\"]\n",
    "\n",
    "# The others\n",
    "scores_1 = np.load(\"scores_1.npz\")\n",
    "weighted_scores_1 = scores_1[\"weighted_scores_1\"]\n",
    "weighted_validation_scores_1 = scores_1[\"weighted_validation_scores_1\"]\n",
    "X_new_final_scores_ext_1 = scores_1[\"X_new_final_scores_ext_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b90664-ce2d-4cc1-892c-e90be22ca811",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_arrays = [weighted_scores_1, weighted_validation_scores_1, X_new_final_scores_ext_1, new_preds_1]\n",
    "titles = [\"LODA\", \"Active-LODA\", \"GLAD\", \"AAA\"]\n",
    "\n",
    "# Set font sizes\n",
    "title_fontsize = 16\n",
    "label_fontsize = 13\n",
    "legend_fontsize = 14\n",
    "tick_fontsize = 10\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))  # 2x2 grid\n",
    "axes = axes.flatten()  # Flatten for easy iteration\n",
    "\n",
    "for i, scores in enumerate(score_arrays):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Define x-axis range\n",
    "    x_min, x_max = np.min(scores), np.max(scores)\n",
    "    x_vals = np.linspace(x_min, x_max, 200)\n",
    "\n",
    "    # Split into nominal and anomaly\n",
    "    nominal_scores = scores[Y_AUC_1 == 0]\n",
    "    anomaly_scores = scores[Y_AUC_1 == 1]\n",
    "\n",
    "    # KDE with proper scaling\n",
    "    if len(nominal_scores) > 1:\n",
    "        kde_nominal = gaussian_kde(nominal_scores)\n",
    "        nominal_density = kde_nominal(x_vals) * (1 - tau)\n",
    "        ax.plot(x_vals, nominal_density, label=\"Nominal (Y=0)\", color=\"blue\")\n",
    "\n",
    "    if len(anomaly_scores) > 1:\n",
    "        kde_anomaly = gaussian_kde(anomaly_scores)\n",
    "        anomaly_density = 10 * kde_anomaly(x_vals) * tau\n",
    "        ax.plot(x_vals, anomaly_density, label=\"Anomaly (Y=1)\", color=\"red\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"{titles[i]} (c=1)\", fontsize=title_fontsize)\n",
    "    ax.set_xlabel(\"Score\", fontsize=label_fontsize)\n",
    "    ax.set_ylabel(\"Aggregated score 'density'\", fontsize=label_fontsize)\n",
    "    ax.legend(fontsize=legend_fontsize)\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='both', labelsize=tick_fontsize)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_ylim(0, 1.55)\n",
    "    if i == 1:\n",
    "        ax.set_ylim(0, 1.1)\n",
    "    if i == 2:\n",
    "        ax.set_ylim(0, 61000)\n",
    "    if i == 3:\n",
    "        ax.set_ylim(0, 11)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"ScoreDensities_1.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18573efb-7550-4676-b0e8-6f6a2ec13a71",
   "metadata": {},
   "source": [
    "## $c=0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4556944-6782-46aa-92d9-8f367c43b0cc",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2099f7-f0ed-4e4a-9714-352515c55eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661e5d3-552b-40c8-b5fd-72a90a7d5093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Two-dimensional Gaussian nominals with one two-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "c = 0.5\n",
    "\n",
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    # Specific arguments:\n",
    "    a_list = [[c,c]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        #print(i+1,'out of',len(models))\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # Our trials: \n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'LogisticRegression'\n",
    "\n",
    "    #Initialization\n",
    "\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "    # Run the initialization function InitActiveAGG:\n",
    "    X_lab, Y_lab, all_labeled_scores = InitActiveAGG(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models)\n",
    "\n",
    "\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.4,min_n_nom=5,min_n_anom=1,tau_exp=tau)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        \n",
    "        \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "# Calculate column averages for each array\n",
    "avg_AAA_0p5 = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_0p5 = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_0p5_onlyAAA.npz\",\n",
    "         avg_AAA_0p5 = avg_AAA_0p5,\n",
    "         avg_nY1_AAA_0p5 = avg_nY1_AAA_0p5,\n",
    "         )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_0p5_onlyAAA.npz\",\n",
    "         new_preds_0p5 = new_preds,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d874ea-e327-45c2-8864-8276a46e4d00",
   "metadata": {},
   "source": [
    "### The other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce5cf3-d463-4067-96ed-0e80a0763531",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6cde2-945f-4b10-a183-95b54adf1f28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Two-dimensional Gaussian nominals with one two-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "c = 0.5\n",
    "\n",
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    # Specific arguments:\n",
    "    a_list = [[c,c]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "            #print('The data points corresponding to unmuted anomalies are:',X_with_anomalies)\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # LODA TRIALS\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # ACTIVE-LODA trials:\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau\n",
    "    \n",
    "    #There are hyperparameters that need to be set in advance for this algorithm. However, \n",
    "    #for simplicity we assume they tal the default values in the function optimize_w.\n",
    "    #C_A = 100  #default in their article\n",
    "    #C_eta = 1000. #default in their article\n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "\n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "                    \n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "    \n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "\n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "\n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "        #print('sorted indices:',sorted_indices)\n",
    "        #print('sorted scores:',sorted_scores)\n",
    "    \n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        #print(my_quantile_sorted_index)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            #print('next top index:',next_top_index)\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "                \n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "                \n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # GLAD trials:\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_old)\n",
    "                y_score = model.score_samples(X_old)\n",
    "                y_score.dtype = np.float64\n",
    "                all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "    \n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]\n",
    "\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "\n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "        \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_LODA_0p5 = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_0p5 = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_0p5 = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_0p5 = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_0p5 = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_0p5 = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_0p5.npz\",\n",
    "         avg_LODA_0p5 = avg_LODA_0p5,\n",
    "         avg_active_LODA_0p5 = avg_active_LODA_0p5,\n",
    "         avg_GLAD_0p5 = avg_GLAD_0p5,\n",
    "         avg_nY1_LODA_0p5 = avg_nY1_LODA_0p5,\n",
    "         avg_nY1_active_LODA_0p5 = avg_nY1_active_LODA_0p5,\n",
    "         avg_nY1_GLAD_0p5 = avg_nY1_GLAD_0p5,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_0p5.npz\",\n",
    "         weighted_scores_0p5 = weighted_scores,\n",
    "         weighted_validation_scores_0p5 = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_0p5 = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77fbaa-70c6-4fb8-acc0-52a837463bf8",
   "metadata": {},
   "source": [
    "### Load the AUC and anomaly count data and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2f351-bf0a-4b44-ac36-c3fa1a515e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_0p5_onlyAAA = np.load(\"AUC_0p5_onlyAAA.npz\")\n",
    "avg_AAA_0p5 = data_0p5_onlyAAA[\"avg_AAA_0p5\"]\n",
    "avg_nY1_AAA_0p5 = data_0p5_onlyAAA[\"avg_nY1_AAA_0p5\"]\n",
    "\n",
    "\n",
    "# The others\n",
    "data_0p5 = np.load(\"AUC_0p5.npz\")\n",
    "avg_LODA_0p5 = data_0p5[\"avg_LODA_0p5\"]\n",
    "avg_active_LODA_0p5 = data_0p5[\"avg_active_LODA_0p5\"]\n",
    "avg_GLAD_0p5 = data_0p5[\"avg_GLAD_0p5\"]\n",
    "avg_nY1_LODA_0p5 = data_0p5[\"avg_nY1_LODA_0p5\"]\n",
    "avg_nY1_active_LODA_0p5 = data_0p5[\"avg_nY1_active_LODA_0p5\"]\n",
    "avg_nY1_GLAD_0p5 = data_0p5[\"avg_nY1_GLAD_0p5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf791664-33b3-45cb-b543-ca90be46f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_0p5, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_0p5, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_0p5, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_0p5, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0c96a-216c-410e-99ef-8e1b462e477e",
   "metadata": {},
   "source": [
    "### Plot cumulative number of anomalies detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe82bf9e-6a64-4b23-9b5d-0783b30a0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_0p5, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_0p5, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_0p5, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_0p5, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edfd86-03b0-4f72-8753-dd5f01e3b105",
   "metadata": {},
   "source": [
    "## Bring back saved variables and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163400f-e02c-40aa-8ba9-1966041a9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2\n",
    "# Ours\n",
    "data_2_onlyAAA = np.load(\"AUC_2_onlyAAA.npz\")\n",
    "avg_AAA_2 = data_2_onlyAAA[\"avg_AAA_2\"]\n",
    "avg_nY1_AAA_2 = data_2_onlyAAA[\"avg_nY1_AAA_2\"]  \n",
    "# The others\n",
    "data_2 = np.load(\"AUC_2.npz\")\n",
    "avg_LODA_2 = data_2[\"avg_LODA_2\"]\n",
    "avg_active_LODA_2 = data_2[\"avg_active_LODA_2\"]\n",
    "avg_GLAD_2 = data_2[\"avg_GLAD_2\"]\n",
    "avg_nY1_LODA_2 = data_2[\"avg_nY1_LODA_2\"]\n",
    "avg_nY1_active_LODA_2 = data_2[\"avg_nY1_active_LODA_2\"]\n",
    "avg_nY1_GLAD_2 = data_2[\"avg_nY1_GLAD_2\"]\n",
    "\n",
    "### 1p5\n",
    "# Ours\n",
    "data_1p5_onlyAAA = np.load(\"AUC_1p5_onlyAAA.npz\")\n",
    "avg_AAA_1p5 = data_1p5_onlyAAA[\"avg_AAA_1p5\"]\n",
    "avg_nY1_AAA_1p5 = data_1p5_onlyAAA[\"avg_nY1_AAA_1p5\"]\n",
    "# The others\n",
    "data_1p5 = np.load(\"AUC_1p5.npz\")\n",
    "avg_LODA_1p5 = data_1p5[\"avg_LODA_1p5\"]\n",
    "avg_active_LODA_1p5 = data_1p5[\"avg_active_LODA_1p5\"]\n",
    "avg_GLAD_1p5 = data_1p5[\"avg_GLAD_1p5\"]\n",
    "avg_nY1_LODA_1p5 = data_1p5[\"avg_nY1_LODA_1p5\"]\n",
    "avg_nY1_active_LODA_1p5 = data_1p5[\"avg_nY1_active_LODA_1p5\"]\n",
    "avg_nY1_GLAD_1p5 = data_1p5[\"avg_nY1_GLAD_1p5\"]\n",
    "\n",
    "### 1\n",
    "# Ours\n",
    "data_1_onlyAAA = np.load(\"AUC_1_onlyAAA.npz\")\n",
    "avg_AAA_1 = data_1_onlyAAA[\"avg_AAA_1\"]\n",
    "avg_nY1_AAA_1 = data_1_onlyAAA[\"avg_nY1_AAA_1\"]\n",
    "# The others\n",
    "data_1 = np.load(\"AUC_1.npz\")\n",
    "avg_LODA_1 = data_1[\"avg_LODA_1\"]\n",
    "avg_active_LODA_1 = data_1[\"avg_active_LODA_1\"]\n",
    "avg_GLAD_1 = data_1[\"avg_GLAD_1\"]\n",
    "avg_nY1_LODA_1 = data_1[\"avg_nY1_LODA_1\"]\n",
    "avg_nY1_active_LODA_1 = data_1[\"avg_nY1_active_LODA_1\"]\n",
    "avg_nY1_GLAD_1 = data_1[\"avg_nY1_GLAD_1\"]\n",
    "\n",
    "### Op5\n",
    "# Ours\n",
    "data_0p5_onlyAAA = np.load(\"AUC_0p5_onlyAAA.npz\")\n",
    "avg_AAA_0p5 = data_0p5_onlyAAA[\"avg_AAA_0p5\"]\n",
    "avg_nY1_AAA_0p5 = data_0p5_onlyAAA[\"avg_nY1_AAA_0p5\"]\n",
    "# The others\n",
    "data_0p5 = np.load(\"AUC_0p5.npz\")\n",
    "avg_LODA_0p5 = data_0p5[\"avg_LODA_0p5\"]\n",
    "avg_active_LODA_0p5 = data_0p5[\"avg_active_LODA_0p5\"]\n",
    "avg_GLAD_0p5 = data_0p5[\"avg_GLAD_0p5\"]\n",
    "avg_nY1_LODA_0p5 = data_0p5[\"avg_nY1_LODA_0p5\"]\n",
    "avg_nY1_active_LODA_0p5 = data_0p5[\"avg_nY1_active_LODA_0p5\"]\n",
    "avg_nY1_GLAD_0p5 = data_0p5[\"avg_nY1_GLAD_0p5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220aeded-cf9f-41d3-9713-9fe64ac0fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of c\n",
    "c_values = [0.5, 1, 1.5, 2]\n",
    "\n",
    "# Create the subplot grid (4 rows, 3 columns)\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 16))  # wider figure for 3 columns\n",
    "axes = axes.reshape(4, 3)  # for easier row-wise access\n",
    "\n",
    "# Iterate over the c values and plot each row\n",
    "for i, c in enumerate(c_values):\n",
    "    # === COLUMN 1: Density Plots ===\n",
    "    a_list = [[c, c]]\n",
    "    anomaly_cov_list = [[[0.1, 0], [0, 0.1]]]\n",
    "    nominal_mean = np.array([0, 0])\n",
    "    nominal_cov = np.array([[1, 0], [0, 1]])\n",
    "    L = len(nominal_mean)\n",
    "\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians,\n",
    "        n_old=n_old,\n",
    "        B=B,\n",
    "        n_loops=n_loops,\n",
    "        tau=tau,\n",
    "        a_list=a_list,\n",
    "        anomaly_cov_list=anomaly_cov_list,\n",
    "        nominal_mean=nominal_mean,\n",
    "        nominal_cov=nominal_cov,\n",
    "        L=L\n",
    "    )\n",
    "\n",
    "    ax1 = axes[i, 0]\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6, ax=ax1)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6, ax=ax1)\n",
    "    ax1.set_title(f\"Density plot (c = {c})\", fontsize=17)\n",
    "    ax1.set_xlabel(\"X-axis\", fontsize=15)\n",
    "    ax1.set_ylabel(\"Y-axis\", fontsize=15)\n",
    "    ax1.grid(True)\n",
    "    ax1.legend(handles=[\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ], loc='upper right', fontsize=13)\n",
    "\n",
    "    # === COLUMN 2: AUC Plots ===\n",
    "\n",
    "    xx = list(range(1, n_loops + 1))\n",
    "    xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "    \n",
    "    ax2 = axes[i, 1]\n",
    "    avg_LODA_plot, avg_active_LODA_plot, avg_GLAD_plot, avg_AAA_plot = [\n",
    "        (avg_LODA_0p5, avg_active_LODA_0p5, avg_GLAD_0p5, avg_AAA_0p5),\n",
    "        (avg_LODA_1, avg_active_LODA_1, avg_GLAD_1, avg_AAA_1),\n",
    "        (avg_LODA_1p5, avg_active_LODA_1p5, avg_GLAD_1p5, avg_AAA_1p5),\n",
    "        (avg_LODA_2, avg_active_LODA_2, avg_GLAD_2, avg_AAA_2)\n",
    "    ][i]\n",
    "\n",
    "    ax2.plot(xx, avg_LODA_plot, label=\"LODA\", linewidth=1.5)\n",
    "    ax2.plot(xxactiveLODA, avg_active_LODA_plot, label=\"Active-LODA\", linewidth=1.5)\n",
    "    ax2.plot(xx, avg_GLAD_plot, label=\"GLAD\", linewidth=1.5)\n",
    "    ax2.plot(xx, avg_AAA_plot, label=\"AAA\", linewidth=1.5)\n",
    "    ax2.set_xlabel(\"Batch\", fontsize=15)\n",
    "    ax2.set_ylabel(\"Average AUC\", fontsize=15)\n",
    "    #ax2.set_xticks(range(5, n_loops + 1, 5))\n",
    "    ax2.grid(True)\n",
    "    ax2.legend(loc=\"best\", fontsize=13)\n",
    "    ax2.set_title(\"AUC over time\", fontsize=17)\n",
    "\n",
    "    # === COLUMN 3: Cumulative Anomalies Detected ===\n",
    "    ax3 = axes[i, 2]\n",
    "    avg_nY1_LODA_plot, avg_nY1_active_LODA_plot, avg_nY1_GLAD_plot, avg_nY1_AAA_plot = [\n",
    "        (avg_nY1_LODA_0p5, avg_nY1_active_LODA_0p5, avg_nY1_GLAD_0p5, avg_nY1_AAA_0p5),\n",
    "        (avg_nY1_LODA_1, avg_nY1_active_LODA_1, avg_nY1_GLAD_1, avg_nY1_AAA_1),\n",
    "        (avg_nY1_LODA_1p5, avg_nY1_active_LODA_1p5, avg_nY1_GLAD_1p5, avg_nY1_AAA_1p5),\n",
    "        (avg_nY1_LODA_2, avg_nY1_active_LODA_2, avg_nY1_GLAD_2, avg_nY1_AAA_2)\n",
    "    ][i]\n",
    "\n",
    "    ax3.plot(xx, avg_nY1_LODA_plot, label=\"LODA\", linewidth=2, linestyle='-')\n",
    "    ax3.plot(xxactiveLODA, avg_nY1_active_LODA_plot, label=\"Active-LODA\", linewidth=2, linestyle=':')\n",
    "    ax3.plot(xx, avg_nY1_GLAD_plot, label=\"GLAD\", linewidth=2, linestyle='-.')\n",
    "    ax3.plot(xx, avg_nY1_AAA_plot, label=\"AAA\", linewidth=2, linestyle='-')\n",
    "    ax3.set_xlabel(\"Batch\",fontsize=15)\n",
    "    ax3.set_ylabel(\"Anomalies detected\",fontsize=15)\n",
    "    ax3.set_title(\"Cumul. anomalies detected\",fontsize=17)\n",
    "    ax3.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax3.legend(fontsize=13)\n",
    "\n",
    "plt.tight_layout(h_pad=3.0)\n",
    "\n",
    "# Save the plot:\n",
    "fig.savefig(\"2d_Gaussian_Nominals_and_Anomalies2.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d90bab-db91-49f9-bcb8-022d11e42bcc",
   "metadata": {},
   "source": [
    "# Ten-dimensional Gaussian nominals with one ten-dimensional Gaussian anomaly.\n",
    "\n",
    "In our simulations we have the nominal data as $N((0,\\ldots,0),I_{10})$ and the anomaly data as $N((c,\\ldots,c),0.1*I_{10})$ for three choices of $c$.\n",
    "\n",
    "Note that the random seed for this set of trials is the same across these three trials, but different from the random seed used for all of the previous trials. This is because the previous random seed above leads to active-LODA's optimization scheme to crash on one of the trials. More broadly, active-LODA struggles, in general, to converge once there are more than one hundred or so anomalies, which is the case in all of the trials here (due to the crazy number of pairwise constraints it is trying to satisfy, e.g., if there are 20 labeled anomalies and 100 labeled nominals, there are 20x100=2000 pairwise constraints!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587cdf6-c751-436c-b8a5-c136da30c970",
   "metadata": {},
   "source": [
    "## 10-dimensional $c=0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156844a-2bbb-4b18-b988-5569b933fa0b",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfaf7a8-1b73-4fc2-a965-a4e09eab8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5792d2e-84ae-44b9-b58b-5be44fffc80c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Ten-dimensional Gaussian nominals with one ten-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "c = 0.5\n",
    "\n",
    "#Set n:\n",
    "num_dim = 10\n",
    "\n",
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "  \n",
    "    \n",
    "    def scaled_identity_matrix(num_dim,c_m):\n",
    "        return [[c_m if i == j else 0 for j in range(num_dim)] for i in range(num_dim)]\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[c]*num_dim]\n",
    "    \n",
    "    anomaly_cov_list = [scaled_identity_matrix(num_dim,.1)]\n",
    "    \n",
    "    nominal_mean = np.array([0]*num_dim)    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array(scaled_identity_matrix(num_dim,1))   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        #print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # Our trials: \n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'LogisticRegression'\n",
    "\n",
    "    #Initialization\n",
    "\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "    # Run the initialization function InitActiveAGG:\n",
    "    X_lab, Y_lab, all_labeled_scores = InitActiveAGG(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models)\n",
    "\n",
    "\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.4,min_n_nom=5,min_n_anom=1,tau_exp=tau)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        #new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:, 1]\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        \n",
    "        \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_AAA_0p5_10d = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_0p5_10d = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_0p5_10d_onlyAAA.npz\",\n",
    "         avg_AAA_0p5_10d = avg_AAA_0p5_10d,\n",
    "         avg_nY1_AAA_0p5_10d = avg_nY1_AAA_0p5_10d,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_0p5_10d_onlyAAA.npz\",\n",
    "         new_preds_0p5_10d = new_preds\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e95c77-6dc3-4ed7-beed-6f4ac5b19a40",
   "metadata": {},
   "source": [
    "### The other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3108699d-f8fb-4c1b-923e-27473a9d7300",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dad88d-26e9-4f38-a494-4bea6a04a391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Ten-dimensional Gaussian nominals with one ten-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "c = 0.5\n",
    "\n",
    "#Set n:\n",
    "num_dim = 10\n",
    "\n",
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "    \n",
    "    def scaled_identity_matrix(num_dim,c_m):\n",
    "        return [[c_m if i == j else 0 for j in range(num_dim)] for i in range(num_dim)]\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[c]*num_dim]\n",
    "    \n",
    "    anomaly_cov_list = [scaled_identity_matrix(num_dim,.1)]\n",
    "    \n",
    "    nominal_mean = np.array([0]*num_dim)    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array(scaled_identity_matrix(num_dim,1))   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # LODA TRIALS\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################        \n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # ACTIVE-LODA trials:\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau\n",
    "    \n",
    "    #There are hyperparameters that need to be set in advance for this algorithm. However, \n",
    "    #for simplicity we assume they tal the default values in the function optimize_w.\n",
    "    #C_A = 100  #default in their article\n",
    "    #C_eta = 1000. #default in their article\n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "    \n",
    "    #Use the first n_min data-points in X to get the number and set of LODA projectors:\n",
    "    #models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()   \n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "                    \n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "    \n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "\n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        #We actually have to, at this point, attach the current versions of H_A\n",
    "        #and H_N to new_unweighted_scores, since in this batch framework, we do\n",
    "        #not have a fixed number of data points from the start to the finish, like\n",
    "        #they do in Das et al. (2016). If we do not do this, it will affect the\n",
    "        #calculation of q_tau over time (a kind of bias will be introduced, maybe\n",
    "        #not the end of the world, but still.)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "\n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "\n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "\n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "      \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # GLAD trials:\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_old)\n",
    "                y_score = model.score_samples(X_old)\n",
    "                y_score.dtype = np.float64\n",
    "                all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "    \n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "                \n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]\n",
    "\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "\n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "        \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_LODA_0p5_10d = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_0p5_10d = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_0p5_10d = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_0p5_10d = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_0p5_10d = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_0p5_10d = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_0p5_10d.npz\",\n",
    "         avg_LODA_0p5_10d = avg_LODA_0p5_10d,\n",
    "         avg_active_LODA_0p5_10d = avg_active_LODA_0p5_10d,\n",
    "         avg_GLAD_0p5_10d = avg_GLAD_0p5_10d,\n",
    "         avg_nY1_LODA_0p5_10d = avg_nY1_LODA_0p5_10d,\n",
    "         avg_nY1_active_LODA_0p5_10d = avg_nY1_active_LODA_0p5_10d,\n",
    "         avg_nY1_GLAD_0p5_10d = avg_nY1_GLAD_0p5_10d,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_0p5_10d.npz\",\n",
    "         weighted_scores_0p5_10d = weighted_scores,\n",
    "         weighted_validation_scores_0p5_10d = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_0p5_10d = X_new_final_scores_ext,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175ad24-b3ff-4f9d-be4b-4cf2ae4c3f7a",
   "metadata": {},
   "source": [
    "### Bring back saved variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540a1d2-bdb3-4467-9e61-63bc42286eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_0p5_10d_onlyAAA = np.load(\"AUC_0p5_10d_onlyAAA.npz\")\n",
    "avg_AAA_0p5_10d = data_0p5_10d_onlyAAA[\"avg_AAA_0p5_10d\"]\n",
    "avg_nY1_AAA_0p5_10d = data_0p5_10d_onlyAAA[\"avg_nY1_AAA_0p5_10d\"]\n",
    "\n",
    "\n",
    "# The others\n",
    "data_0p5_10d = np.load(\"AUC_0p5_10d.npz\")\n",
    "avg_LODA_0p5_10d = data_0p5_10d[\"avg_LODA_0p5_10d\"]\n",
    "avg_active_LODA_0p5_10d = data_0p5_10d[\"avg_active_LODA_0p5_10d\"]\n",
    "avg_GLAD_0p5_10d = data_0p5_10d[\"avg_GLAD_0p5_10d\"]\n",
    "avg_nY1_LODA_0p5_10d = data_0p5_10d[\"avg_nY1_LODA_0p5_10d\"]\n",
    "avg_nY1_active_LODA_0p5_10d = data_0p5_10d[\"avg_nY1_active_LODA_0p5_10d\"]\n",
    "avg_nY1_GLAD_0p5_10d = data_0p5_10d[\"avg_nY1_GLAD_0p5_10d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d786d5-3f6c-4c1a-83d3-e2304a90e097",
   "metadata": {},
   "source": [
    "### Plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240ea19-5510-4c9e-8fb9-efe82ee18f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_0p5_10d, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_0p5_10d, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_0p5_10d, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_0p5_10d, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40048c4c-07cb-4b7f-b8d9-e2c52dd4fc72",
   "metadata": {},
   "source": [
    "### Cumulative number of detected anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2e6c4-2120-4542-8fc2-09a4638bffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_0p5_10d, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_0p5_10d, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_0p5_10d, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_0p5_10d, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b74e1-ece3-4f6c-88d6-8300dadab1e1",
   "metadata": {},
   "source": [
    "## 10-dimensional $c=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d891c-5a3e-43d8-bac2-6ed43d1de546",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7ea3a-7f35-45fb-b73a-ef8982489099",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac586978-bfc3-4284-b68a-bad0e60f9407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Ten-dimensional Gaussian nominals with one ten-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "c = 1\n",
    "\n",
    "#Set n:\n",
    "num_dim = 10\n",
    "\n",
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "    \n",
    "    def scaled_identity_matrix(num_dim,c_m):\n",
    "        return [[c_m if i == j else 0 for j in range(num_dim)] for i in range(num_dim)]\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[c]*num_dim]\n",
    "    \n",
    "    anomaly_cov_list = [scaled_identity_matrix(num_dim,.1)]\n",
    "    \n",
    "    nominal_mean = np.array([0]*num_dim)    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array(scaled_identity_matrix(num_dim,1))   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # Our trials: \n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'LogisticRegression'\n",
    "\n",
    "    #Initialization\n",
    "\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "    # Run the initialization function InitActiveAGG:\n",
    "    X_lab, Y_lab, all_labeled_scores = InitActiveAGG(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models)\n",
    "\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.4,min_n_nom=5,min_n_anom=1,tau_exp=tau)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        #new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:, 1]\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        \n",
    "        \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_AAA_1_10d = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_1_10d = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_1_10d_onlyAAA.npz\",\n",
    "         avg_AAA_1_10d = avg_AAA_1_10d,\n",
    "         avg_nY1_AAA_1_10d = avg_nY1_AAA_1_10d,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_1_10d_onlyAAA.npz\",\n",
    "         new_preds_1_10d = new_preds\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0668b0-8a79-4865-9ab5-8f2656bb318c",
   "metadata": {},
   "source": [
    "### The other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf5dcc-2ba3-46a1-bed1-54436ae25b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b2b9cd-6af1-4d0a-93dd-84b51a392bfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Ten-dimensional Gaussian nominals with one ten-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "c = 1\n",
    "\n",
    "#Set n:\n",
    "num_dim = 10\n",
    "\n",
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    def scaled_identity_matrix(num_dim,c_m):\n",
    "        return [[c_m if i == j else 0 for j in range(num_dim)] for i in range(num_dim)]\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[c]*num_dim]\n",
    "    \n",
    "    anomaly_cov_list = [scaled_identity_matrix(num_dim,.1)]\n",
    "    \n",
    "    nominal_mean = np.array([0]*num_dim)    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array(scaled_identity_matrix(num_dim,1))   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # LODA TRIALS\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # ACTIVE-LODA trials:\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau\n",
    "    \n",
    "    #There are hyperparameters that need to be set in advance for this algorithm. However, \n",
    "    #for simplicity we assume they tal the default values in the function optimize_w.\n",
    "    #C_A = 100  #default in their article\n",
    "    #C_eta = 1000. #default in their article\n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "\n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "                    \n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)  \n",
    "    \n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "            \n",
    "        #Calculate the AUC for the current batch ONLY:\n",
    "        #active_LODA_AUC[r] = roc_auc_score(Y_new,temp_new_active_LODA_scores) \n",
    "        \n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "\n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "        \n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "\n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "\n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "                \n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # GLAD trials:\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_old)\n",
    "                y_score = model.score_samples(X_old)\n",
    "                y_score.dtype = np.float64\n",
    "                all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "    \n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]\n",
    "\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "        \n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "        \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_LODA_1_10d = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_1_10d = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_1_10d = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_1_10d = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_1_10d = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_1_10d = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_1_10d.npz\",\n",
    "         avg_LODA_1_10d = avg_LODA_1_10d,\n",
    "         avg_active_LODA_1_10d = avg_active_LODA_1_10d,\n",
    "         avg_GLAD_1_10d = avg_GLAD_1_10d,\n",
    "         avg_nY1_LODA_1_10d = avg_nY1_LODA_1_10d,\n",
    "         avg_nY1_active_LODA_1_10d = avg_nY1_active_LODA_1_10d,\n",
    "         avg_nY1_GLAD_1_10d = avg_nY1_GLAD_1_10d,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_1_10d.npz\",\n",
    "         weighted_scores_1_10d = weighted_scores,\n",
    "         weighted_validation_scores_1_10d = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_1_10d = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a652a6f-c547-4ab6-ad68-0921dd2371a8",
   "metadata": {},
   "source": [
    "### Bring back saved variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974c4c8-d7b9-4083-9be2-4dad47c9b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_1_10d_onlyAAA = np.load(\"AUC_1_10d_onlyAAA.npz\")\n",
    "avg_AAA_1_10d = data_1_10d_onlyAAA[\"avg_AAA_1_10d\"]\n",
    "avg_nY1_AAA_1_10d = data_1_10d_onlyAAA[\"avg_nY1_AAA_1_10d\"]\n",
    "\n",
    "\n",
    "# The others\n",
    "data_1_10d = np.load(\"AUC_1_10d.npz\")\n",
    "avg_LODA_1_10d = data_1_10d[\"avg_LODA_1_10d\"]\n",
    "avg_active_LODA_1_10d = data_1_10d[\"avg_active_LODA_1_10d\"]\n",
    "avg_GLAD_1_10d = data_1_10d[\"avg_GLAD_1_10d\"]\n",
    "avg_nY1_LODA_1_10d = data_1_10d[\"avg_nY1_LODA_1_10d\"]\n",
    "avg_nY1_active_LODA_1_10d = data_1_10d[\"avg_nY1_active_LODA_1_10d\"]\n",
    "avg_nY1_GLAD_1_10d = data_1_10d[\"avg_nY1_GLAD_1_10d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dcf5c9-8ed3-4749-930f-4f6c7499ca44",
   "metadata": {},
   "source": [
    "### Plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcdeee-5088-42da-b6ef-0f501748e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_1_10d, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_1_10d, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_1_10d, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_1_10d, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03bdb8b-e668-4091-a22c-0b76926fd6f5",
   "metadata": {},
   "source": [
    "### Plot cumulative number of anomalies detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059355e6-8e73-4283-8976-6a1b92dc4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_1_10d, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_1_10d, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_1_10d, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_1_10d, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15413f51-8776-4b92-a437-fd6cc1fbd0d5",
   "metadata": {},
   "source": [
    "## 10-dimensional $c=1.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbde29dc-4704-41da-a4f8-c6a2577cd082",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9995be-c299-4ba4-aba0-bca71e4c8046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the same random seed np.random.seed(123456789) as everywhere else, active-LODA crashes due to the solver failing on the 5th loop.\n",
    "#We therefore change the seed here for these trials. \n",
    "\n",
    "np.random.seed(12345678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19c501-b5df-4d9b-b622-64b6ff349097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Ten-dimensional Gaussian nominals with one ten-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "c = 1.5\n",
    "\n",
    "#Set n:\n",
    "num_dim = 10\n",
    "\n",
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    \n",
    "    \n",
    "    def scaled_identity_matrix(num_dim,c_m):\n",
    "        return [[c_m if i == j else 0 for j in range(num_dim)] for i in range(num_dim)]\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[c]*num_dim]\n",
    "    \n",
    "    anomaly_cov_list = [scaled_identity_matrix(num_dim,.1)]\n",
    "    \n",
    "    nominal_mean = np.array([0]*num_dim)    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array(scaled_identity_matrix(num_dim,1))   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # Our trials: \n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'LogisticRegression'\n",
    "\n",
    "    #Initialization\n",
    "\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "    # Run the initialization function InitActiveAGG:\n",
    "    X_lab, Y_lab, all_labeled_scores = InitActiveAGG(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models)\n",
    "\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.4,min_n_nom=5,min_n_anom=1,tau_exp=tau)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        \n",
    "        \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_AAA_1p5_10d = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_1p5_10d = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_1p5_10d_onlyAAA.npz\",\n",
    "         avg_AAA_1p5_10d = avg_AAA_1p5_10d,\n",
    "         avg_nY1_AAA_1p5_10d = avg_nY1_AAA_1p5_10d,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_1p5_10d_onlyAAA.npz\",\n",
    "         new_preds_1p5_10d = new_preds\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78951677-a65d-4325-b2e8-7cb8d5b181d5",
   "metadata": {},
   "source": [
    "### The other three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218852aa-cb40-41ac-9553-198eafe41f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the same random seed np.random.seed(123456789) as everywhere else, active-LODA crashes due to the solver failing on the 5th loop.\n",
    "#We therefore change the seed here for these trials. \n",
    "np.random.seed(12345678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716bc57-7419-48eb-ae55-607b4d93ba2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Ten-dimensional Gaussian nominals with one ten-dimensional Gaussian anomaly \n",
    "############################################################################\n",
    "\n",
    "c = 1.5\n",
    "\n",
    "#Set n:\n",
    "num_dim = 10\n",
    "\n",
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    def scaled_identity_matrix(num_dim,c_m):\n",
    "        return [[c_m if i == j else 0 for j in range(num_dim)] for i in range(num_dim)]\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[c]*num_dim]\n",
    "    \n",
    "    anomaly_cov_list = [scaled_identity_matrix(num_dim,.1)]\n",
    "    \n",
    "    nominal_mean = np.array([0]*num_dim)    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array(scaled_identity_matrix(num_dim,1))   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same mixture distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # LODA TRIALS\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "        \n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # ACTIVE-LODA trials:\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau\n",
    "    \n",
    "    #There are hyperparameters that need to be set in advance for this algorithm. However, \n",
    "    #for simplicity we assume they tal the default values in the function optimize_w.\n",
    "    #C_A = 100  #default in their article\n",
    "    #C_eta = 1000. #default in their article\n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "\n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "                    \n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "    \n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "        \n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "\n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "                \n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "                \n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # GLAD trials:\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_old)\n",
    "                y_score = model.score_samples(X_old)\n",
    "                y_score.dtype = np.float64\n",
    "                all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "    \n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]\n",
    "\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "        \n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "       \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "\n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "        \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_LODA_1p5_10d = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_1p5_10d = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_1p5_10d = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_1p5_10d = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_1p5_10d = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_1p5_10d = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_1p5_10d.npz\",\n",
    "         avg_LODA_1p5_10d = avg_LODA_1p5_10d,\n",
    "         avg_active_LODA_1p5_10d = avg_active_LODA_1p5_10d,\n",
    "         avg_GLAD_1p5_10d = avg_GLAD_1p5_10d,\n",
    "         avg_nY1_LODA_1p5_10d = avg_nY1_LODA_1p5_10d,\n",
    "         avg_nY1_active_LODA_1p5_10d = avg_nY1_active_LODA_1p5_10d,\n",
    "         avg_nY1_GLAD_1p5_10d = avg_nY1_GLAD_1p5_10d,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_1p5_10d.npz\",\n",
    "         weighted_scores_1p5_10d = weighted_scores,\n",
    "         weighted_validation_scores_1p5_10d = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_1p5_10d = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097eaffe-bbc4-4183-96ed-2e2b44f05afe",
   "metadata": {},
   "source": [
    "### Bring back saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0be4c-349f-4d42-9546-795bd5162c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_1p5_10d_onlyAAA = np.load(\"AUC_1p5_10d_onlyAAA.npz\")\n",
    "avg_AAA_1p5_10d = data_1p5_10d_onlyAAA[\"avg_AAA_1p5_10d\"]\n",
    "avg_nY1_AAA_1p5_10d = data_1p5_10d_onlyAAA[\"avg_nY1_AAA_1p5_10d\"]\n",
    "\n",
    "\n",
    "# The others\n",
    "data_1p5_10d = np.load(\"AUC_1p5_10d.npz\")\n",
    "avg_LODA_1p5_10d = data_1p5_10d[\"avg_LODA_1p5_10d\"]\n",
    "avg_active_LODA_1p5_10d = data_1p5_10d[\"avg_active_LODA_1p5_10d\"]\n",
    "avg_GLAD_1p5_10d = data_1p5_10d[\"avg_GLAD_1p5_10d\"]\n",
    "avg_nY1_LODA_1p5_10d = data_1p5_10d[\"avg_nY1_LODA_1p5_10d\"]\n",
    "avg_nY1_active_LODA_1p5_10d = data_1p5_10d[\"avg_nY1_active_LODA_1p5_10d\"]\n",
    "avg_nY1_GLAD_1p5_10d = data_1p5_10d[\"avg_nY1_GLAD_1p5_10d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e530e25-d470-4e7a-bfb7-c555c3daacb5",
   "metadata": {},
   "source": [
    "### Plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86463ca-1a74-42d2-9a3c-9bc98222094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_1p5_10d, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_1p5_10d, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_1p5_10d, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_1p5_10d, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9561a-38e9-4579-8f05-0b57f8211b13",
   "metadata": {},
   "source": [
    "### Plot cumulative number of anomalies detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc7130-7331-4eb4-8bd8-9df8675f8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_1p5_10d, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_1p5_10d, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_1p5_10d, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_1p5_10d, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86054ea3-c836-4c82-85f9-8b678e145bad",
   "metadata": {},
   "source": [
    "## Bring back saved variables and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc0c09-2fd4-4402-9bff-c9e65658534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_0p5_10d_onlyAAA = np.load(\"AUC_0p5_10d_onlyAAA.npz\")\n",
    "avg_AAA_0p5_10d = data_0p5_10d_onlyAAA[\"avg_AAA_0p5_10d\"]\n",
    "avg_nY1_AAA_0p5_10d = data_0p5_10d_onlyAAA[\"avg_nY1_AAA_0p5_10d\"]\n",
    "# The others\n",
    "data_0p5_10d = np.load(\"AUC_0p5_10d.npz\")\n",
    "avg_LODA_0p5_10d = data_0p5_10d[\"avg_LODA_0p5_10d\"]\n",
    "avg_active_LODA_0p5_10d = data_0p5_10d[\"avg_active_LODA_0p5_10d\"]\n",
    "avg_GLAD_0p5_10d = data_0p5_10d[\"avg_GLAD_0p5_10d\"]\n",
    "avg_nY1_LODA_0p5_10d = data_0p5_10d[\"avg_nY1_LODA_0p5_10d\"]\n",
    "avg_nY1_active_LODA_0p5_10d = data_0p5_10d[\"avg_nY1_active_LODA_0p5_10d\"]\n",
    "avg_nY1_GLAD_0p5_10d = data_0p5_10d[\"avg_nY1_GLAD_0p5_10d\"]\n",
    "\n",
    "# Ours\n",
    "data_1_10d_onlyAAA = np.load(\"AUC_1_10d_onlyAAA.npz\")\n",
    "avg_AAA_1_10d = data_1_10d_onlyAAA[\"avg_AAA_1_10d\"]\n",
    "avg_nY1_AAA_1_10d = data_1_10d_onlyAAA[\"avg_nY1_AAA_1_10d\"]\n",
    "# The others\n",
    "data_1_10d = np.load(\"AUC_1_10d.npz\")\n",
    "avg_LODA_1_10d = data_1_10d[\"avg_LODA_1_10d\"]\n",
    "avg_active_LODA_1_10d = data_1_10d[\"avg_active_LODA_1_10d\"]\n",
    "avg_GLAD_1_10d = data_1_10d[\"avg_GLAD_1_10d\"]\n",
    "avg_nY1_LODA_1_10d = data_1_10d[\"avg_nY1_LODA_1_10d\"]\n",
    "avg_nY1_active_LODA_1_10d = data_1_10d[\"avg_nY1_active_LODA_1_10d\"]\n",
    "avg_nY1_GLAD_1_10d = data_1_10d[\"avg_nY1_GLAD_1_10d\"]\n",
    "\n",
    "# Ours\n",
    "data_1p5_10d_onlyAAA = np.load(\"AUC_1p5_10d_onlyAAA.npz\")\n",
    "avg_AAA_1p5_10d = data_1p5_10d_onlyAAA[\"avg_AAA_1p5_10d\"]\n",
    "avg_nY1_AAA_1p5_10d = data_1p5_10d_onlyAAA[\"avg_nY1_AAA_1p5_10d\"]\n",
    "# The others\n",
    "data_1p5_10d = np.load(\"AUC_1p5_10d.npz\")\n",
    "avg_LODA_1p5_10d = data_1p5_10d[\"avg_LODA_1p5_10d\"]\n",
    "avg_active_LODA_1p5_10d = data_1p5_10d[\"avg_active_LODA_1p5_10d\"]\n",
    "avg_GLAD_1p5_10d = data_1p5_10d[\"avg_GLAD_1p5_10d\"]\n",
    "avg_nY1_LODA_1p5_10d = data_1p5_10d[\"avg_nY1_LODA_1p5_10d\"]\n",
    "avg_nY1_active_LODA_1p5_10d = data_1p5_10d[\"avg_nY1_active_LODA_1p5_10d\"]\n",
    "avg_nY1_GLAD_1p5_10d = data_1p5_10d[\"avg_nY1_GLAD_1p5_10d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718285dc-d3ac-4e7b-8f3e-761f8cf175f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim = 10\n",
    "\n",
    "# Create the subplot grid (2 rows, 2 columns)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18,6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Add the AUC subplots:\n",
    "\n",
    "xx = list(range(1, n_loops + 1))\n",
    "xxactiveLODA = list(range(1, int(n_loops/2) + 1))\n",
    "\n",
    "# Iterate through the sets of arrays for AUC plots\n",
    "for i, (avg_LODA_plot, avg_active_LODA_plot, avg_GLAD_plot, avg_AAA_plot) in enumerate([\n",
    "    (avg_LODA_0p5_10d, avg_active_LODA_0p5_10d, avg_GLAD_0p5_10d, avg_AAA_0p5_10d),\n",
    "    (avg_LODA_1_10d, avg_active_LODA_1_10d, avg_GLAD_1_10d, avg_AAA_1_10d),\n",
    "    (avg_LODA_1p5_10d, avg_active_LODA_1p5_10d, avg_GLAD_1p5_10d, avg_AAA_1p5_10d),\n",
    "]):\n",
    "    # Target subplot in the second column (index 1 for columns)\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot the data on the subplot\n",
    "    ax.plot(xx, avg_LODA_plot, label=\"LODA\",linewidth = 2.5)\n",
    "    ax.plot(xxactiveLODA, avg_active_LODA_plot, label=\"Active-LODA\",linewidth = 2.5)\n",
    "    ax.plot(xx, avg_GLAD_plot, label=\"GLAD\", linewidth = 2.5)\n",
    "    ax.plot(xx, avg_AAA_plot, label=\"AAA\", linewidth = 2.5)\n",
    "    \n",
    "    # Add labels and grid\n",
    "    ax.set_xlabel(\"Batch\", fontsize=12)\n",
    "    ax.set_ylabel(\"Average AUC\", fontsize=12)\n",
    "    ax.set_xticks(xx)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(loc=\"best\", fontsize=10)\n",
    "    \n",
    "    # Optional: Add a title for each subplot\n",
    "    ax.set_title(f\"Average AUC over time (c={(i+1)/2})\", fontsize=14)\n",
    "\n",
    "    ax.set_xticks(range(5, n_loops + 1, 5))\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot:\n",
    "fig.savefig(\"10d_Gaussian_Nominals_and_Anomalies.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee98197-54fc-4bcf-b8f0-abf87343d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the subplot grid (3 rows, 2 columns)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(8, 12))  # Adjust figsize as needed\n",
    "\n",
    "# Define x-axis values\n",
    "xx = list(range(1, n_loops + 1))\n",
    "xxactiveLODA = list(range(1, int(n_loops/2) + 1))\n",
    "\n",
    "# Plot in the first column only\n",
    "for i, (avg_LODA_plot, avg_active_LODA_plot, avg_GLAD_plot, avg_AAA_plot) in enumerate([\n",
    "    (avg_LODA_0p5_10d, avg_active_LODA_0p5_10d, avg_GLAD_0p5_10d, avg_AAA_0p5_10d),\n",
    "    (avg_LODA_1_10d, avg_active_LODA_1_10d, avg_GLAD_1_10d, avg_AAA_1_10d),\n",
    "    (avg_LODA_1p5_10d, avg_active_LODA_1p5_10d, avg_GLAD_1p5_10d, avg_AAA_1p5_10d),\n",
    "]):\n",
    "    ax = axes[i, 0]  # First column\n",
    "\n",
    "    ax.plot(xx, avg_LODA_plot, label=\"LODA\", linewidth=2.5)\n",
    "    ax.plot(xxactiveLODA, avg_active_LODA_plot, label=\"Active-LODA\", linewidth=2.5)\n",
    "    ax.plot(xx, avg_GLAD_plot, label=\"GLAD\", linewidth=2.5)\n",
    "    ax.plot(xx, avg_AAA_plot, label=\"AAA\", linewidth=2.5)\n",
    "\n",
    "    ax.set_xlabel(\"Batch\", fontsize=15)\n",
    "    ax.set_ylabel(\"Average AUC\", fontsize=15)\n",
    "    #ax.set_xticks(range(5, n_loops + 1, 5))\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc=\"best\", fontsize=13)\n",
    "    ax.set_title(f\"AUC over time (c = {(i+1)/2})\", fontsize=17)\n",
    "\n",
    "for i, (avg_nY1_LODA_plot, avg_nY1_active_LODA_plot, avg_nY1_GLAD_plot, avg_nY1_AAA_plot) in enumerate([\n",
    "    (avg_nY1_LODA_0p5_10d, avg_nY1_active_LODA_0p5_10d, avg_nY1_GLAD_0p5_10d, avg_nY1_AAA_0p5_10d),\n",
    "    (avg_nY1_LODA_1_10d, avg_nY1_active_LODA_1_10d, avg_nY1_GLAD_1_10d, avg_nY1_AAA_1_10d),\n",
    "    (avg_nY1_LODA_1p5_10d, avg_nY1_active_LODA_1p5_10d, avg_nY1_GLAD_1p5_10d, avg_nY1_AAA_1p5_10d),\n",
    "]):\n",
    "    ax = axes[i, 1]  # second column\n",
    "\n",
    "    ax.plot(xx, avg_nY1_LODA_plot, label=\"LODA\", linewidth=1.5, linestyle='-')\n",
    "    ax.plot(xxactiveLODA, avg_nY1_active_LODA_plot, label=\"Active-LODA\", linewidth=1.5, linestyle=':')\n",
    "    ax.plot(xx, avg_nY1_GLAD_plot, label=\"GLAD\", linewidth=1.5, linestyle='-.')\n",
    "    ax.plot(xx, avg_nY1_AAA_plot, label=\"AAA\", linewidth=1.5, linestyle='-')\n",
    "\n",
    "    ax.set_xlabel(\"Batch\", fontsize=15)\n",
    "    ax.set_ylabel(\"Anomalies detected\", fontsize=15)\n",
    "    ax.set_title(\"Cumul. anomalies detected\", fontsize=17)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend(fontsize=13)\n",
    "\n",
    "# Adjust layout and save\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"10d_Gaussian_Nominals_and_Anomalies2.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de301cc0-a0dc-4040-9cd0-55a104825b4d",
   "metadata": {},
   "source": [
    "# Three further two-dimensional settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13086b9d",
   "metadata": {},
   "source": [
    "## Two-dimensional uniform distribution with reject regions at anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f294f4-9ebd-463a-b857-1cb8b24b4e80",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c03976-73a7-49bb-8f62-f578787864a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaeb636-a454-4eaa-871e-357126dd848e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    num_dim = 2\n",
    "    a_list = [np.ones(num_dim)*0.5]\n",
    "    epsilon = np.sqrt(tau)/2\n",
    "    L = num_dim\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        uniform_sampling_point_mass_with_epsilon_n_dim_rejection,\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,  \n",
    "        tau = tau, \n",
    "        a_list = a_list, \n",
    "        epsilon = epsilon, \n",
    "        L = L, \n",
    "        lower=0, \n",
    "        upper=1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        uniform_sampling_point_mass_with_epsilon_n_dim_rejection,\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,  \n",
    "        tau = tau, \n",
    "        a_list = a_list, \n",
    "        epsilon = epsilon, \n",
    "        L = L, \n",
    "        lower=0, \n",
    "        upper=1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        #print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # Our trials: \n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'LogisticRegression'\n",
    "\n",
    "    #Initialization\n",
    "\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "    # Run the initialization function InitActiveAGG:\n",
    "    X_lab, Y_lab, all_labeled_scores = InitActiveAGG(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models)\n",
    "\n",
    "\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.4,min_n_nom=5,min_n_anom=1,tau_exp=tau)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        #new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:, 1]\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        \n",
    "        \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "# Calculate column averages for each array\n",
    "avg_AAA_unif = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_unif = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_unif_onlyAAA.npz\",\n",
    "         avg_AAA_unif = avg_AAA_unif,\n",
    "         avg_nY1_AAA_unif = avg_nY1_AAA_unif,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_unif_onlyAAA.npz\",\n",
    "         new_preds_unif = new_preds\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e328c-f032-4dd0-806e-7c9fd148b464",
   "metadata": {},
   "source": [
    "### The other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2602fb-9a73-46f7-a96f-b4b491e86bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16393065-1045-42ea-871d-5a2c31da999b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    num_dim = 2\n",
    "    a_list = [np.ones(num_dim)*0.5]\n",
    "    epsilon = np.sqrt(tau)/2\n",
    "    L = num_dim\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        uniform_sampling_point_mass_with_epsilon_n_dim_rejection,\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,  \n",
    "        tau = tau, \n",
    "        a_list = a_list, \n",
    "        epsilon = epsilon, \n",
    "        L = L, \n",
    "        lower=0, \n",
    "        upper=1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        uniform_sampling_point_mass_with_epsilon_n_dim_rejection,\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,  \n",
    "        tau = tau, \n",
    "        a_list = a_list, \n",
    "        epsilon = epsilon, \n",
    "        L = L, \n",
    "        lower=0, \n",
    "        upper=1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # LODA TRIALS\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # ACTIVE-LODA trials:\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau\n",
    "    \n",
    "    #There are hyperparameters that need to be set in advance for this algorithm. However, \n",
    "    #for simplicity we assume they tal the default values in the function optimize_w.\n",
    "    #C_A = 100  #default in their article\n",
    "    #C_eta = 1000. #default in their article\n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "    \n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "\n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "    \n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "            \n",
    "        #Calculate the AUC for the current batch ONLY:\n",
    "        #active_LODA_AUC[r] = roc_auc_score(Y_new,temp_new_active_LODA_scores) \n",
    "        \n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "\n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "    \n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "                \n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "                \n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # GLAD trials:\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)    \n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_old)\n",
    "                y_score = model.score_samples(X_old)\n",
    "                y_score.dtype = np.float64\n",
    "                all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "    \n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]\n",
    "\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "        \n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "        \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_LODA_unif = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_unif = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_unif = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "\n",
    "avg_nY1_LODA_unif = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_unif = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_unif = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_unif.npz\",\n",
    "         avg_LODA_unif = avg_LODA_unif,\n",
    "         avg_active_LODA_unif = avg_active_LODA_unif,\n",
    "         avg_GLAD_unif = avg_GLAD_unif,\n",
    "         avg_nY1_LODA_unif = avg_nY1_LODA_unif,\n",
    "         avg_nY1_active_LODA_unif = avg_nY1_active_LODA_unif,\n",
    "         avg_nY1_GLAD_unif = avg_nY1_GLAD_unif,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_unif.npz\",\n",
    "         weighted_scores_unif = weighted_scores,\n",
    "         weighted_validation_scores_unif = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_unif = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a20eb3-0ebe-41fb-9947-645cc4ca3ded",
   "metadata": {},
   "source": [
    "### Bring back saved variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c31b0-a926-4edf-bb61-c68d99aa462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_unif_onlyAAA = np.load(\"AUC_unif_onlyAAA.npz\")\n",
    "avg_AAA_unif = data_unif_onlyAAA[\"avg_AAA_unif\"]\n",
    "avg_nY1_AAA_unif = data_unif_onlyAAA[\"avg_nY1_AAA_unif\"]\n",
    "\n",
    "\n",
    "# The others\n",
    "data_unif = np.load(\"AUC_unif.npz\")\n",
    "avg_LODA_unif = data_unif[\"avg_LODA_unif\"]\n",
    "avg_active_LODA_unif = data_unif[\"avg_active_LODA_unif\"]\n",
    "avg_GLAD_unif = data_unif[\"avg_GLAD_unif\"]\n",
    "avg_nY1_LODA_unif = data_unif[\"avg_nY1_LODA_unif\"]\n",
    "avg_nY1_active_LODA_unif = data_unif[\"avg_nY1_active_LODA_unif\"]\n",
    "avg_nY1_GLAD_unif = data_unif[\"avg_nY1_GLAD_unif\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db137e65-e89a-43bc-95e3-97fa9bab01b6",
   "metadata": {},
   "source": [
    "### Plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62db62-210c-4720-92d8-896b5feeb655",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_unif, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_unif, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_unif, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_unif, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4bdbc7-b8ef-450d-8562-aec8ed085ec0",
   "metadata": {},
   "source": [
    "### Plot cumulative number of anomalies detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063aa7b-c2ed-4ccb-87e3-14964515f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_unif, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_unif, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_unif, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_unif, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ea280d",
   "metadata": {},
   "source": [
    "## Two-dimensional uniform distribution on circle with reject regions at anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb25cc4-dafe-424d-9f84-65a23ce52704",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d9b4a-7a69-4364-95d8-06e8428a0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5420f9-3c8c-43ce-a21a-365db529b787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    #Set n:\n",
    "    num_dim = 2\n",
    "    # Specific arguments:\n",
    "    a_list = [np.array([0.0, 0.0])]\n",
    "    L = num_dim\n",
    "    radius = 1\n",
    "    epsilon = radius*np.exp((1/L)*(np.log(tau)-np.log(1-tau)))\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        sample_uniform_with_anomalies_in_ball,\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,  \n",
    "        tau = tau, \n",
    "        a_list = a_list, \n",
    "        epsilon = epsilon, \n",
    "        L = L,\n",
    "        radius = radius\n",
    "    )\n",
    "    \n",
    "    \n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        sample_uniform_with_anomalies_in_ball,\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,  \n",
    "        tau = tau, \n",
    "        a_list = a_list, \n",
    "        epsilon = epsilon, \n",
    "        L = L, \n",
    "        radius=radius\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # Our trials: \n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'LogisticRegression'\n",
    "\n",
    "    #Initialization\n",
    "\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "    # Run the initialization function InitActiveAGG:\n",
    "    X_lab, Y_lab, all_labeled_scores = InitActiveAGG(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models)\n",
    "\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.4,min_n_nom=5,min_n_anom=1,tau_exp=tau)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        #new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:, 1]\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        \n",
    "        \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B  \n",
    "\n",
    "avg_AAA_circ = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_circ = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_circ_onlyAAA.npz\",\n",
    "         avg_AAA_circ = avg_AAA_circ,\n",
    "         avg_nY1_AAA_circ = avg_nY1_AAA_circ,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_circ_onlyAAA.npz\",\n",
    "         new_preds_circ = new_preds\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66cd4e-bae4-4413-b8bb-7a4be40d13a0",
   "metadata": {},
   "source": [
    "### The other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8938ebcc-4ec7-409f-9850-0c44da01e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d4cdda-61bb-40a3-b6fa-8bda2e48f22e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    #Set n:\n",
    "    num_dim = 2\n",
    "    # Specific arguments:\n",
    "    a_list = [np.array([0.0, 0.0])]\n",
    "    L = num_dim\n",
    "    radius = 1\n",
    "    epsilon = radius*np.exp((1/L)*(np.log(tau)-np.log(1-tau)))\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        sample_uniform_with_anomalies_in_ball,\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,  \n",
    "        tau = tau, \n",
    "        a_list = a_list, \n",
    "        epsilon = epsilon, \n",
    "        L = L,\n",
    "        radius = radius\n",
    "    )\n",
    "    \n",
    "    \n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        sample_uniform_with_anomalies_in_ball,\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,  \n",
    "        tau = tau, \n",
    "        a_list = a_list, \n",
    "        epsilon = epsilon, \n",
    "        L = L, \n",
    "        radius=radius\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # LODA TRIALS\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # ACTIVE-LODA trials:\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau\n",
    "    \n",
    "    #There are hyperparameters that need to be set in advance for this algorithm. However, \n",
    "    #for simplicity we assume they tal the default values in the function optimize_w.\n",
    "    #C_A = 100  #default in their article\n",
    "    #C_eta = 1000. #default in their article\n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "    \n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()    \n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #print('There were no labeled anomalies in the old data')\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "                    \n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "    \n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "\n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "        \n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "\n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            #print('next top index:',next_top_index)\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "                \n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # GLAD trials:\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_old)\n",
    "                y_score = model.score_samples(X_old)\n",
    "                y_score.dtype = np.float64\n",
    "                all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "    \n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]\n",
    " \n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "        \n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "        \n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "        \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "\n",
    "# Calculate column averages for each array\n",
    "avg_LODA_circ = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_circ = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_circ = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_circ = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_circ = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_circ = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_circ.npz\",\n",
    "         avg_LODA_circ = avg_LODA_circ,\n",
    "         avg_active_LODA_circ = avg_active_LODA_circ,\n",
    "         avg_GLAD_circ = avg_GLAD_circ,\n",
    "         avg_nY1_LODA_circ = avg_nY1_LODA_circ,\n",
    "         avg_nY1_active_LODA_circ = avg_nY1_active_LODA_circ,\n",
    "         avg_nY1_GLAD_circ = avg_nY1_GLAD_circ,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_circ.npz\",\n",
    "         weighted_scores_circ = weighted_scores,\n",
    "         weighted_validation_scores_circ = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_circ = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b663f-b79f-4291-ad24-c96a76f2af77",
   "metadata": {},
   "source": [
    "### Bring back saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f51f9-ca49-438a-8d71-fb5bbfb34121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_circ_onlyAAA = np.load(\"AUC_circ_onlyAAA.npz\")\n",
    "avg_AAA_circ = data_circ_onlyAAA[\"avg_AAA_circ\"]\n",
    "avg_nY1_AAA_circ = data_circ_onlyAAA[\"avg_nY1_AAA_circ\"]\n",
    "\n",
    "\n",
    "# The others\n",
    "data_circ = np.load(\"AUC_circ.npz\")\n",
    "avg_LODA_circ = data_circ[\"avg_LODA_circ\"]\n",
    "avg_active_LODA_circ = data_circ[\"avg_active_LODA_circ\"]\n",
    "avg_GLAD_circ = data_circ[\"avg_GLAD_circ\"]\n",
    "avg_nY1_LODA_circ = data_circ[\"avg_nY1_LODA_circ\"]\n",
    "avg_nY1_active_LODA_circ = data_circ[\"avg_nY1_active_LODA_circ\"]\n",
    "avg_nY1_GLAD_circ = data_circ[\"avg_nY1_GLAD_circ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41cd0c-f53d-45d0-80ac-248a0c3207ac",
   "metadata": {},
   "source": [
    "### Plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296388f7-8a48-4c39-95e4-b15ae7acc068",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_circ, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_circ, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_circ, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_circ, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036208fd-c281-4f34-83c0-660f8fca257c",
   "metadata": {},
   "source": [
    "### Plot cumulative number of anomalies detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572a0d7-c216-40a8-bc15-aa72e90d81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_circ, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_circ, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_circ, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_circ, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb23c1-4014-4d19-a428-4d491b44fcd0",
   "metadata": {},
   "source": [
    "## Two-dimensional Gaussian with anomalies on edge of a circle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe36149d-b9e0-46ed-adbd-b810f1b8eebb",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2779f37-162c-4a48-85d4-d322e416f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b9c1a-2716-4511-8f97-9f73e6c5c59e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    num_dim = 2\n",
    "    # Specific arguments:\n",
    "    nominal_mean = np.array([0, 0])\n",
    "    nominal_cov = np.array([[1.0, 0], [0, 1.0]])\n",
    "    L = num_dim\n",
    "    radius = 1\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_uniform_surface_anomalies,\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,  \n",
    "        tau = tau, \n",
    "        nominal_mean = nominal_mean,\n",
    "        nominal_cov = nominal_cov,\n",
    "        L = L,\n",
    "        radius = radius\n",
    "    )\n",
    "    \n",
    "    \n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_uniform_surface_anomalies,\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,  \n",
    "        tau = tau, \n",
    "        nominal_mean = nominal_mean,\n",
    "        nominal_cov = nominal_cov,\n",
    "        L = L,\n",
    "        radius = radius\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # Our trials: \n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'LogisticRegression'\n",
    "\n",
    "    #Initialization\n",
    "\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "    # Run the initialization function InitActiveAGG:\n",
    "    X_lab, Y_lab, all_labeled_scores = InitActiveAGG(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models)\n",
    "\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.4,min_n_nom=5,min_n_anom=1,tau_exp=tau)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        #new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:, 1]\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        \n",
    "        \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_AAA_ring = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_ring = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_ring_onlyAAA.npz\",\n",
    "         avg_AAA_ring = avg_AAA_ring,\n",
    "         avg_nY1_AAA_ring = avg_nY1_AAA_ring,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_ring_onlyAAA.npz\",\n",
    "         new_preds_ring = new_preds\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba089513-5ddc-488f-bde6-1fc190449774",
   "metadata": {},
   "source": [
    "### The other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f5c00-bb0a-4ce8-ac1b-a6207ff354be",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5777a-b8ec-41e9-b30f-03382d7bfd0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    num_dim = 2\n",
    "    # Specific arguments:\n",
    "    nominal_mean = np.array([0, 0])\n",
    "    nominal_cov = np.array([[1.0, 0], [0, 1.0]])\n",
    "    L = num_dim\n",
    "    radius = 1\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_uniform_surface_anomalies,\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,  \n",
    "        tau = tau, \n",
    "        nominal_mean = nominal_mean,\n",
    "        nominal_cov = nominal_cov,\n",
    "        L = L,\n",
    "        radius = radius\n",
    "    )\n",
    "    \n",
    "    \n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_uniform_surface_anomalies,\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,  \n",
    "        tau = tau, \n",
    "        nominal_mean = nominal_mean,\n",
    "        nominal_cov = nominal_cov,\n",
    "        L = L,\n",
    "        radius = radius\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Your code for plotting data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot density of nominals (blue)\n",
    "    sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Plot density of anomalies (red)\n",
    "    sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6)\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Density Plot (2D)\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Manually create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "        Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')  # Manually adding legend\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot for nominals (blue)\n",
    "    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], color='blue', alpha=0.6, label=\"Nominals\")\n",
    "    \n",
    "    # Scatter plot for anomalies (red)\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', alpha=0.6, label=\"Anomalies\")\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Scatter Plot of Nominals and Anomalies\", fontsize=16)\n",
    "    plt.xlabel(\"X-axis\", fontsize=14)\n",
    "    plt.ylabel(\"Y-axis\", fontsize=14)\n",
    "    \n",
    "    # Adding legend\n",
    "    plt.legend(loc='upper right')  # Automatically adding legend with labels from scatter plots\n",
    "    \n",
    "    # Adding grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Showing the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        #print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            #print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "    #Getting the LODA models and the scores on the external validation set:\n",
    "    #Simply use the first n_min data-points in X to do this.\n",
    "    models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set:\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],best_m))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # LODA TRIALS\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # ACTIVE-LODA trials:\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau\n",
    "    \n",
    "    #There are hyperparameters that need to be set in advance for this algorithm. However, \n",
    "    #for simplicity we assume they tal the default values in the function optimize_w.\n",
    "    #C_A = 100  #default in their article\n",
    "    #C_eta = 1000. #default in their article    \n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "\n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "                    \n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "\n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "        \n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "    \n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "                \n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # GLAD trials:\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_old)\n",
    "                y_score = model.score_samples(X_old)\n",
    "                y_score.dtype = np.float64\n",
    "                all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "    \n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]\n",
    "\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "\n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "        \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_LODA_ring = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_ring = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_ring = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_ring = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_ring = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_ring = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_ring.npz\",\n",
    "         avg_LODA_ring = avg_LODA_ring,\n",
    "         avg_active_LODA_ring = avg_active_LODA_ring,\n",
    "         avg_GLAD_ring = avg_GLAD_ring,\n",
    "         avg_nY1_LODA_ring = avg_nY1_LODA_ring,\n",
    "         avg_nY1_active_LODA_ring = avg_nY1_active_LODA_ring,\n",
    "         avg_nY1_GLAD_ring = avg_nY1_GLAD_ring,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_ring.npz\",\n",
    "         weighted_scores_ring = weighted_scores,\n",
    "         weighted_validation_scores_ring = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_ring = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d3339-b9e4-4327-a5a7-65f51d0d0f99",
   "metadata": {},
   "source": [
    "### Bring back saved variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c8b7a-c3b2-4088-91a3-d9062fa6237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_ring_onlyAAA = np.load(\"AUC_ring_onlyAAA.npz\")\n",
    "avg_AAA_ring = data_ring_onlyAAA[\"avg_AAA_ring\"]\n",
    "avg_nY1_AAA_ring = data_ring_onlyAAA[\"avg_nY1_AAA_ring\"]\n",
    "\n",
    "# The others\n",
    "data_ring = np.load(\"AUC_ring.npz\")\n",
    "avg_LODA_ring = data_ring[\"avg_LODA_ring\"]\n",
    "avg_active_LODA_ring = data_ring[\"avg_active_LODA_ring\"]\n",
    "avg_GLAD_ring = data_ring[\"avg_GLAD_ring\"]\n",
    "avg_nY1_LODA_ring = data_ring[\"avg_nY1_LODA_ring\"]\n",
    "avg_nY1_active_LODA_ring = data_ring[\"avg_nY1_active_LODA_ring\"]\n",
    "avg_nY1_GLAD_ring = data_ring[\"avg_nY1_GLAD_ring\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f8e14-9f3f-4903-91db-9897ad73c3a6",
   "metadata": {},
   "source": [
    "### Plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f4d7a-f2af-4449-83c2-f8d46c770afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_ring, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_ring, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_ring, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_ring, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f531d-fab8-4b3f-a19a-08e6f4e5b917",
   "metadata": {},
   "source": [
    "### Plot cumulative number of anomalies detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d342d3-881d-45f9-9448-db0c02dd72ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_ring, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_ring, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_ring, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_ring, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c2cd5-0538-4833-9004-8282218830b6",
   "metadata": {},
   "source": [
    "## Bring back all saved variables and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb4df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_unif_onlyAAA = np.load(\"AUC_unif_onlyAAA.npz\")\n",
    "avg_AAA_unif = data_unif_onlyAAA[\"avg_AAA_unif\"]\n",
    "avg_nY1_AAA_unif = data_unif_onlyAAA[\"avg_nY1_AAA_unif\"]\n",
    "# The others\n",
    "data_unif = np.load(\"AUC_unif.npz\")\n",
    "avg_LODA_unif = data_unif[\"avg_LODA_unif\"]\n",
    "avg_active_LODA_unif = data_unif[\"avg_active_LODA_unif\"]\n",
    "avg_GLAD_unif = data_unif[\"avg_GLAD_unif\"]\n",
    "avg_nY1_LODA_unif = data_unif[\"avg_nY1_LODA_unif\"]\n",
    "avg_nY1_active_LODA_unif = data_unif[\"avg_nY1_active_LODA_unif\"]\n",
    "avg_nY1_GLAD_unif = data_unif[\"avg_nY1_GLAD_unif\"]\n",
    "\n",
    "# Ours\n",
    "data_circ_onlyAAA = np.load(\"AUC_circ_onlyAAA.npz\")\n",
    "avg_AAA_circ = data_circ_onlyAAA[\"avg_AAA_circ\"]\n",
    "avg_nY1_AAA_circ = data_circ_onlyAAA[\"avg_nY1_AAA_circ\"]\n",
    "# The others\n",
    "data_circ = np.load(\"AUC_circ.npz\")\n",
    "avg_LODA_circ = data_circ[\"avg_LODA_circ\"]\n",
    "avg_active_LODA_circ = data_circ[\"avg_active_LODA_circ\"]\n",
    "avg_GLAD_circ = data_circ[\"avg_GLAD_circ\"]\n",
    "avg_nY1_LODA_circ = data_circ[\"avg_nY1_LODA_circ\"]\n",
    "avg_nY1_active_LODA_circ = data_circ[\"avg_nY1_active_LODA_circ\"]\n",
    "avg_nY1_GLAD_circ = data_circ[\"avg_nY1_GLAD_circ\"]\n",
    "\n",
    "# Ours\n",
    "data_ring_onlyAAA = np.load(\"AUC_ring_onlyAAA.npz\")\n",
    "avg_AAA_ring = data_ring_onlyAAA[\"avg_AAA_ring\"]\n",
    "avg_nY1_AAA_ring = data_ring_onlyAAA[\"avg_nY1_AAA_ring\"]\n",
    "# The others\n",
    "data_ring = np.load(\"AUC_ring.npz\")\n",
    "avg_LODA_ring = data_ring[\"avg_LODA_ring\"]\n",
    "avg_active_LODA_ring = data_ring[\"avg_active_LODA_ring\"]\n",
    "avg_GLAD_ring = data_ring[\"avg_GLAD_ring\"]\n",
    "avg_nY1_LODA_ring = data_ring[\"avg_nY1_LODA_ring\"]\n",
    "avg_nY1_active_LODA_ring = data_ring[\"avg_nY1_active_LODA_ring\"]\n",
    "avg_nY1_GLAD_ring = data_ring[\"avg_nY1_GLAD_ring\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9936b1e-5799-4acd-b64b-30168d7d7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the subplot grid (3 rows, 3 columns now)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "# Define x-axis values\n",
    "xx = list(range(1, n_loops + 1))\n",
    "xxactiveLODA = list(range(1, int(n_loops/2) + 1))\n",
    "\n",
    "################################################################################\n",
    "#Unif:\n",
    "    \n",
    "num_dim = 2\n",
    "a_list = [np.ones(num_dim)*0.5]\n",
    "epsilon = np.sqrt(tau)/2\n",
    "L = num_dim\n",
    "\n",
    "# Sampling\n",
    "X, Y = sample_data(\n",
    "    uniform_sampling_point_mass_with_epsilon_n_dim_rejection,\n",
    "    n_old=n_old,                                           # Initial number of data points\n",
    "    B=B,                                                   # Batch size\n",
    "    n_loops=n_loops,  \n",
    "    tau = tau, \n",
    "    a_list = a_list, \n",
    "    epsilon = epsilon, \n",
    "    L = L, \n",
    "    lower=0, \n",
    "    upper=1\n",
    ")\n",
    "\n",
    "# Plot on the i-th subplot in the first column\n",
    "ax = axes[0, 0]\n",
    "sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6, ax=ax)\n",
    "sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6, ax=ax)\n",
    "\n",
    "# Add title, labels, and grid\n",
    "ax.set_title(\"Density Plot\", fontsize=14)\n",
    "ax.set_xlabel(\"X-axis\", fontsize=12)\n",
    "ax.set_ylabel(\"Y-axis\", fontsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "    Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=14)\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "################################################################################\n",
    "#Circ:\n",
    "\n",
    "#Set n:\n",
    "num_dim = 2\n",
    "# Specific arguments:\n",
    "a_list = [np.array([0.0, 0.0])]\n",
    "L = num_dim\n",
    "radius = 1\n",
    "epsilon = radius*np.exp((1/L)*(np.log(tau)-np.log(1-tau)))\n",
    "\n",
    "# Sampling\n",
    "X, Y = sample_data(\n",
    "    sample_uniform_with_anomalies_in_ball,\n",
    "    n_old=n_old,                                           # Initial number of data points\n",
    "    B=B,                                                   # Batch size\n",
    "    n_loops=n_loops,  \n",
    "    tau = tau, \n",
    "    a_list = a_list, \n",
    "    epsilon = epsilon, \n",
    "    L = L,\n",
    "    radius = radius\n",
    ")\n",
    "\n",
    "# Plot on the i-th subplot in the first column\n",
    "ax = axes[1, 0]\n",
    "sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6, ax=ax)\n",
    "sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6, ax=ax)\n",
    "\n",
    "# Add title, labels, and grid\n",
    "ax.set_title(\"Density Plot\", fontsize=14)\n",
    "ax.set_xlabel(\"X-axis\", fontsize=12)\n",
    "ax.set_ylabel(\"Y-axis\", fontsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "    Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=14)\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "################################################################################\n",
    "#Ring:\n",
    "    \n",
    "num_dim = 2\n",
    "# Specific arguments:\n",
    "nominal_mean = np.array([0, 0])\n",
    "nominal_cov = np.array([[1.0, 0], [0, 1.0]])\n",
    "L = num_dim\n",
    "radius = 1\n",
    "\n",
    "# Sampling\n",
    "X, Y = sample_data(\n",
    "    multivariate_gaussian_sampling_with_uniform_surface_anomalies,\n",
    "    n_old=n_old,                                           # Initial number of data points\n",
    "    B=B,                                                   # Batch size\n",
    "    n_loops=n_loops,  \n",
    "    tau = tau, \n",
    "    nominal_mean = nominal_mean,\n",
    "    nominal_cov = nominal_cov,\n",
    "    L = L,\n",
    "    radius = radius\n",
    ")\n",
    "\n",
    "# Plot on the i-th subplot in the first column\n",
    "ax = axes[2, 0]\n",
    "sns.kdeplot(x=X[Y == 0][:, 0], y=X[Y == 0][:, 1], cmap=\"Blues\", fill=True, alpha=0.6, ax=ax)\n",
    "sns.kdeplot(x=X[Y == 1][:, 0], y=X[Y == 1][:, 1], cmap=\"Reds\", fill=True, alpha=0.6, ax=ax)\n",
    "\n",
    "# Add title, labels, and grid\n",
    "ax.set_title(\"Density plot\", fontsize=17)\n",
    "ax.set_xlabel(\"X-axis\", fontsize=15)\n",
    "ax.set_ylabel(\"Y-axis\", fontsize=15)\n",
    "ax.grid(True)\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "    Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=14)\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################\n",
    "# Add the AUC subplots too:\n",
    "\n",
    "\n",
    "# Iterate through the sets of arrays for AUC plots\n",
    "for i, (avg_LODA_plot, avg_active_LODA_plot, avg_GLAD_plot, avg_AAA_plot) in enumerate([\n",
    "    (avg_LODA_unif, avg_active_LODA_unif, avg_GLAD_unif, avg_AAA_unif),\n",
    "    (avg_LODA_circ, avg_active_LODA_circ, avg_GLAD_circ, avg_AAA_circ),\n",
    "    (avg_LODA_ring, avg_active_LODA_ring, avg_GLAD_ring, avg_AAA_ring)\n",
    "]):\n",
    "    # Target subplot in the second column (index 1 for columns)\n",
    "    ax = axes[i, 1]\n",
    "    \n",
    "    # Plot the data on the subplot\n",
    "    ax.plot(xx, avg_LODA_plot, label=\"LODA\",linewidth=1.5)\n",
    "    ax.plot(xxactiveLODA, avg_active_LODA_plot, label=\"Active-LODA\",linewidth=1.5)\n",
    "    ax.plot(xx, avg_GLAD_plot, label=\"GLAD\",linewidth=1.5)\n",
    "    ax.plot(xx, avg_AAA_plot, label=\"AAA\",linewidth=1.5)\n",
    "    \n",
    "    # Add labels and grid\n",
    "    ax.set_xlabel(\"Batch\", fontsize=15)\n",
    "    ax.set_ylabel(\"Average AUC\", fontsize=15)\n",
    "    #ax.set_xticks(xx)\n",
    "    #ax.set_xticks(range(5, n_loops + 1, 5))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(loc=\"best\", fontsize=13)\n",
    "    \n",
    "    # Optional: Add a title for each subplot\n",
    "    ax.set_title(\"Average AUC over time\", fontsize=17)\n",
    "\n",
    "###############################################################################@\n",
    "# Cumulative anomaly plots\n",
    "\n",
    "for i, (avg_nY1_LODA_plot, avg_nY1_active_LODA_plot, avg_nY1_GLAD_plot, avg_nY1_AAA_plot) in enumerate([\n",
    "    (avg_nY1_LODA_unif, avg_nY1_active_LODA_unif, avg_nY1_GLAD_unif, avg_nY1_AAA_unif),\n",
    "    (avg_nY1_LODA_circ, avg_nY1_active_LODA_circ, avg_nY1_GLAD_circ, avg_nY1_AAA_circ),\n",
    "    (avg_nY1_LODA_ring, avg_nY1_active_LODA_ring, avg_nY1_GLAD_ring, avg_nY1_AAA_ring),\n",
    "]):\n",
    "    ax = axes[i, 2]\n",
    "\n",
    "    # Plot the cumulative anomaly counts\n",
    "    ax.plot(xx, avg_nY1_LODA_plot, label=\"LODA\", linewidth=1.5, linestyle='-')\n",
    "    ax.plot(xxactiveLODA, avg_nY1_active_LODA_plot, label=\"Active-LODA\", linewidth=1.5, linestyle=':')\n",
    "    ax.plot(xx, avg_nY1_GLAD_plot, label=\"GLAD\", linewidth=1.5, linestyle='-.')\n",
    "    ax.plot(xx, avg_nY1_AAA_plot, label=\"AAA\", linewidth=1.5, linestyle='-')\n",
    "\n",
    "    ax.set_xlabel(\"Batch\", fontsize=15)\n",
    "    ax.set_ylabel(\"Anomalies detected\", fontsize=15)\n",
    "    ax.set_title(\"Cumul. anomalies detected\", fontsize=17)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend(fontsize=13)\n",
    "\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot:\n",
    "fig.savefig(\"2d_Scenarios2.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8e43e-4f43-46b9-a7f3-0d04ed8fa692",
   "metadata": {},
   "source": [
    "# Score function trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca921689-27a6-4bad-b2fa-2baa6250b9bc",
   "metadata": {},
   "source": [
    "## Global argument values\n",
    "\n",
    "There are a number of arguments which are required for all of the data generation methods. At this point in the script we shall give them actual default values. In the interest of specific simulation schemes, we may change these values further below to gain a better understanding of certain results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b1507-f3c2-4f46-a416-76b8eb7588e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of \"old\" data points we start with:\n",
    "n_old = 1000\n",
    "\n",
    "# The number of data points in each future batch:\n",
    "B = 500\n",
    "\n",
    "# The number of future batches:\n",
    "n_loops = 200\n",
    "\n",
    "# The mixture parameter: (there is no mixture required for the data generation part of this part of script)\n",
    "tau = 0.00\n",
    "\n",
    "# Choose n_send for any method:\n",
    "n_send = 5\n",
    "\n",
    "#n_min required for GLAD:\n",
    "n_min = 1000\n",
    "\n",
    "#Set an upper bound for the number of LODA projections:\n",
    "M_max = 15\n",
    "\n",
    "#Parameter set to default 0.01 in Pevny (2015). However, in small-dimensional settings (e.g., d=2) this\n",
    "#may not be a good idea?\n",
    "tau_M = 0.1\n",
    "\n",
    "#Proportion of initial data we suppose we know the true anomaly status of:\n",
    "u = 0.1\n",
    "\n",
    "#Number of simulation trials for each setting:\n",
    "n_trials = 5\n",
    "\n",
    "#Number of models:\n",
    "n_models = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445eef3-aa48-4b96-b8f9-46fb30726a13",
   "metadata": {},
   "source": [
    "## The random score way\n",
    "\n",
    "Here we simply assign to each and every data point a uniform random score between 0 and 1 over a set of such \"models\". We then choose one of the models, and some subinterval in [0,1], and give anomaly labels = 1 to points with scores in this interval, and nominal labels = 0 to all the rest. Thus the scores will be linked with anomaly labels for only this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6a7f6-a80c-492c-84c8-6c7c2e9fedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc822ffb-a38b-4c2d-83eb-eaf9d01bd265",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a8b26-55ab-49d9-81a7-f32083840d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "    \n",
    "    # Set the location of the anomaly scores (out of 0.05, 0.25, 0.5, 0.75, 0.95 ?)\n",
    "    my_centre = 0.5\n",
    "    \n",
    "    #Set tau back to 0 for each loop, since we don't want a mixture in the original data now:\n",
    "    tau = 0\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[0.5,0.5]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #RESET TAU TO ITS TRUE VALUE:\n",
    "    tau = 0.01\n",
    "    \n",
    "    curr_method = \"RandomScore\"\n",
    "    \n",
    "    # Build the models dictionary\n",
    "    models = build_random_score_models(M=n_models)\n",
    "    best_m = n_models\n",
    "    \n",
    "    new_unweighted_scores_fixed = np.zeros((X.shape[0], len(models)))\n",
    "        \n",
    "    # Iterate through the models and get their scores\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X)\n",
    "        y_score = model.score_samples(X)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_scores_fixed[:, i] = y_score.squeeze()\n",
    "    \n",
    "    new_unweighted_validation_scores = np.zeros((X_AUC.shape[0], len(models)))\n",
    "    \n",
    "    # Do the same thing for the external data:\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:, i] = y_score.squeeze()\n",
    "    \n",
    "    \n",
    "    # (Re)Define the labels Y based on some feature of the data. Here to begin with \n",
    "    # this will be some subinterval for the scores from the first model.\n",
    "    \n",
    "    Y = np.empty((np.shape(X)[0],))\n",
    "    anom_interval_min = my_centre - (tau)\n",
    "    anom_interval_max = my_centre +  (tau)\n",
    "    for i in range(len(Y)):\n",
    "        Y[i] = 1*(anom_interval_min <=  new_unweighted_scores_fixed[i,0] <= anom_interval_max) \n",
    "    \n",
    "    Y_AUC = np.empty((np.shape(X_AUC)[0],))\n",
    "    anom_interval_min = my_centre - (tau)\n",
    "    anom_interval_max = my_centre +  (tau)\n",
    "    for i in range(len(Y_AUC)):\n",
    "        Y_AUC[i] = 1*(anom_interval_min <=  new_unweighted_validation_scores[i,0] <= anom_interval_max) \n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "        if curr_method == \"RandomScore\" and n_remaining_anomalies > 0:\n",
    "            related_scores = []\n",
    "            for jj in range(n_old):\n",
    "                if Y_muted[jj] == 1:\n",
    "                    related_scores = related_scores + [new_unweighted_scores_fixed[jj,n_models-1]]\n",
    "    \n",
    "    \n",
    "    ###############################################################################\n",
    "    # AAA method\n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'RandomForestClassifier'\n",
    "    \n",
    "    #Initialization\n",
    "    \n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "        # Run the initialization function InitActiveAGG_2:\n",
    "        if curr_method == \"RandomScore\":\n",
    "            X_lab, Y_lab, all_labeled_scores = InitActiveAGG_2(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models,curr_method=curr_method,now_scores = new_unweighted_scores_fixed[:n_old,:])\n",
    "        \n",
    "        else:\n",
    "            X_lab, Y_lab, all_labeled_scores = InitActiveAGG_2(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models,curr_method=curr_method)\n",
    "    \n",
    "    \n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        if curr_method == \"RandomScore\":\n",
    "            X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG_2(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.6,min_n_nom=5,min_n_anom=1,tau_exp=tau,curr_method = curr_method,now_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index,:])  \n",
    "        else:\n",
    "            X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG_2(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.6,min_n_nom=5,min_n_anom=1,tau_exp=tau,curr_method = curr_method)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "\n",
    "       \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        #new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:, 1]\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        \n",
    "        \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "# Calculate column averages for each array\n",
    "avg_AAA_RS1 = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_RS1 = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_RS1_onlyAAA.npz\",\n",
    "         avg_AAA_RS1 = avg_AAA_RS1,\n",
    "         avg_nY1_AAA_RS1 = avg_nY1_AAA_RS1,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_RS1_onlyAAA.npz\",\n",
    "         new_preds_RS1 = new_preds\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797c61b-5947-495c-b910-174a0a12c27b",
   "metadata": {},
   "source": [
    "### The other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e17be-3cbc-48ee-b063-6eca85dd623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9ea97-a3a0-4768-8111-5f790302d43b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "    \n",
    "    # Set the location of the anomaly scores (out of 0.05, 0.25, 0.5, 0.75, 0.95 ?)\n",
    "    my_centre = 0.5\n",
    "    \n",
    "    #Set tau back to 0 for each loop, since we don't want a mixture in the original data now:\n",
    "    tau = 0\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[0.5,0.5]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #RESET TAU TO ITS TRUE VALUE:\n",
    "    tau = 0.01\n",
    "    \n",
    "    curr_method = \"RandomScore\"\n",
    "    \n",
    "    # Build the models dictionary\n",
    "    models = build_random_score_models(M=n_models)\n",
    "    best_m = n_models\n",
    "    \n",
    "    new_unweighted_scores_fixed = np.zeros((X.shape[0], len(models)))\n",
    "        \n",
    "    # Iterate through the models and get their scores\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X)\n",
    "        y_score = model.score_samples(X)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_scores_fixed[:, i] = y_score.squeeze()\n",
    "    \n",
    "    new_unweighted_validation_scores = np.zeros((X_AUC.shape[0], len(models)))\n",
    "    \n",
    "    # Do the same thing for the external data:\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_AUC)\n",
    "        y_score = model.score_samples(X_AUC)\n",
    "        y_score.dtype = np.float64\n",
    "        new_unweighted_validation_scores[:, i] = y_score.squeeze()\n",
    "    \n",
    "    \n",
    "    # (Re)Define the labels Y based on some feature of the data. Here to begin with \n",
    "    # this will be some subinterval for the scores from the first model.\n",
    "    \n",
    "    Y = np.empty((np.shape(X)[0],))\n",
    "    anom_interval_min = my_centre - (tau)\n",
    "    anom_interval_max = my_centre +  (tau)\n",
    "    for i in range(len(Y)):\n",
    "        Y[i] = 1*(anom_interval_min <=  new_unweighted_scores_fixed[i,0] <= anom_interval_max) \n",
    "    \n",
    "    Y_AUC = np.empty((np.shape(X_AUC)[0],))\n",
    "    anom_interval_min = my_centre - (tau)\n",
    "    anom_interval_max = my_centre +  (tau)\n",
    "    for i in range(len(Y_AUC)):\n",
    "        Y_AUC[i] = 1*(anom_interval_min <=  new_unweighted_validation_scores[i,0] <= anom_interval_max) \n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "        if curr_method == \"RandomScore\" and n_remaining_anomalies > 0:\n",
    "            related_scores = []\n",
    "            for jj in range(n_old):\n",
    "                if Y_muted[jj] == 1:\n",
    "                    related_scores = related_scores + [new_unweighted_scores_fixed[jj,n_models-1]]\n",
    "    \n",
    "    \n",
    "    ###################################################################################\n",
    "    #LODA\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "\n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "            new_unweighted_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index ,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                \n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "       \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    \n",
    "    #############################################################################################\n",
    "    #ACTIVE LODA\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau \n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "    \n",
    "    #Use the first n_min data-points in X to get the number and set of LODA projectors:\n",
    "    #models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "            new_unweighted_scores = new_unweighted_scores_fixed[:n_old,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "        \n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #print('There were no labeled anomalies in the old data')\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "                    \n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "\n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "\n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "    \n",
    "            if curr_method == \"RandomScore\":\n",
    "                H_A = new_unweighted_scores_fixed[:n_old,:]\n",
    "                H_A = H_A[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "            new_unweighted_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index ,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                \n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                new_unweighted_scores[:,i] = y_score.squeeze()            \n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "            \n",
    "        #Calculate the AUC for the current batch ONLY:\n",
    "        #active_LODA_AUC[r] = roc_auc_score(Y_new,temp_new_active_LODA_scores) \n",
    "        \n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        #We actually have to, at this point, attach the current versions of H_A\n",
    "        #and H_N to new_unweighted_scores, since in this batch framework, we do\n",
    "        #not have a fixed number of data points from the start to the finish, like\n",
    "        #they do in Das et al. (2016). If we do not do this, it will affect the\n",
    "        #calculation of q_tau over time (a kind of bias will be introduced, maybe\n",
    "        #not the end of the world, but still.)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "\n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "\n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "\n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            #print('next top index:',next_top_index)\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "                \n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "    \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "    ##########################################################################\n",
    "    #GLAD\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    model_GLAD.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    \n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "    \n",
    "            if curr_method == \"RandomScore\":\n",
    "                all_unweighted_scores = new_unweighted_scores_fixed[:np.shape(X_old)[0],:]\n",
    "            else:\n",
    "                for i, (name, model) in enumerate(models.items()):\n",
    "                    model.fit(X_old)\n",
    "                    y_score = model.score_samples(X_old)\n",
    "                    y_score.dtype = np.float64\n",
    "                    all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "                all_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "\n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]    \n",
    "    \n",
    "        # Next, compile the model with the second custom loss function:\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "        \n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "    \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab, np.array(Y_lab), epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "        \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_LODA_RS1 = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_RS1 = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_RS1 = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_RS1 = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_RS1 = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_RS1 = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_RS1.npz\",\n",
    "         avg_LODA_RS1 = avg_LODA_RS1,\n",
    "         avg_active_LODA_RS1 = avg_active_LODA_RS1,\n",
    "         avg_GLAD_RS1 = avg_GLAD_RS1,\n",
    "         avg_nY1_LODA_RS1 = avg_nY1_LODA_RS1,\n",
    "         avg_nY1_active_LODA_RS1 = avg_nY1_active_LODA_RS1,\n",
    "         avg_nY1_GLAD_RS1 = avg_nY1_GLAD_RS1,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_RS1.npz\",\n",
    "         weighted_scores_RS1 = weighted_scores,\n",
    "         weighted_validation_scores_RS1 = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_RS1 = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c2d9fe-ed10-44b7-8abc-039688fbed20",
   "metadata": {},
   "source": [
    "### Plot the AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04805411-a58f-4c43-ac8a-afe18485f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_RS1, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_RS1, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_RS1, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_RS1, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c1d54-3765-4272-8741-cbdc5569ef8f",
   "metadata": {},
   "source": [
    "### Plot cumulative anomalies detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95507df-a465-482f-a9a0-cde1d6f96802",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_RS1, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_RS1, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_RS1, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_RS1, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48964e5e-5702-4cf6-926c-7531d022f6c7",
   "metadata": {},
   "source": [
    "## The random Gaussian score way I\n",
    "\n",
    "Here we take each data point, we randomly assign it to nominal or anomaly (via the value of $tau$). We then create ten anomaly detectors, nine of which have uniform random score values, whilst the tenth's scores are generated from different Gaussian distributions depending on whether the relevant point has been assigned to be an anomaly or a nominal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c9aa7-e135-4849-9094-61a5724e4fba",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104f7400-aa87-4b39-a8a2-0d4d135e0da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d6ad8f-a010-4369-a620-eb8d03d3fe59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nominal_params = (0, 1)\n",
    "anomaly_params = (0, 0.01)\n",
    "\n",
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials)     \n",
    "    \n",
    "    #Set tau back to 0 for each loop, since we don't want a mixture in the original data now:\n",
    "    tau = 0\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[0.5,0.5]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "     \n",
    "    #RESET TAU TO ITS TRUE VALUE:\n",
    "    tau = 0.01\n",
    "    \n",
    "    curr_method = \"RandomScore\"\n",
    "    \n",
    "    best_m = n_models\n",
    "    \n",
    "    # Build models\n",
    "    models = build_custom_score_models(M=n_models, tau=tau, nominal_params=nominal_params, anomaly_params=anomaly_params)\n",
    "    \n",
    "    new_unweighted_scores_fixed = np.zeros((X.shape[0], len(models)))\n",
    "    \n",
    "    # Generate scores\n",
    "    i = -1\n",
    "    for name, model in models.items():\n",
    "        i = i + 1\n",
    "        model.fit(X)\n",
    "        if isinstance(model, MixtureGaussianScoreModel):\n",
    "            new_unweighted_scores_fixed[:, i], Y = model.score_samples(X)\n",
    "        else:\n",
    "            new_unweighted_scores_fixed[:, i] = model.score_samples(X)\n",
    "    \n",
    "    \n",
    "    new_unweighted_validation_scores = np.zeros((X_AUC.shape[0], len(models)))\n",
    "    \n",
    "    # Generate scores\n",
    "    i = -1\n",
    "    for name, model in models.items():\n",
    "        i = i + 1\n",
    "        model.fit(X_AUC)\n",
    "        if isinstance(model, MixtureGaussianScoreModel):\n",
    "            new_unweighted_validation_scores[:, i], Y_AUC = model.score_samples(X_AUC)\n",
    "        else:\n",
    "            new_unweighted_validation_scores[:, i] = model.score_samples(X_AUC)\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        #print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "        if curr_method == \"RandomScore\" and n_remaining_anomalies > 0:\n",
    "            related_scores = []\n",
    "            for jj in range(n_old):\n",
    "                if Y_muted[jj] == 1:\n",
    "                    related_scores = related_scores + [new_unweighted_scores_fixed[jj,n_models-1]]\n",
    "\n",
    "    ###############################################################################\n",
    "    # AAA method\n",
    "\n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'RandomForestClassifier'\n",
    "    \n",
    "    #Initialization\n",
    "    \n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "        # Run the initialization function InitActiveAGG_2:\n",
    "        if curr_method == \"RandomScore\":\n",
    "            X_lab, Y_lab, all_labeled_scores = InitActiveAGG_2(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models,curr_method=curr_method,now_scores = new_unweighted_scores_fixed[:n_old,:])\n",
    "        \n",
    "        else:\n",
    "            X_lab, Y_lab, all_labeled_scores = InitActiveAGG_2(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models,curr_method=curr_method)\n",
    "    \n",
    "    \n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        if curr_method == \"RandomScore\":\n",
    "            X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG_2(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.6,min_n_nom=5,min_n_anom=1,tau_exp=tau,curr_method = curr_method,now_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index,:])  \n",
    "        else:\n",
    "            X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG_2(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.6,min_n_nom=5,min_n_anom=1,tau_exp=tau,curr_method = curr_method)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        #new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:, 1]\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "\n",
    "# Calculate column averages for each array\n",
    "avg_AAA_RS2 = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_RS2 = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_RS2_onlyAAA.npz\",\n",
    "         avg_AAA_RS2 = avg_AAA_RS2,\n",
    "         avg_nY1_AAA_RS2 = avg_nY1_AAA_RS2,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_RS2_onlyAAA.npz\",\n",
    "         new_preds_RS2 = new_preds\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268aabf9-2b3f-4db3-bbf8-24da2e2e70aa",
   "metadata": {},
   "source": [
    "### The other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f49e19-dd26-458f-824d-af7bdbd57df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf76c7-1948-49f7-bea9-e907803ed23a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nominal_params = (0, 1)\n",
    "anomaly_params = (0, 0.01)\n",
    "\n",
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "    \n",
    "    #Set tau back to 0 for each loop, since we don't want a mixture in the original data now:\n",
    "    tau = 0\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[0.5,0.5]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    #RESET TAU TO ITS TRUE VALUE:\n",
    "    tau = 0.01\n",
    "    \n",
    "    curr_method = \"RandomScore\"\n",
    "    \n",
    "    best_m = n_models\n",
    "    \n",
    "    # Build models\n",
    "    models = build_custom_score_models(M=n_models, tau=tau, nominal_params=nominal_params, anomaly_params=anomaly_params)\n",
    "    \n",
    "    new_unweighted_scores_fixed = np.zeros((X.shape[0], len(models)))\n",
    "    \n",
    "    # Generate scores\n",
    "    i = -1\n",
    "    for name, model in models.items():\n",
    "        i = i + 1\n",
    "        model.fit(X)\n",
    "        if isinstance(model, MixtureGaussianScoreModel):\n",
    "            new_unweighted_scores_fixed[:, i], Y = model.score_samples(X)\n",
    "        else:\n",
    "            new_unweighted_scores_fixed[:, i] = model.score_samples(X)\n",
    "    \n",
    "    \n",
    "    new_unweighted_validation_scores = np.zeros((X_AUC.shape[0], len(models)))\n",
    "    \n",
    "    # Generate scores\n",
    "    i = -1\n",
    "    for name, model in models.items():\n",
    "        i = i + 1\n",
    "        model.fit(X_AUC)\n",
    "        if isinstance(model, MixtureGaussianScoreModel):\n",
    "            new_unweighted_validation_scores[:, i], Y_AUC = model.score_samples(X_AUC)\n",
    "        else:\n",
    "            new_unweighted_validation_scores[:, i] = model.score_samples(X_AUC)\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "        if curr_method == \"RandomScore\" and n_remaining_anomalies > 0:\n",
    "            related_scores = []\n",
    "            for jj in range(n_old):\n",
    "                if Y_muted[jj] == 1:\n",
    "                    related_scores = related_scores + [new_unweighted_scores_fixed[jj,n_models-1]]\n",
    "    \n",
    "    ###################################################################################\n",
    "    #LODA\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "            new_unweighted_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index ,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                \n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    \n",
    "    #############################################################################################\n",
    "    #ACTIVE LODA\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau \n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "    \n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "            new_unweighted_scores = new_unweighted_scores_fixed[:n_old,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "        \n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "        \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "\n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "\n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "    \n",
    "            if curr_method == \"RandomScore\":\n",
    "                H_A = new_unweighted_scores_fixed[:n_old,:]\n",
    "                H_A = H_A[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "            new_unweighted_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index ,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                \n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "\n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "        \n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "    \n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "                \n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "    \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "            \n",
    "    ##########################################################################\n",
    "    #GLAD\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    model_GLAD.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    \n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "    \n",
    "            if curr_method == \"RandomScore\":\n",
    "                all_unweighted_scores = new_unweighted_scores_fixed[:np.shape(X_old)[0],:]\n",
    "            else:\n",
    "                for i, (name, model) in enumerate(models.items()):\n",
    "                    model.fit(X_old)\n",
    "                    y_score = model.score_samples(X_old)\n",
    "                    y_score.dtype = np.float64\n",
    "                    all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "                all_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "        \n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]\n",
    "    \n",
    "        # Next, compile the model with the second custom loss function:\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "        \n",
    "        #model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss(X_lab, Y_lab, q_tau_tm1, all_labeled_scores,model_GLAD, X_so_far, mylambda, b), metrics=['accuracy'])\n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "    \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "          \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "\n",
    "# Calculate column averages for each array\n",
    "avg_LODA_RS2 = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_RS2 = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_RS2 = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_RS2 = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_RS2 = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_RS2 = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_RS2.npz\",\n",
    "         avg_LODA_RS2 = avg_LODA_RS2,\n",
    "         avg_active_LODA_RS2 = avg_active_LODA_RS2,\n",
    "         avg_GLAD_RS2 = avg_GLAD_RS2,\n",
    "         avg_nY1_LODA_RS2 = avg_nY1_LODA_RS2,\n",
    "         avg_nY1_active_LODA_RS2 = avg_nY1_active_LODA_RS2,\n",
    "         avg_nY1_GLAD_RS2 = avg_nY1_GLAD_RS2,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_RS2.npz\",\n",
    "         weighted_scores_RS2 = weighted_scores,\n",
    "         weighted_validation_scores_RS2 = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_RS2 = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5487f7c-4b83-4446-8e8d-dbbbdc2ca49f",
   "metadata": {},
   "source": [
    "### Plot the AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3439022-55c6-45ea-8c5a-e680eb5912cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_RS2, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_RS2, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_RS2, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_RS2, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64d4a6-da80-48c9-a976-899fe5336263",
   "metadata": {},
   "source": [
    "### Plot the cumulative anomalies detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329316f0-6f2f-4f79-b7b2-c79a457c1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_RS2, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_RS2, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_RS2, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_RS2, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c38fa-2d8d-4042-9404-b31bb2fbf9b6",
   "metadata": {},
   "source": [
    "## The random Gaussian score way II\n",
    "\n",
    "Here we take each data point, we randomly assign it to nominal or anomaly (via the value of $tau$). We then create ten anomaly detectors, nine of which have uniform random score values, whilst the tenth's scores are generated from different Gaussian distributions depending on whether the relevant point has been assigned to be an anomaly or a nominal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713082fa-de33-4277-9abf-873becade289",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af2b50-11ab-4166-b449-c336002356e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2d8e3-dcc2-4591-b09e-852d0a20e081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nominal_params = (0, 1)\n",
    "anomaly_params = (2.5, 0.01)\n",
    "\n",
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "    \n",
    "    #Set tau back to 0 for each loop, since we don't want a mixture in the original data now:\n",
    "    tau = 0\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[0.5,0.5]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #RESET TAU TO ITS TRUE VALUE:\n",
    "    tau = 0.01\n",
    "    \n",
    "    curr_method = \"RandomScore\"\n",
    "    \n",
    "    best_m = n_models\n",
    "    \n",
    "    # Build models\n",
    "    models = build_custom_score_models(M=n_models, tau=tau, nominal_params=nominal_params, anomaly_params=anomaly_params)\n",
    "    \n",
    "    new_unweighted_scores_fixed = np.zeros((X.shape[0], len(models)))\n",
    "    \n",
    "    # Generate scores\n",
    "    i = -1\n",
    "    for name, model in models.items():\n",
    "        i = i + 1\n",
    "        model.fit(X)\n",
    "        if isinstance(model, MixtureGaussianScoreModel):\n",
    "            new_unweighted_scores_fixed[:, i], Y = model.score_samples(X)\n",
    "        else:\n",
    "            new_unweighted_scores_fixed[:, i] = model.score_samples(X)\n",
    "    \n",
    "    \n",
    "    new_unweighted_validation_scores = np.zeros((X_AUC.shape[0], len(models)))\n",
    "    \n",
    "    # Generate scores\n",
    "    i = -1\n",
    "    for name, model in models.items():\n",
    "        i = i + 1\n",
    "        model.fit(X_AUC)\n",
    "        if isinstance(model, MixtureGaussianScoreModel):\n",
    "            new_unweighted_validation_scores[:, i], Y_AUC = model.score_samples(X_AUC)\n",
    "        else:\n",
    "            new_unweighted_validation_scores[:, i] = model.score_samples(X_AUC)\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "        if curr_method == \"RandomScore\" and n_remaining_anomalies > 0:\n",
    "            related_scores = []\n",
    "            for jj in range(n_old):\n",
    "                if Y_muted[jj] == 1:\n",
    "                    related_scores = related_scores + [new_unweighted_scores_fixed[jj,n_models-1]]\n",
    "    \n",
    "    ###############################################################################\n",
    "    # AAA method\n",
    "\n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'RandomForestClassifier'\n",
    "    \n",
    "    #Initialization\n",
    "    \n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "\n",
    "        # Run the initialization function InitActiveAGG_2:\n",
    "        if curr_method == \"RandomScore\":\n",
    "            X_lab, Y_lab, all_labeled_scores = InitActiveAGG_2(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models,curr_method=curr_method,now_scores = new_unweighted_scores_fixed[:n_old,:])\n",
    "        \n",
    "        else:\n",
    "            X_lab, Y_lab, all_labeled_scores = InitActiveAGG_2(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models,curr_method=curr_method)\n",
    "    \n",
    "    \n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        if curr_method == \"RandomScore\":\n",
    "            X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG_2(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.6,min_n_nom=5,min_n_anom=1,tau_exp=tau,curr_method = curr_method,now_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index,:])  \n",
    "        else:\n",
    "            X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG_2(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.6,min_n_nom=5,min_n_anom=1,tau_exp=tau,curr_method = curr_method)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        #new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:, 1]\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "\n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "\n",
    "# Calculate column averages for each array\n",
    "avg_AAA_RS2_2p5 = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_RS2_2p5 = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_RS2_2p5_onlyAAA.npz\",\n",
    "         avg_AAA_RS2_2p5 = avg_AAA_RS2_2p5,\n",
    "         avg_nY1_AAA_RS2_2p5 = avg_nY1_AAA_RS2_2p5,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_RS2_2p5_onlyAAA.npz\",\n",
    "         new_preds_RS2_2p5 = new_preds\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927416e-8ed6-40b7-ba2c-c2236af2d243",
   "metadata": {},
   "source": [
    "### The other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975d8b6-b1c9-43ae-8ae9-0e3220ce56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5a8b6-d457-43aa-97a5-ed0809a0dc84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nominal_params = (0, 1)\n",
    "anomaly_params = (2.5, 0.01)\n",
    "\n",
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "      \n",
    "    #Set tau back to 0 for each loop, since we don't want a mixture in the original data now:\n",
    "    tau = 0\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[0.5,0.5]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #RESET TAU TO ITS TRUE VALUE:\n",
    "    tau = 0.01\n",
    "    \n",
    "    curr_method = \"RandomScore\"\n",
    "    \n",
    "    best_m = n_models\n",
    "    \n",
    "    # Build models\n",
    "    models = build_custom_score_models(M=n_models, tau=tau, nominal_params=nominal_params, anomaly_params=anomaly_params)\n",
    "    \n",
    "    new_unweighted_scores_fixed = np.zeros((X.shape[0], len(models)))\n",
    "    \n",
    "    # Generate scores\n",
    "    i = -1\n",
    "    for name, model in models.items():\n",
    "        i = i + 1\n",
    "        model.fit(X)\n",
    "        if isinstance(model, MixtureGaussianScoreModel):\n",
    "            new_unweighted_scores_fixed[:, i], Y = model.score_samples(X)\n",
    "        else:\n",
    "            new_unweighted_scores_fixed[:, i] = model.score_samples(X)\n",
    "    \n",
    "    \n",
    "    new_unweighted_validation_scores = np.zeros((X_AUC.shape[0], len(models)))\n",
    "    \n",
    "    # Generate scores\n",
    "    i = -1\n",
    "    for name, model in models.items():\n",
    "        i = i + 1\n",
    "        model.fit(X_AUC)\n",
    "        if isinstance(model, MixtureGaussianScoreModel):\n",
    "            new_unweighted_validation_scores[:, i], Y_AUC = model.score_samples(X_AUC)\n",
    "        else:\n",
    "            new_unweighted_validation_scores[:, i] = model.score_samples(X_AUC)\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "    \n",
    "        if curr_method == \"RandomScore\" and n_remaining_anomalies > 0:\n",
    "            related_scores = []\n",
    "            for jj in range(n_old):\n",
    "                if Y_muted[jj] == 1:\n",
    "                    related_scores = related_scores + [new_unweighted_scores_fixed[jj,n_models-1]]\n",
    "    \n",
    "    ###################################################################################\n",
    "    #LODA\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "            new_unweighted_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index ,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                \n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "\n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    \n",
    "    #############################################################################################\n",
    "    #ACTIVE LODA\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau \n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "    \n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "            new_unweighted_scores = new_unweighted_scores_fixed[:n_old,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "        \n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #print('There were no labeled anomalies in the old data')\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "                    \n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "    \n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "    \n",
    "            if curr_method == \"RandomScore\":\n",
    "                H_A = new_unweighted_scores_fixed[:n_old,:]\n",
    "                H_A = H_A[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "\n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "            new_unweighted_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index ,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                \n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "\n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        #We actually have to, at this point, attach the current versions of H_A\n",
    "        #and H_N to new_unweighted_scores, since in this batch framework, we do\n",
    "        #not have a fixed number of data points from the start to the finish, like\n",
    "        #they do in Das et al. (2016). If we do not do this, it will affect the\n",
    "        #calculation of q_tau over time (a kind of bias will be introduced, maybe\n",
    "        #not the end of the world, but still.)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "        \n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "\n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            #print('next top index:',next_top_index)\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "                \n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "\n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "\n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "    ##########################################################################\n",
    "    #GLAD\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    model_GLAD.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    \n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "    \n",
    "            if curr_method == \"RandomScore\":\n",
    "                all_unweighted_scores = new_unweighted_scores_fixed[:np.shape(X_old)[0],:]\n",
    "            else:\n",
    "                for i, (name, model) in enumerate(models.items()):\n",
    "                    model.fit(X_old)\n",
    "                    y_score = model.score_samples(X_old)\n",
    "                    y_score.dtype = np.float64\n",
    "                    all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "                all_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "\n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        #print('top k indices:',top_k_indices)\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    " \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index] \n",
    "    \n",
    "        # Next, compile the model with the second custom loss function:\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "        \n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "    \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "        \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "    \n",
    "# Calculate column averages for each array\n",
    "avg_LODA_RS2_2p5 = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_RS2_2p5 = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_RS2_2p5 = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_RS2_2p5 = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_RS2_2p5 = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_RS2_2p5 = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_RS2_2p5.npz\",\n",
    "         avg_LODA_RS2_2p5 = avg_LODA_RS2_2p5,\n",
    "         avg_active_LODA_RS2_2p5 = avg_active_LODA_RS2_2p5,\n",
    "         avg_GLAD_RS2_2p5 = avg_GLAD_RS2_2p5,\n",
    "         avg_nY1_LODA_RS2_2p5 = avg_nY1_LODA_RS2_2p5,\n",
    "         avg_nY1_active_LODA_RS2_2p5 = avg_nY1_active_LODA_RS2_2p5,\n",
    "         avg_nY1_GLAD_RS2_2p5 = avg_nY1_GLAD_RS2_2p5,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_RS2_2p5.npz\",\n",
    "         weighted_scores_RS2_2p5 = weighted_scores,\n",
    "         weighted_validation_scores_RS2_2p5 = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_RS2_2p5 = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f57ffa-d62f-47de-81ad-e13760322152",
   "metadata": {},
   "source": [
    "### Plot the AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46d17c-92a9-4d77-907e-61280e0fe696",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_RS2_2p5, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_RS2_2p5, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_RS2_2p5, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_RS2_2p5, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ce831-606c-4269-9fa8-00bda83f62e9",
   "metadata": {},
   "source": [
    "### Plot the cumulative anomalies detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9c029-234b-4898-beb3-63da19419137",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_RS2_2p5, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_RS2_2p5, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_RS2_2p5, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_RS2_2p5, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40520e6f-da0b-4349-9217-6e5fc85a7a50",
   "metadata": {},
   "source": [
    "## The random Gaussian score way III\n",
    "\n",
    "Here we take each data point, we randomly assign it to nominal or anomaly (via the value of $tau$). We then create ten anomaly detectors, nine of which have uniform random score values, whilst the tenth's scores are generated from different Gaussian distributions depending on whether the relevant point has been assigned to be an anomaly or a nominal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe194fe-1334-4bd7-89a8-cdce16a00c51",
   "metadata": {},
   "source": [
    "### Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12391100-5048-4ad0-bf6c-88b14d9879bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b880af-6608-4aa8-97ff-6d5f57fdd4d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "    \n",
    "    #Set tau back to 0 for each loop, since we don't want a mixture in the original data now:\n",
    "    tau = 0\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[0.5,0.5]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #RESET TAU TO ITS TRUE VALUE:\n",
    "    tau = 0.01\n",
    "    \n",
    "    curr_method = \"RandomScore\"\n",
    "    \n",
    "    nominal_params = (0, 1)\n",
    "    anomaly_params_list = [(-2.5, 0.01), (2.5, 0.01)]  # Two anomaly distributions\n",
    "    best_m = n_models\n",
    "    \n",
    "    # Build models\n",
    "    models = build_custom_score_models2(\n",
    "        M=n_models, \n",
    "        tau=tau, \n",
    "        nominal_params=nominal_params, \n",
    "        anomaly_params_list=anomaly_params_list\n",
    "    )\n",
    "    \n",
    "    # Generate scores for training data\n",
    "    new_unweighted_scores_fixed = np.zeros((X.shape[0], len(models)))\n",
    "    \n",
    "    i = -1\n",
    "    for name, model in models.items():\n",
    "        i += 1\n",
    "        model.fit(X)\n",
    "        if isinstance(model, ExtendedMixtureGaussianScoreModel):\n",
    "            new_unweighted_scores_fixed[:, i], Y = model.score_samples(X)\n",
    "        else:\n",
    "            new_unweighted_scores_fixed[:, i] = model.score_samples(X)\n",
    "    \n",
    "    # Generate scores for validation data\n",
    "    new_unweighted_validation_scores = np.zeros((X_AUC.shape[0], len(models)))\n",
    "    \n",
    "    i = -1\n",
    "    for name, model in models.items():\n",
    "        i += 1\n",
    "        model.fit(X_AUC)\n",
    "        if isinstance(model, ExtendedMixtureGaussianScoreModel):\n",
    "            new_unweighted_validation_scores[:, i], Y_AUC = model.score_samples(X_AUC)\n",
    "        else:\n",
    "            new_unweighted_validation_scores[:, i] = model.score_samples(X_AUC)\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "\n",
    "    #Slightly modified below to ensure that there is one anomaly on each side, rather than 2 in total only:\n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        n_with_anomalies_L = 0\n",
    "        n_with_anomalies_R = 0\n",
    "        while (n_remaining_anomalies != 2) or (n_remaining_nominals != 98) or (n_with_anomalies_L != 1) or (n_with_anomalies_R != 1):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            \n",
    "            \n",
    "            #Get the score value corresponding to the unmuted anomalies:\n",
    "            score_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    score_with_anomalies.append(new_unweighted_scores_fixed[i,n_models-1])  # Use append instead of concatenation for efficiency\n",
    "\n",
    "            #print('score_with_anomalies:',score_with_anomalies)\n",
    "            if len(score_with_anomalies) > 0:\n",
    "                # Flatten the list of arrays into a single array\n",
    "                score_with_anomalies_array = np.vstack(score_with_anomalies)\n",
    "            \n",
    "                # Count negatives and positives across all elements\n",
    "                n_with_anomalies_L = np.sum(score_with_anomalies_array < 0)\n",
    "                n_with_anomalies_R = np.sum(score_with_anomalies_array > 0)\n",
    "            else:\n",
    "                n_with_anomalies_L = 0\n",
    "                n_with_anomalies_R = 0\n",
    "                \n",
    "        if curr_method == \"RandomScore\" and n_remaining_anomalies > 0:\n",
    "            related_scores = []\n",
    "            for jj in range(n_old):\n",
    "                if Y_muted[jj] == 1:\n",
    "                    related_scores = related_scores + [new_unweighted_scores_fixed[jj,n_models-1]]\n",
    "    \n",
    "    ###############################################################################\n",
    "    # AAA method\n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'RandomForestClassifier'\n",
    "    \n",
    "    #Initialization\n",
    "    \n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "        # Run the initialization function InitActiveAGG_2:\n",
    "        if curr_method == \"RandomScore\":\n",
    "            X_lab, Y_lab, all_labeled_scores = InitActiveAGG_2(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models,curr_method=curr_method,now_scores = new_unweighted_scores_fixed[:n_old,:])\n",
    "        \n",
    "        else:\n",
    "            X_lab, Y_lab, all_labeled_scores = InitActiveAGG_2(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models,curr_method=curr_method)\n",
    "    \n",
    "    \n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        if curr_method == \"RandomScore\":\n",
    "            X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG_2(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.6,min_n_nom=5,min_n_anom=1,tau_exp=tau,curr_method = curr_method,now_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index,:])  \n",
    "        else:\n",
    "            X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG_2(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.6,min_n_nom=5,min_n_anom=1,tau_exp=tau,curr_method = curr_method)  \n",
    "        \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        #new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:, 1]\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "            \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "\n",
    "# Calculate column averages for each array\n",
    "avg_AAA_RS3 = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_RS3 = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_RS3_onlyAAA.npz\",\n",
    "         avg_AAA_RS3 = avg_AAA_RS3,\n",
    "         avg_nY1_AAA_RS3 = avg_nY1_AAA_RS3,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_RS3_onlyAAA.npz\",\n",
    "         new_preds_RS3 = new_preds\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14cd6c-2011-4b87-b94c-9e40df3167be",
   "metadata": {},
   "source": [
    "### The other three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17272dca-2ed9-4e90-b37c-bf2724be25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890aed2-64d3-4799-9bc3-96fd25af577e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/2)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/2)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "\n",
    "    #Set tau back to 0 for each loop, since we don't want a mixture in the original data now:\n",
    "    tau = 0\n",
    "    \n",
    "    # Specific arguments:\n",
    "    a_list = [[0.5,0.5]]\n",
    "    anomaly_cov_list = [ \n",
    "        [[0.1,0],[0,0.1]]\n",
    "    ]\n",
    "    nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "    nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "    L = len(nominal_mean)\n",
    "    \n",
    "    # Sampling\n",
    "    X, Y = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=n_old,                                           # Initial number of data points\n",
    "        B=B,                                                   # Batch size\n",
    "        n_loops=n_loops,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    # Massive external data-set from the same distribution to look at AUC\n",
    "    X_AUC, Y_AUC = sample_data(\n",
    "        multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "        n_old=5000,                                           # Initial number of data points\n",
    "        B=0,                                                   # Batch size\n",
    "        n_loops=0,                                       # Number of batches\n",
    "        tau=tau,                                               # Fraction of anomalies\n",
    "        a_list=a_list,                                         # Anomaly means\n",
    "        anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "        nominal_mean=nominal_mean,                             # Nominal mean\n",
    "        nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "        L=L                                                    # Dimensionality\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #RESET TAU TO ITS TRUE VALUE:\n",
    "    tau = 0.01\n",
    "    \n",
    "    curr_method = \"RandomScore\"\n",
    "    \n",
    "    nominal_params = (0, 1)\n",
    "    anomaly_params_list = [(-2.5, 0.01), (2.5, 0.01)]  # Two anomaly distributions\n",
    "    best_m = n_models\n",
    "    \n",
    "    # Build models\n",
    "    models = build_custom_score_models2(\n",
    "        M=n_models, \n",
    "        tau=tau, \n",
    "        nominal_params=nominal_params, \n",
    "        anomaly_params_list=anomaly_params_list\n",
    "    )\n",
    "    \n",
    "    # Generate scores for training data\n",
    "    new_unweighted_scores_fixed = np.zeros((X.shape[0], len(models)))\n",
    "    \n",
    "    i = -1\n",
    "    for name, model in models.items():\n",
    "        i += 1\n",
    "        model.fit(X)\n",
    "        if isinstance(model, ExtendedMixtureGaussianScoreModel):\n",
    "            new_unweighted_scores_fixed[:, i], Y = model.score_samples(X)\n",
    "        else:\n",
    "            new_unweighted_scores_fixed[:, i] = model.score_samples(X)\n",
    "    \n",
    "    # Generate scores for validation data\n",
    "    new_unweighted_validation_scores = np.zeros((X_AUC.shape[0], len(models)))\n",
    "    \n",
    "    i = -1\n",
    "    for name, model in models.items():\n",
    "        i += 1\n",
    "        model.fit(X_AUC)\n",
    "        if isinstance(model, ExtendedMixtureGaussianScoreModel):\n",
    "            new_unweighted_validation_scores[:, i], Y_AUC = model.score_samples(X_AUC)\n",
    "        else:\n",
    "            new_unweighted_validation_scores[:, i] = model.score_samples(X_AUC)\n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "\n",
    "    #Slightly modified below to ensure that there is one anomaly on each side, rather than 2 in total only:\n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        n_with_anomalies_L = 0\n",
    "        n_with_anomalies_R = 0\n",
    "        while (n_remaining_anomalies != 2) or (n_remaining_nominals != 98) or (n_with_anomalies_L != 1) or (n_with_anomalies_R != 1):\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            \n",
    "            \n",
    "            #Get the score value corresponding to the unmuted anomalies:\n",
    "            score_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    score_with_anomalies.append(new_unweighted_scores_fixed[i,n_models-1])  # Use append instead of concatenation for efficiency\n",
    "\n",
    "            if len(score_with_anomalies) > 0:\n",
    "                # Flatten the list of arrays into a single array\n",
    "                score_with_anomalies_array = np.vstack(score_with_anomalies)\n",
    "            \n",
    "                # Count negatives and positives across all elements\n",
    "                n_with_anomalies_L = np.sum(score_with_anomalies_array < 0)\n",
    "                n_with_anomalies_R = np.sum(score_with_anomalies_array > 0)\n",
    "            else:\n",
    "                n_with_anomalies_L = 0\n",
    "                n_with_anomalies_R = 0\n",
    "    \n",
    "        if curr_method == \"RandomScore\" and n_remaining_anomalies > 0:\n",
    "            related_scores = []\n",
    "            for jj in range(n_old):\n",
    "                if Y_muted[jj] == 1:\n",
    "                    related_scores = related_scores + [new_unweighted_scores_fixed[jj,n_models-1]]\n",
    "    \n",
    "    ###################################################################################\n",
    "    #LODA\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "            new_unweighted_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index ,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                \n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "\n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    \n",
    "    #############################################################################################\n",
    "    #ACTIVE LODA\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau \n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "    \n",
    "    #Use the first n_min data-points in X to get the number and set of LODA projectors:\n",
    "    #models, best_m, scores = LODA_Choose_M(X[:min(n_min,n_old+B*n_loops),:],M_max=M_max,tau_M=tau_M)\n",
    "    \n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/best_m for i in range(best_m)])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, best_m))\n",
    "    H_N = np.empty((0, best_m))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,best_m))\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "            new_unweighted_scores = new_unweighted_scores_fixed[:n_old,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "        \n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "                    \n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "     \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "    \n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "    \n",
    "            if curr_method == \"RandomScore\":\n",
    "                H_A = new_unweighted_scores_fixed[:n_old,:]\n",
    "                H_A = H_A[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,best_m):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/2)):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,best_m))\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "            new_unweighted_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index ,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                \n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        #We actually have to, at this point, attach the current versions of H_A\n",
    "        #and H_N to new_unweighted_scores, since in this batch framework, we do\n",
    "        #not have a fixed number of data points from the start to the finish, like\n",
    "        #they do in Das et al. (2016). If we do not do this, it will affect the\n",
    "        #calculation of q_tau over time (a kind of bias will be introduced, maybe\n",
    "        #not the end of the world, but still.)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "        \n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "    \n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "\n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, best_m))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, best_m))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "                \n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "    \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "            \n",
    "    ##########################################################################\n",
    "    #GLAD\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], best_m)\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    model_GLAD.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, best_m), b)\n",
    "    \n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,best_m])\n",
    "        all_unweighted_scores = np.empty([0,best_m])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,best_m])\n",
    "            all_labeled_scores = np.empty([0,best_m])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],best_m])\n",
    "    \n",
    "            if curr_method == \"RandomScore\":\n",
    "                all_unweighted_scores = new_unweighted_scores_fixed[:np.shape(X_old)[0],:]\n",
    "            else:\n",
    "                for i, (name, model) in enumerate(models.items()):\n",
    "                    model.fit(X_old)\n",
    "                    y_score = model.score_samples(X_old)\n",
    "                    y_score.dtype = np.float64\n",
    "                    all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],best_m])\n",
    "    \n",
    "        if curr_method == \"RandomScore\":\n",
    "                all_scores = new_unweighted_scores_fixed[curr_L_index:curr_R_index,:]\n",
    "        else:\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_new)\n",
    "                y_score = model.score_samples(X_new)\n",
    "                y_score.dtype = np.float64\n",
    "                all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "\n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]\n",
    "    \n",
    "        # Next, compile the model with the second custom loss function:\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "        \n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "    \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "        \n",
    "    \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "\n",
    "# Calculate column averages for each array\n",
    "avg_LODA_RS3 = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_RS3 = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_RS3 = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_RS3 = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_RS3 = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_RS3 = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_RS3.npz\",\n",
    "         avg_LODA_RS3 = avg_LODA_RS3,\n",
    "         avg_active_LODA_RS3 = avg_active_LODA_RS3,\n",
    "         avg_GLAD_RS3 = avg_GLAD_RS3,\n",
    "         avg_nY1_LODA_RS3 = avg_nY1_LODA_RS3,\n",
    "         avg_nY1_active_LODA_RS3 = avg_nY1_active_LODA_RS3,\n",
    "         avg_nY1_GLAD_RS3 = avg_nY1_GLAD_RS3,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_RS3.npz\",\n",
    "         weighted_scores_RS3 = weighted_scores,\n",
    "         weighted_validation_scores_RS3 = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_RS3 = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d8436-a7f5-4136-a823-d99776a5f537",
   "metadata": {},
   "source": [
    "### Plot the AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11684531-01fd-44c0-956a-6d0b4ddac7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the column averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(xx,avg_LODA_RS3, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA,avg_active_LODA_RS3, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx,avg_GLAD_RS3, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx,avg_AAA_RS3, label=\"AAA\", marker='d')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Average AUC\", fontsize=14)\n",
    "\n",
    "plt.xticks(ticks=xx)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5902ebe9-021a-41d2-8191-960905336846",
   "metadata": {},
   "source": [
    "### Plot the cumulative anomalies detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884e5c1-dd43-425d-a1f2-5a9d519eaab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(1,n_loops+1))\n",
    "xxactiveLODA = list(range(1,int(n_loops/2)+1))\n",
    "\n",
    "# Plot the four lines\n",
    "plt.plot(xx, avg_nY1_LODA_RS3, label=\"LODA\", marker='o')\n",
    "plt.plot(xxactiveLODA, avg_nY1_active_LODA_RS3, label=\"Active-LODA\", marker='s')\n",
    "plt.plot(xx, avg_nY1_GLAD_RS3, label=\"GLAD\", marker='^')\n",
    "plt.plot(xx, avg_nY1_AAA_RS3, label=\"AAA\", marker='d')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Anomalies detected\")\n",
    "plt.title(\"Cumulative anomalies detected\")\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30951c-a83f-4f9b-aa3c-2363ab9430a7",
   "metadata": {},
   "source": [
    "## Bring back all saved variables and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f0fb9-484a-4cb2-8e46-2b2371405e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours\n",
    "data_RS1_onlyAAA = np.load(\"AUC_RS1_onlyAAA.npz\")\n",
    "avg_AAA_RS1 = data_RS1_onlyAAA[\"avg_AAA_RS1\"]\n",
    "avg_nY1_AAA_RS1 = data_RS1_onlyAAA[\"avg_nY1_AAA_RS1\"]\n",
    "# The others\n",
    "data_RS1 = np.load(\"AUC_RS1.npz\")\n",
    "avg_LODA_RS1 = data_RS1[\"avg_LODA_RS1\"]\n",
    "avg_active_LODA_RS1 = data_RS1[\"avg_active_LODA_RS1\"]\n",
    "avg_GLAD_RS1 = data_RS1[\"avg_GLAD_RS1\"]\n",
    "avg_nY1_LODA_RS1 = data_RS1[\"avg_nY1_LODA_RS1\"]\n",
    "avg_nY1_active_LODA_RS1 = data_RS1[\"avg_nY1_active_LODA_RS1\"]\n",
    "avg_nY1_GLAD_RS1 = data_RS1[\"avg_nY1_GLAD_RS1\"]\n",
    "\n",
    "# Ours\n",
    "data_RS2_onlyAAA = np.load(\"AUC_RS2_onlyAAA.npz\")\n",
    "avg_AAA_RS2 = data_RS2_onlyAAA[\"avg_AAA_RS2\"]\n",
    "avg_nY1_AAA_RS2 = data_RS2_onlyAAA[\"avg_nY1_AAA_RS2\"]\n",
    "# The others\n",
    "data_RS2 = np.load(\"AUC_RS2.npz\")\n",
    "avg_LODA_RS2 = data_RS2[\"avg_LODA_RS2\"]\n",
    "avg_active_LODA_RS2 = data_RS2[\"avg_active_LODA_RS2\"]\n",
    "avg_GLAD_RS2 = data_RS2[\"avg_GLAD_RS2\"]\n",
    "avg_nY1_LODA_RS2 = data_RS2[\"avg_nY1_LODA_RS2\"]\n",
    "avg_nY1_active_LODA_RS2 = data_RS2[\"avg_nY1_active_LODA_RS2\"]\n",
    "avg_nY1_GLAD_RS2 = data_RS2[\"avg_nY1_GLAD_RS2\"]\n",
    "\n",
    "# Ours\n",
    "data_RS2_2p5_onlyAAA = np.load(\"AUC_RS2_2p5_onlyAAA.npz\")\n",
    "avg_AAA_RS2_2p5 = data_RS2_2p5_onlyAAA[\"avg_AAA_RS2_2p5\"]\n",
    "avg_nY1_AAA_RS2_2p5 = data_RS2_2p5_onlyAAA[\"avg_nY1_AAA_RS2_2p5\"]\n",
    "# The others\n",
    "data_RS2_2p5 = np.load(\"AUC_RS2_2p5.npz\")\n",
    "avg_LODA_RS2_2p5 = data_RS2_2p5[\"avg_LODA_RS2_2p5\"]\n",
    "avg_active_LODA_RS2_2p5 = data_RS2_2p5[\"avg_active_LODA_RS2_2p5\"]\n",
    "avg_GLAD_RS2_2p5 = data_RS2_2p5[\"avg_GLAD_RS2_2p5\"]\n",
    "avg_nY1_LODA_RS2_2p5 = data_RS2_2p5[\"avg_nY1_LODA_RS2_2p5\"]\n",
    "avg_nY1_active_LODA_RS2_2p5 = data_RS2_2p5[\"avg_nY1_active_LODA_RS2_2p5\"]\n",
    "avg_nY1_GLAD_RS2_2p5 = data_RS2_2p5[\"avg_nY1_GLAD_RS2_2p5\"]\n",
    "\n",
    "# Ours\n",
    "data_RS3_onlyAAA = np.load(\"AUC_RS3_onlyAAA.npz\")\n",
    "avg_AAA_RS3 = data_RS3_onlyAAA[\"avg_AAA_RS3\"]\n",
    "avg_nY1_AAA_RS3 = data_RS3_onlyAAA[\"avg_nY1_AAA_RS3\"]\n",
    "# The others\n",
    "data_RS3 = np.load(\"AUC_RS3.npz\")\n",
    "avg_LODA_RS3 = data_RS3[\"avg_LODA_RS3\"]\n",
    "avg_active_LODA_RS3 = data_RS3[\"avg_active_LODA_RS3\"]\n",
    "avg_GLAD_RS3 = data_RS3[\"avg_GLAD_RS3\"]\n",
    "avg_nY1_LODA_RS3 = data_RS3[\"avg_nY1_LODA_RS3\"]\n",
    "avg_nY1_active_LODA_RS3 = data_RS3[\"avg_nY1_active_LODA_RS3\"]\n",
    "avg_nY1_GLAD_RS3 = data_RS3[\"avg_nY1_GLAD_RS3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434aaca-3d13-46f4-9c21-692c88709e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the subplot grid (3 rows, 2 columns)\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "#axes = axes.flatten()\n",
    "\n",
    "################################################################################\n",
    "## PLOTS\n",
    "\n",
    "my_centre = 0.5\n",
    "#Set tau back to 0 for each loop, since we don't want a mixture in the original data now:\n",
    "tau = 0\n",
    "\n",
    "# Specific arguments:\n",
    "a_list = [[0.5,0.5]]\n",
    "anomaly_cov_list = [ \n",
    "    [[0.1,0],[0,0.1]]\n",
    "]\n",
    "nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "L = len(nominal_mean)\n",
    "\n",
    "# Sampling\n",
    "X, Y = sample_data(\n",
    "    multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "    n_old=n_old,                                           # Initial number of data points\n",
    "    B=B,                                                   # Batch size\n",
    "    n_loops=n_loops,                                       # Number of batches\n",
    "    tau=tau,                                               # Fraction of anomalies\n",
    "    a_list=a_list,                                         # Anomaly means\n",
    "    anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "    nominal_mean=nominal_mean,                             # Nominal mean\n",
    "    nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "    L=L                                                    # Dimensionality\n",
    ")\n",
    "\n",
    "\n",
    "#RESET TAU TO ITS TRUE VALUE:\n",
    "tau = 0.01\n",
    "\n",
    "curr_method = \"RandomScore\"\n",
    "\n",
    "# Build the models dictionary\n",
    "models = build_random_score_models(M=n_models)\n",
    "best_m = n_models\n",
    "\n",
    "new_unweighted_scores_fixed = np.zeros((X.shape[0], len(models)))\n",
    "    \n",
    "# Iterate through the models and get their scores\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    model.fit(X)\n",
    "    y_score = model.score_samples(X)\n",
    "    y_score.dtype = np.float64\n",
    "    new_unweighted_scores_fixed[:, i] = y_score.squeeze()\n",
    "\n",
    "# (Re)Define the labels Y based on some feature of the data. Here to begin with \n",
    "# this will be some subinterval for the scores from the first model.\n",
    "\n",
    "Y = np.empty((np.shape(X)[0],))\n",
    "anom_interval_min = my_centre - (tau)\n",
    "anom_interval_max = my_centre +  (tau)\n",
    "for i in range(len(Y)):\n",
    "    Y[i] = 1*(anom_interval_min <=  new_unweighted_scores_fixed[i,0] <= anom_interval_max) \n",
    "\n",
    "first_column_scores = new_unweighted_scores_fixed[:,0]\n",
    "\n",
    "# Separate the scores based on their labels\n",
    "nominal_scores = first_column_scores[Y == 0]\n",
    "anomaly_scores = first_column_scores[Y == 1]\n",
    "\n",
    "# Calculate the number of nominal and anomaly points\n",
    "num_nominal = len(nominal_scores)\n",
    "num_anomaly = len(anomaly_scores)\n",
    "\n",
    "# Create a range of values for plotting\n",
    "x_range = np.linspace(min(first_column_scores), max(first_column_scores), 1000).reshape(-1, 1)\n",
    "\n",
    "# Fit KDE for nominal and anomaly scores\n",
    "kde_nominal = KernelDensity(kernel='gaussian', bandwidth=0.005).fit(nominal_scores.reshape(-1, 1))\n",
    "kde_anomaly = KernelDensity(kernel='gaussian', bandwidth=0.005).fit(anomaly_scores.reshape(-1, 1))\n",
    "\n",
    "# Compute densities for the range of x values\n",
    "nominal_density = np.exp(kde_nominal.score_samples(x_range))\n",
    "anomaly_density = np.exp(kde_anomaly.score_samples(x_range))\n",
    "\n",
    "# Compute the mixture model (weighted sum of the two densities)\n",
    "mixture_density = (num_nominal / len(first_column_scores)) * nominal_density + \\\n",
    "                  (num_anomaly / len(first_column_scores)) * anomaly_density\n",
    "\n",
    "# Plot the densities\n",
    "ax = axes[0, 0]\n",
    "\n",
    "# Plot the Nominal distribution\n",
    "ax.plot(x_range, nominal_density, label='Nominal Class (Y=0)', color='blue', linestyle='--')\n",
    "\n",
    "# Plot the Anomaly distribution\n",
    "ax.plot(x_range, anomaly_density, label='Anomaly Class (Y=1)', color='red', linestyle='--')\n",
    "\n",
    "# Plot the Mixture model\n",
    "#ax.plot(x_range, mixture_density, label='Mixture Model', color='black', linewidth=2)\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Score', fontsize=15)\n",
    "ax.set_ylabel('Density', fontsize=15)\n",
    "ax.set_title('Score densities', fontsize=17)\n",
    "ax.grid(True)\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "    Line2D([0], [0], color='red', lw=2, label=\"Anomalies\"),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "## Second method:\n",
    "nominal_params = (0, 1)\n",
    "anomaly_params = (0, 0.05)\n",
    "### WARNING: The true anomaly_params are (0, 0.01) but it makes for a fugly plot,\n",
    "### so instead here we set the variance to 0.05 instead for visual purposes.\n",
    "\n",
    "#Set tau back to 0 for each loop, since we don't want a mixture in the original data now:\n",
    "tau = 0\n",
    "\n",
    "# Specific arguments:\n",
    "a_list = [[0.5,0.5]]\n",
    "anomaly_cov_list = [ \n",
    "    [[0.1,0],[0,0.1]]\n",
    "]\n",
    "nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "L = len(nominal_mean)\n",
    "\n",
    "# Sampling\n",
    "X, Y = sample_data(\n",
    "    multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "    n_old=n_old,                                           # Initial number of data points\n",
    "    B=B,                                                   # Batch size\n",
    "    n_loops=n_loops,                                       # Number of batches\n",
    "    tau=tau,                                               # Fraction of anomalies\n",
    "    a_list=a_list,                                         # Anomaly means\n",
    "    anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "    nominal_mean=nominal_mean,                             # Nominal mean\n",
    "    nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "    L=L                                                    # Dimensionality\n",
    ")\n",
    "\n",
    "#RESET TAU TO ITS TRUE VALUE:\n",
    "tau = 0.01\n",
    "\n",
    "curr_method = \"RandomScore\"\n",
    "\n",
    "best_m = n_models\n",
    "\n",
    "# Build models\n",
    "models = build_custom_score_models(M=n_models, tau=tau, nominal_params=nominal_params, anomaly_params=anomaly_params)\n",
    "\n",
    "new_unweighted_scores_fixed = np.zeros((X.shape[0], len(models)))\n",
    "\n",
    "# Generate scores\n",
    "i = -1\n",
    "for name, model in models.items():\n",
    "    i = i + 1\n",
    "    model.fit(X)\n",
    "    if isinstance(model, MixtureGaussianScoreModel):\n",
    "        new_unweighted_scores_fixed[:, i], Y = model.score_samples(X)\n",
    "    else:\n",
    "        new_unweighted_scores_fixed[:, i] = model.score_samples(X)\n",
    "\n",
    "ax = axes[1, 0]\n",
    "\n",
    "# Visual verification of densities:\n",
    "last_column_scores = new_unweighted_scores_fixed[:, -1]\n",
    "\n",
    "# Separate the scores based on their labels\n",
    "nominal_scores = last_column_scores[Y == 0]\n",
    "anomaly_scores = last_column_scores[Y == 1]\n",
    "\n",
    "# Number of points in each class\n",
    "num_nominal = len(nominal_scores)\n",
    "num_anomaly = len(anomaly_scores)\n",
    "\n",
    "# Create a range of values for plotting\n",
    "x_range = np.linspace(min(last_column_scores), max(last_column_scores), 1000)\n",
    "\n",
    "# Create the two distributions (Nominal and Anomaly)\n",
    "nominal_dist = norm.pdf(x_range, nominal_params[0], nominal_params[1])\n",
    "anomaly_dist = norm.pdf(x_range, anomaly_params[0], anomaly_params[1])\n",
    "\n",
    "# Compute the mixture model (weighted sum of the two distributions)\n",
    "mixture_dist = (1 - tau) * nominal_dist + tau * anomaly_dist\n",
    "\n",
    "\n",
    "# Plot the Nominal distribution\n",
    "ax.plot(x_range, nominal_dist, label='Nominal Class (Y=0)', color='blue', linestyle='--')\n",
    "\n",
    "# Plot the Anomaly distribution\n",
    "ax.plot(x_range, anomaly_dist, label='Anomaly Class (Y=1)', color='red', linestyle='--')\n",
    "\n",
    "# Plot the Mixture model\n",
    "#ax.plot(x_range, mixture_dist, label='Mixture Model', color='black', linewidth=2)\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Score', fontsize=15)\n",
    "ax.set_ylabel('Density', fontsize=15)\n",
    "ax.set_title('Score densities', fontsize=17)\n",
    "ax.legend(loc=\"best\")\n",
    "ax.grid(True)\n",
    "\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "    Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=12)\n",
    "\n",
    "#####################################################################################################\n",
    "## Third method\n",
    "\n",
    "nominal_params = (0, 1)\n",
    "anomaly_params = (2.5, 0.05)\n",
    "### WARNING: The true anomaly_params are (0, 0.01) but it makes for a fugly plot,\n",
    "### so instead here we set the variance to 0.05 instead for visual purposes.\n",
    "\n",
    "#Set tau back to 0 for each loop, since we don't want a mixture in the original data now:\n",
    "tau = 0\n",
    "\n",
    "# Specific arguments:\n",
    "a_list = [[0.5,0.5]]\n",
    "anomaly_cov_list = [ \n",
    "    [[0.1,0],[0,0.1]]\n",
    "]\n",
    "nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "L = len(nominal_mean)\n",
    "\n",
    "# Sampling\n",
    "X, Y = sample_data(\n",
    "    multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "    n_old=n_old,                                           # Initial number of data points\n",
    "    B=B,                                                   # Batch size\n",
    "    n_loops=n_loops,                                       # Number of batches\n",
    "    tau=tau,                                               # Fraction of anomalies\n",
    "    a_list=a_list,                                         # Anomaly means\n",
    "    anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "    nominal_mean=nominal_mean,                             # Nominal mean\n",
    "    nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "    L=L                                                    # Dimensionality\n",
    ")\n",
    "\n",
    "#RESET TAU TO ITS TRUE VALUE:\n",
    "tau = 0.01\n",
    "\n",
    "curr_method = \"RandomScore\"\n",
    "\n",
    "best_m = n_models\n",
    "\n",
    "# Build models\n",
    "models = build_custom_score_models(M=n_models, tau=tau, nominal_params=nominal_params, anomaly_params=anomaly_params)\n",
    "\n",
    "new_unweighted_scores_fixed = np.zeros((X.shape[0], len(models)))\n",
    "\n",
    "# Generate scores\n",
    "i = -1\n",
    "for name, model in models.items():\n",
    "    i = i + 1\n",
    "    model.fit(X)\n",
    "    if isinstance(model, MixtureGaussianScoreModel):\n",
    "        new_unweighted_scores_fixed[:, i], Y = model.score_samples(X)\n",
    "    else:\n",
    "        new_unweighted_scores_fixed[:, i] = model.score_samples(X)\n",
    "\n",
    "\n",
    "\n",
    "ax = axes[2, 0]\n",
    "\n",
    "# Visual verification of densities:\n",
    "last_column_scores = new_unweighted_scores_fixed[:, -1]\n",
    "\n",
    "# Separate the scores based on their labels\n",
    "nominal_scores = last_column_scores[Y == 0]\n",
    "anomaly_scores = last_column_scores[Y == 1]\n",
    "\n",
    "# Number of points in each class\n",
    "num_nominal = len(nominal_scores)\n",
    "num_anomaly = len(anomaly_scores)\n",
    "\n",
    "# Create a range of values for plotting\n",
    "x_range = np.linspace(min(last_column_scores), max(last_column_scores), 1000)\n",
    "\n",
    "# Create the two distributions (Nominal and Anomaly)\n",
    "nominal_dist = norm.pdf(x_range, nominal_params[0], nominal_params[1])\n",
    "anomaly_dist = norm.pdf(x_range, anomaly_params[0], anomaly_params[1])\n",
    "\n",
    "# Compute the mixture model (weighted sum of the two distributions)\n",
    "mixture_dist = (1 - tau) * nominal_dist + tau * anomaly_dist\n",
    "\n",
    "\n",
    "# Plot the Nominal distribution\n",
    "ax.plot(x_range, nominal_dist, label='Nominal Class (Y=0)', color='blue', linestyle='--')\n",
    "\n",
    "# Plot the Anomaly distribution\n",
    "ax.plot(x_range, anomaly_dist, label='Anomaly Class (Y=1)', color='red', linestyle='--')\n",
    "\n",
    "# Plot the Mixture model\n",
    "#ax.plot(x_range, mixture_dist, label='Mixture Model', color='black', linewidth=2)\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Score', fontsize=15)\n",
    "ax.set_ylabel('Density', fontsize=15)\n",
    "ax.set_title('Score densities', fontsize=17)\n",
    "ax.legend(loc=\"best\")\n",
    "ax.grid(True)\n",
    "\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "    Line2D([0], [0], color='red', lw=2, label=\"Anomalies\")\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='center left', fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "## Fourth method\n",
    "\n",
    "nominal_params = (0, 1)\n",
    "anomaly_params_list = [(-2.5, 0.05), (2.5, 0.05)]  # Two anomaly distributions\n",
    "### WARNING: The true anomaly_params are  have variance 0.01 but it makes for a fugly plot,\n",
    "### so instead here we set the variance to 0.05 instead for visual purposes\n",
    "\n",
    "\n",
    "#Set tau back to 0 for each loop, since we don't want a mixture in the original data now:\n",
    "tau = 0\n",
    "\n",
    "# Specific arguments:\n",
    "a_list = [[0.5,0.5]]\n",
    "anomaly_cov_list = [ \n",
    "    [[0.1,0],[0,0.1]]\n",
    "]\n",
    "nominal_mean = np.array([0,0])    # Mean of the nominal Gaussian distribution\n",
    "nominal_cov = np.array([[1,0],[0,1]])   # Covariance of the nominal Gaussian distribution\n",
    "L = len(nominal_mean)\n",
    "\n",
    "# Sampling\n",
    "X, Y = sample_data(\n",
    "    multivariate_gaussian_sampling_with_anomaly_gaussians, # Sampling scheme\n",
    "    n_old=n_old,                                           # Initial number of data points\n",
    "    B=B,                                                   # Batch size\n",
    "    n_loops=n_loops,                                       # Number of batches\n",
    "    tau=tau,                                               # Fraction of anomalies\n",
    "    a_list=a_list,                                         # Anomaly means\n",
    "    anomaly_cov_list=anomaly_cov_list,                     # Anomaly covariance matrices\n",
    "    nominal_mean=nominal_mean,                             # Nominal mean\n",
    "    nominal_cov=nominal_cov,                               # Nominal covariance matrix\n",
    "    L=L                                                    # Dimensionality\n",
    ")\n",
    "\n",
    "#RESET TAU TO ITS TRUE VALUE:\n",
    "tau = 0.01\n",
    "\n",
    "curr_method = \"RandomScore\"\n",
    "\n",
    "best_m = n_models\n",
    "\n",
    "\n",
    "# Build models\n",
    "models = build_custom_score_models2(\n",
    "    M=n_models, \n",
    "    tau=tau, \n",
    "    nominal_params=nominal_params, \n",
    "    anomaly_params_list=anomaly_params_list\n",
    ")\n",
    "\n",
    "# Generate scores for training data\n",
    "new_unweighted_scores_fixed = np.zeros((X.shape[0], len(models)))\n",
    "\n",
    "i = -1\n",
    "for name, model in models.items():\n",
    "    i += 1\n",
    "    model.fit(X)\n",
    "    if isinstance(model, ExtendedMixtureGaussianScoreModel):\n",
    "        new_unweighted_scores_fixed[:, i], Y = model.score_samples(X)\n",
    "    else:\n",
    "        new_unweighted_scores_fixed[:, i] = model.score_samples(X)\n",
    "\n",
    "\n",
    "\n",
    "ax = axes[3, 0]\n",
    "\n",
    "last_column_scores = new_unweighted_scores_fixed[:, -1]\n",
    "\n",
    "# Separate the scores based on their labels\n",
    "nominal_scores = last_column_scores[Y == 0]\n",
    "anomaly_scores = last_column_scores[Y == 1]\n",
    "\n",
    "# Number of points in each class\n",
    "num_nominal = len(nominal_scores)\n",
    "num_anomaly = len(anomaly_scores)\n",
    "\n",
    "# Create a range of values for plotting\n",
    "x_range = np.linspace(min(last_column_scores), max(last_column_scores), 1000)\n",
    "\n",
    "# Create the nominal distribution\n",
    "nominal_dist = norm.pdf(x_range, nominal_params[0], nominal_params[1])\n",
    "\n",
    "# Create the two anomaly distributions\n",
    "anomaly_dist_1 = norm.pdf(x_range, anomaly_params_list[0][0], anomaly_params_list[0][1])\n",
    "anomaly_dist_2 = norm.pdf(x_range, anomaly_params_list[1][0], anomaly_params_list[1][1])\n",
    "\n",
    "# Compute the mixture model (weighted sum of the distributions)\n",
    "mixture_dist = (\n",
    "    (1 - tau) * nominal_dist +\n",
    "    (tau / 2) * anomaly_dist_1 +\n",
    "    (tau / 2) * anomaly_dist_2\n",
    ")\n",
    "\n",
    "\n",
    "# Plot the Nominal distribution\n",
    "ax.plot(x_range, nominal_dist, label='Nominal Class (Y=0)', color='blue', linestyle='--')\n",
    "\n",
    "# Plot the Anomaly distributions\n",
    "ax.plot(x_range, anomaly_dist_1, label='Anomaly Class 1 (Y=1)', color='red', linestyle='--')\n",
    "ax.plot(x_range, anomaly_dist_2, label='Anomaly Class 2 (Y=1)', color='green', linestyle='--')\n",
    "\n",
    "# Plot the Mixture model\n",
    "#ax.plot(x_range, mixture_dist, label='Mixture Model', color='black', linewidth=2)\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Score', fontsize=15)\n",
    "ax.set_ylabel('Density', fontsize=15)\n",
    "ax.set_title('Score densities', fontsize=17)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='blue', lw=2, label=\"Nominals\"),\n",
    "    Line2D([0], [0], color='red', lw=2, label=\"Anomalies class 1\"),\n",
    "    Line2D([0], [0], color='green', lw=2, label=\"Anomalies class 2\")\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=12)\n",
    "\n",
    "\n",
    "######################################################################################################\n",
    "#### SECOND COLUMN ###################################################################################\n",
    "######################################################################################################\n",
    "# Add the AUC subplots too:\n",
    "\n",
    "xx = list(range(1, n_loops + 1))\n",
    "xxactiveLODA = list(range(1, int(n_loops/2) + 1))\n",
    "\n",
    "\n",
    "# Iterate through the sets of arrays for AUC plots\n",
    "for i, (avg_LODA_plot, avg_active_LODA_plot, avg_GLAD_plot, avg_AAA_plot) in enumerate([\n",
    "    (avg_LODA_RS1, avg_active_LODA_RS1, avg_GLAD_RS1, avg_AAA_RS1),\n",
    "    (avg_LODA_RS2, avg_active_LODA_RS2, avg_GLAD_RS2, avg_AAA_RS2),\n",
    "    (avg_LODA_RS2_2p5, avg_active_LODA_RS2_2p5, avg_GLAD_RS2_2p5, avg_AAA_RS2_2p5),\n",
    "    (avg_LODA_RS3, avg_active_LODA_RS3, avg_GLAD_RS3, avg_AAA_RS3)\n",
    "]):\n",
    "    # Target subplot in the second column (index 1 for columns)\n",
    "    ax = axes[i, 1]\n",
    "    \n",
    "    # Plot the data on the subplot\n",
    "    ax.plot(xx, avg_LODA_plot, label=\"LODA\",linewidth=1.5)\n",
    "    ax.plot(xxactiveLODA, avg_active_LODA_plot, label=\"Active-LODA\",linewidth=1.5)\n",
    "    ax.plot(xx, avg_GLAD_plot, label=\"GLAD\",linewidth=1.5)\n",
    "    ax.plot(xx, avg_AAA_plot, label=\"AAA\",linewidth=1.5)\n",
    "    \n",
    "    # Add labels and grid\n",
    "    ax.set_xlabel(\"Batch\", fontsize=15)\n",
    "    ax.set_ylabel(\"Average AUC\", fontsize=15)\n",
    "    #ax.set_xticks(xx)\n",
    "    #ax.set_xticks(range(5, n_loops + 1, 5))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(loc=\"best\", fontsize=13)\n",
    "    \n",
    "    # Optional: Add a title for each subplot\n",
    "    ax.set_title(\"Average AUC over time\", fontsize=17)\n",
    "\n",
    "###################################\n",
    "## THIRD COLUMN ###################\n",
    "###################################\n",
    "\n",
    "# Third column: Cumulative anomaly detection plots using RS* data\n",
    "for i, (avg_nY1_LODA_plot, avg_nY1_active_LODA_plot, avg_nY1_GLAD_plot, avg_nY1_AAA_plot) in enumerate([\n",
    "    (avg_nY1_LODA_RS1, avg_nY1_active_LODA_RS1, avg_nY1_GLAD_RS1, avg_nY1_AAA_RS1),\n",
    "    (avg_nY1_LODA_RS2, avg_nY1_active_LODA_RS2, avg_nY1_GLAD_RS2, avg_nY1_AAA_RS2),\n",
    "    (avg_nY1_LODA_RS2_2p5, avg_nY1_active_LODA_RS2_2p5, avg_nY1_GLAD_RS2_2p5, avg_nY1_AAA_RS2_2p5),\n",
    "    (avg_nY1_LODA_RS3, avg_nY1_active_LODA_RS3, avg_nY1_GLAD_RS3, avg_nY1_AAA_RS3),\n",
    "]):\n",
    "    ax = axes[i, 2]\n",
    "\n",
    "    ax.plot(xx, avg_nY1_LODA_plot, label=\"LODA\", linewidth=1.5, linestyle='-')\n",
    "    ax.plot(xxactiveLODA, avg_nY1_active_LODA_plot, label=\"Active-LODA\", linewidth=1.5, linestyle=':')\n",
    "    ax.plot(xx, avg_nY1_GLAD_plot, label=\"GLAD\", linewidth=1.5, linestyle='-.')\n",
    "    ax.plot(xx, avg_nY1_AAA_plot, label=\"AAA\", linewidth=1.5, linestyle='-')\n",
    "\n",
    "    ax.set_xlabel(\"Batch\", fontsize=15)\n",
    "    ax.set_ylabel(\"Anomalies detected\", fontsize=15)\n",
    "    ax.set_title(\"Cumul. anomalies detected\", fontsize=17)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend(fontsize=13)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot:\n",
    "fig.savefig(\"Score_Scenarios2.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6fe7e-cf01-4014-8013-e9ea099d4e40",
   "metadata": {},
   "source": [
    "# Time series data trials\n",
    "\n",
    "Here we generate time series data and then perform anomaly detection on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3b433-598b-4dba-81a8-8d3cf05ccb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine tau if necessary:\n",
    "my_tau = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce5701e-acff-469a-809c-bca55553d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARAMETERS ##\n",
    "\n",
    "#Batch size\n",
    "B = 500\n",
    "\n",
    "#number of loops:\n",
    "n_loops=200\n",
    "\n",
    "# number of trials\n",
    "n_trials = 5\n",
    "\n",
    "# percentage to not hide:\n",
    "u = 0.2\n",
    "\n",
    "# MAKE THE INITIAL DATA SAME SIZE AS BATCHES:\n",
    "n_old = B\n",
    "\n",
    "#tau\n",
    "my_tau = 0.01\n",
    "\n",
    "#Number of dimensions:\n",
    "my_d = 10\n",
    "#Number of data points:\n",
    "n = n_old + B*n_loops\n",
    "#Number of validation points (must be a multiple of B):\n",
    "n_AUC = B*n_loops\n",
    "#Number of nearest neighbors in the LOF method:\n",
    "n_neighbors = int(np.ceil(B*my_tau))\n",
    "mean_G1=np.full(my_d, 5) \n",
    "mean_G2=np.full(my_d, 5.5) \n",
    "mean_G3=np.full(my_d, 6)\n",
    "c_anom=0.01\n",
    "#Number of included LODA models:\n",
    "n_LODA_models = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921abdf6-7e80-4142-bbdb-3ae3bb84e747",
   "metadata": {},
   "source": [
    "### Only our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f1358-2a66-422c-b0f2-3433851ead3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e137ec-ab19-49bd-9f62-a4b07f53cd76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_AAA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_nY1_AAA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "    \n",
    "    # Generate the data\n",
    "    X, Y = generate_time_series(my_tau=my_tau, d=my_d, n = n,mean_G1=mean_G1, mean_G2=mean_G2, mean_G3=mean_G3,c_anom = c_anom)\n",
    "    X_AUC, Y_AUC = generate_time_series(my_tau=my_tau, d=my_d, n = n_AUC,mean_G1=mean_G1, mean_G2=mean_G2, mean_G3=mean_G3,c_anom = c_anom)   \n",
    "    \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "\n",
    "    #Need at least 2 anomalies in the old data for these simulations to function\n",
    "    if np.sum(Y[:n_old] == 1) < 2:\n",
    "        raise ValueError(\"There are not at least two true anomalies in the old data.\")\n",
    "    \n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "        #while myplay==0:\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "\n",
    "\n",
    "    # Define additional models to include\n",
    "    extra_models = {\n",
    "        \"IsolationForest\": IsolationForest(),\n",
    "        \"OneClassSVM\": OneClassSVM(),\n",
    "        \"EuclideanDifferenceAnomalyDetector\": EuclideanDifferenceAnomalyDetector(),\n",
    "        \"LocalOutlierFactor\": LocalOutlierFactor(novelty=True),\n",
    "        \"RandomScore\": RandomScore(),\n",
    "    }\n",
    "    \n",
    "    # Create all models (including the LODA models and additional ones)\n",
    "    models = Create_Anomaly_Models(my_d, n_LODA_models=n_LODA_models, additional_models=extra_models)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set. Here, to be more precise,\n",
    "    #since we now have score functions that are contextual per batch, it makes sense to calculate raw\n",
    "    #scores on the external validation set per batch of size B too.\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],len(models)))\n",
    "    for j in range(n_loops):\n",
    "        #left and right indices:\n",
    "        inlef = B*j \n",
    "        inrig = B*(j+1)\n",
    "        new_unweighted_validation_scores[inlef:inrig,:] = Compute_Model_Scores(X_AUC[inlef:inrig,:],models)\n",
    "\n",
    "    # Since active LODA has convergence issues with the raw scores, we have to rescale the above just for\n",
    "    # active-LODA\n",
    "    new_unweighted_validation_scores_AL = (new_unweighted_validation_scores - new_unweighted_validation_scores.min(axis=0)) / (new_unweighted_validation_scores.max(axis=0) - new_unweighted_validation_scores.min(axis=0)) + 0.0000000001\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # Our trials: \n",
    "    \n",
    "    #Choose a supervised method that will be applied:\n",
    "    supervised_method = 'RandomForestClassifier'\n",
    "\n",
    "    #Initialization\n",
    "\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_old = None\n",
    "        Y_old = None\n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted.tolist()  #We directly use Y_muted, as in all the other methods\n",
    "    \n",
    "    # Run the initialization function InitActiveAGG:\n",
    "    X_lab, Y_lab, all_labeled_scores = InitActiveAGG(X_old = X_old,Y_old = Y_old,n_data_min = 100, models=models)\n",
    "\n",
    "\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    AAA_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Learn from labeled data, propose new predicted anomalies, and propose other data to label:\n",
    "        X_old, X_lab, all_labeled_scores, indices_to_expert, learned_model, supervised_indices = ActiveAGG(X_new = X_new, X_old = X_old, X_lab = X_lab, Y_lab = Y_lab, all_labeled_scores = all_labeled_scores, models=models,supervised_method = supervised_method,n_data_min = 100,n_data_max = B, min_n_labeled = 5,n_send=n_send,pc_top = 0.6,min_n_nom=5,min_n_anom=1,tau_exp=0.001)    \n",
    "        # Pretend to be the expert and add the true labels to the proposed data:\n",
    "        expert_provided_labels = [Y_new[j] for j in indices_to_expert]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_AAA[curr_trial, r] = nY1\n",
    "        \n",
    "        #Test the current learned model on the external data in order to calculate the AUC:\n",
    "        #new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:, 1]\n",
    "        if learned_model != None:\n",
    "            new_preds = learned_model.predict_proba(new_unweighted_validation_scores)[:,1]\n",
    "            AAA_AUC[r] = roc_auc_score(Y_AUC,new_preds)\n",
    "            all_AAA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,new_preds)\n",
    "        else:\n",
    "            AAA_AUC[r] = 0.5\n",
    "            all_AAA_AUC[curr_trial,r] = 0.5\n",
    "\n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "\n",
    "# Calculate column averages for each array\n",
    "avg_AAA_TS1 = np.mean(all_AAA_AUC, axis=0)\n",
    "avg_nY1_AAA_TS1 = np.mean(all_nY1_AAA, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_TS1_onlyAAA.npz\",\n",
    "         avg_AAA_TS1 = avg_AAA_TS1,\n",
    "         avg_nY1_AAA_TS1 = avg_nY1_AAA_TS1,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_TS1_onlyAAA.npz\",\n",
    "         new_preds_TS1 = new_preds\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44570b3f-a090-42d7-a196-d53ac3f22700",
   "metadata": {},
   "source": [
    "### The other three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc10d0a-0355-41bf-a098-b8b1f293bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91097dff-f9cd-41c8-8faf-9db92e118fa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_LODA_AUC = np.zeros((n_trials,n_loops))\n",
    "all_active_LODA_AUC = np.zeros((n_trials,int(n_loops/4)))\n",
    "all_GLAD_AUC = np.zeros((n_trials,n_loops))\n",
    "\n",
    "all_nY1_LODA = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "all_nY1_active_LODA = np.zeros((n_trials, int(n_loops/4)))  # Array to store nY1 values\n",
    "all_nY1_GLAD = np.zeros((n_trials, n_loops))  # Array to store nY1 values\n",
    "\n",
    "for curr_trial in range(n_trials):\n",
    "    print('We are in trial',curr_trial+1,'out of',n_trials) \n",
    "    \n",
    "    # Generate the data\n",
    "    X, Y = generate_time_series(my_tau=my_tau, d=my_d, n = n,mean_G1=mean_G1, mean_G2=mean_G2, mean_G3=mean_G3,c_anom = c_anom)\n",
    "    X_AUC, Y_AUC = generate_time_series(my_tau=my_tau, d=my_d, n = n_AUC,mean_G1=mean_G1, mean_G2=mean_G2, mean_G3=mean_G3,c_anom = c_anom)   \n",
    "    \n",
    "    # Masking the old data, if it exists:\n",
    "    if n_old == 0:\n",
    "        #print('There is no initial data.')\n",
    "        Y_muted = np.empty((0,)).astype(float)\n",
    "\n",
    "    #Need at least 2 anomalies in the old data for these simulations to function\n",
    "    if np.sum(Y[:n_old] == 1) < 2:\n",
    "        raise ValueError(\"There are not at least two true anomalies in the old data.\")\n",
    "    \n",
    "    \n",
    "    if n_old > 0:\n",
    "        n_remaining_anomalies = 0\n",
    "        n_remaining_nominals = 100\n",
    "        while (n_remaining_anomalies != 2) & (n_remaining_nominals != 98):\n",
    "        #while myplay==0:\n",
    "            n_old_anomalies = np.sum(Y[:n_old] == 1)\n",
    "            n_old_nominals = n_old - n_old_anomalies\n",
    "            #Calculate how many labels to show and how many to hide:\n",
    "            n_hide = int(np.ceil((1-u)*n_old)) # if u > 0 then at least one label will be shown due to ceiling function\n",
    "            n_hide_pc = (n_hide/n_old)*100\n",
    "            #Randomly select n_hide of the n_old data points to mask:\n",
    "            permute_indices = np.random.permutation(n_old)\n",
    "            # The n_anomalies indices in permute_indices will correspond to the anomalies:\n",
    "            hide_indices = permute_indices[0:n_hide]\n",
    "            #Fill in Y_muted:\n",
    "            Y_muted = Y[:n_old].astype(float)\n",
    "            Y_muted[hide_indices] = np.nan\n",
    "            n_remaining_anomalies = np.sum(Y_muted == 1)\n",
    "            n_remaining_nominals = np.sum(Y_muted == 0)\n",
    "            print('There were',n_old_anomalies,'anomalies and',n_old_nominals,'nominals in the initial data. After randomly masking',n_hide_pc,'% of the initial data, there remain',n_remaining_anomalies,'labeled anomalies and',n_remaining_nominals,'labeled nominals.')\n",
    "            #Get the values of X corresponding to the unmuted anomalies:\n",
    "            X_with_anomalies = []\n",
    "            for i in range(n_old):\n",
    "                if Y_muted[i] == 1:\n",
    "                    X_with_anomalies = X_with_anomalies + [X[i,:]]\n",
    "\n",
    "\n",
    "    # Define additional models to include\n",
    "    extra_models = {\n",
    "        \"IsolationForest\": IsolationForest(),\n",
    "        \"OneClassSVM\": OneClassSVM(),\n",
    "        \"EuclideanDifferenceAnomalyDetector\": EuclideanDifferenceAnomalyDetector(),\n",
    "        \"LocalOutlierFactor\": LocalOutlierFactor(novelty=True),\n",
    "        \"RandomScore\": RandomScore(),\n",
    "    }\n",
    "    \n",
    "    # Create all models (including the LODA models and additional ones)\n",
    "    models = Create_Anomaly_Models(my_d, n_LODA_models=n_LODA_models, additional_models=extra_models)\n",
    "    \n",
    "    #Calculate the unweighted scores on the massive external validation set. Here, to be more precise,\n",
    "    #since we now have score functions that are contextual per batch, it makes sense to calculate raw\n",
    "    #scores on the external validation set per batch of size B too.\n",
    "    new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],len(models)))\n",
    "    for j in range(n_loops):\n",
    "        #left and right indices:\n",
    "        inlef = B*j \n",
    "        inrig = B*(j+1)\n",
    "        new_unweighted_validation_scores[inlef:inrig,:] = Compute_Model_Scores(X_AUC[inlef:inrig,:],models)\n",
    "\n",
    "    # Since active LODA has convergence issues with the raw scores, we have to rescale the above just for\n",
    "    # active-LODA\n",
    "    new_unweighted_validation_scores_AL = (new_unweighted_validation_scores - new_unweighted_validation_scores.min(axis=0)) / (new_unweighted_validation_scores.max(axis=0) - new_unweighted_validation_scores.min(axis=0)) + 0.0000000001\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # LODA TRIALS\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    LODA_AUC = [0]*n_loops\n",
    "        \n",
    "    weighted_scores=np.mean(new_unweighted_validation_scores,axis=1)\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(n_loops):\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,len(models)))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "            \n",
    "        #Final LODA scores are averages over anomaly detectors\n",
    "        new_LODA_scores = np.mean(new_unweighted_scores,axis=1)\n",
    "\n",
    "        #############################################################\n",
    "        # Sort these scores:\n",
    "        top_k = n_send\n",
    "        sorted_indices = np.argsort(new_LODA_scores)\n",
    "        sorted_scores = new_LODA_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_LODA[curr_trial,r] = nY1\n",
    "        #############################################################\n",
    "\n",
    "        \n",
    "        #############################################################\n",
    "        LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        all_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    #####################################################################################################################\n",
    "    # ACTIVE-LODA trials:\n",
    "    \n",
    "    #We do however have to provide C_tau since it needs to be used before the optimization\n",
    "    #function. C_tau = 0.03 is the default suggested in Das et al. (2016).\n",
    "    C_tau = tau\n",
    "    \n",
    "    #There are hyperparameters that need to be set in advance for this algorithm. However, \n",
    "    #for simplicity we assume they tal the default values in the function optimize_w.\n",
    "    #C_A = 100  #default in their article\n",
    "    #C_eta = 1000. #default in their article   \n",
    "    \n",
    "    #In active LODA, the whole budget of n_send items per loop is dedicated to sending the \n",
    "    #items with highest predicted scores:\n",
    "    top_k = n_send\n",
    "    \n",
    "    #So now that we have the LODA projectors (i.e., a set of best_m anomaly detectors),\n",
    "    #we can begin.\n",
    "    \n",
    "    #We shall initialize the vector of weights as being equal and summing to 1:\n",
    "    w_old = np.array([1/len(models) for i in range(len(models))])\n",
    "    \n",
    "    #We also initialize arrays to put the unweighted scores of labeled data into:\n",
    "    H_A = np.empty((0, len(models)))\n",
    "    H_N = np.empty((0, len(models)))\n",
    "    \n",
    "    #We also initialize a fake anomaly alert to 0 (see below). This means basically that\n",
    "    #we have not so far had to add a \"fake anomaly\" to the optimization due to there only\n",
    "    #being labeled nominals so far.\n",
    "    fake_anomaly = 0\n",
    "    \n",
    "    #Unlike basic LODA, here anomaly and nominal labels MATTER. In particular, at the\n",
    "    #beginning, it matters whether there is initial \"old\" data, and if so, whether some\n",
    "    #or all of it is already labeled. If n_old > 0, then we have already calculated Y_muted\n",
    "    #earlier in this script (for some fixed percentage u of this \"old\" data for which we\n",
    "    #suppose we know its true label)\n",
    "    \n",
    "    #If there were initial data and at least one labeled nominal (following Das et al. (2016))\n",
    "    if n_old > 0 and np.sum(Y_muted == 0) > 0:\n",
    "            \n",
    "        #Extract the initial data from X:\n",
    "        X_new = X[:n_old,:]\n",
    "    \n",
    "        #Calculate the unweighted scores for each LODA projector on the initial data: \n",
    "        new_unweighted_scores = np.empty((n_old,len(models)))\n",
    "    \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "    \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "\n",
    "        # Active LODA has convergence problems as soon as we are no longer using LODA projections which are \n",
    "        # similarly scaled. Thus, here, we rescale scores to [0,1]\n",
    "        new_unweighted_scores = (new_unweighted_scores - new_unweighted_scores.min(axis=0)) / (new_unweighted_scores.max(axis=0) - new_unweighted_scores.min(axis=0)) + 0.0000000001\n",
    "    \n",
    "        #Calculate the sum of the linear combination of these scores weighted by w_old:\n",
    "        new_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "    \n",
    "        #Sort new_scores from smallest to largest, whilst retaining the indices.\n",
    "        sorted_indices = np.argsort(new_scores)\n",
    "        sorted_scores = new_scores[sorted_indices]\n",
    "    \n",
    "        #Calculate q_tau on this initial data: \n",
    "        #WARNING: one of the underlying problems with active-LODA is that it basically expects\n",
    "        #anomalies to have the highest scores from the get go. But here, in this first loop,\n",
    "        #it very well could be that the anomalies have all the LOWEST scores. The calculation\n",
    "        #of q_tau at this point is therefore a bit weird/pointless. Also, active-LODA basically\n",
    "        #expects positive weights, especially in its minimization step, with its L2 norm penalty\n",
    "        #on the weights. This means that active-LODA does not reach its true potential, as\n",
    "        #defined and coded by Das et al. (2016). The trouble is is that without a penalty on \n",
    "        #making the weights not too big, not too small, and not necessarily positive, it will\n",
    "        #remain suboptimal. \n",
    "        my_quantile_sorted_index = int(np.floor(n_old*(1-C_tau)))-1*(C_tau != 1)\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "    \n",
    "        #Create the arrays H_A and H_N:\n",
    "        #First we take H_A. In Das et al. (2016) they allow for the case that there\n",
    "        #are no labeled anomalies, only labeled nominals.\n",
    "        if np.sum(Y_muted == 1) == 0:\n",
    "            #Set a \"fake anomaly alert to 1\":\n",
    "            fake_anomaly = 1\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "                    \n",
    "        else:\n",
    "            #There is at least one labeled anomaly:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            H_N = new_unweighted_scores[(Y_muted==0),:]\n",
    "           \n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "            \n",
    "            temp_unweighted = np.concatenate([H_A,H_N])\n",
    "            temp_pred = np.matmul(temp_unweighted,w_new)\n",
    "            temp_YA = [1]*np.shape(H_A)[0]\n",
    "            temp_YN = [0]*np.shape(H_N)[0]\n",
    "            temp_Y = temp_YA + temp_YN\n",
    "            \n",
    "        #We can now update w_old with the value of w_new. If this whole big loop wasn't run,\n",
    "        #then w_old will stay at its original value.\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "    \n",
    "    #Else if there were initial data but no labeled nominals: Das et al. (2016)\n",
    "    #provide no details for what to do here. It is possible though unlikely that\n",
    "    #in the initial data, there were no labeled nominals but there was at least\n",
    "    #one labeled anomaly. We do have to check this, since we will need to initialize\n",
    "    #H_A in this case, even if H_N is empty.\n",
    "    elif n_old > 0 and np.sum(Y_muted == 0) == 0:\n",
    "        if np.sum(Y_muted == 1) > 0:\n",
    "            #initialize H_A:\n",
    "            H_A = new_unweighted_scores[(Y_muted==1),:]\n",
    "            #Potential Python issue whereby if there is just one anomaly, weird\n",
    "            #things happen with array shapes. To guard against this:\n",
    "            if np.sum(Y_muted == 1) == 1:\n",
    "                if np.shape(H_A) != (1,len(models)):\n",
    "                    H_A = H_A.reshape(1,-1)\n",
    "                \n",
    "    #So, at this point, either we had no old data and w_old is still equal weights, \n",
    "    #with H_A, H_N empty arrays, or else there was old data, and H_A and H_N may \n",
    "    #have been added to (or not), while w_old may or may not have already been \n",
    "    #updated.\n",
    "    \n",
    "    #We next move to the batch data.\n",
    "    \n",
    "    curr_L_index = n_old\n",
    "    active_LODA_AUC = [0]*n_loops\n",
    "\n",
    "    which_lab = [i for i in range(len(Y_muted)) if not np.isnan(Y_muted[i])]\n",
    "    Y_lab = [Y_muted[j] for j in which_lab]\n",
    "    \n",
    "    for r in range(int(n_loops/4)):\n",
    "        print('Current loop is',r,'of',int(n_loops/4))\n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        X_new = X[curr_L_index:curr_R_index ,:]\n",
    "        Y_new = Y[curr_L_index:curr_R_index]\n",
    "        \n",
    "        n_new_true_anom = sum(Y_new==1)\n",
    "        \n",
    "        #Update for the next loop:\n",
    "        curr_L_index = curr_L_index + B\n",
    "        \n",
    "        new_unweighted_scores = np.empty((B,len(models)))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            \n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            new_unweighted_scores[:,i] = y_score.squeeze()\n",
    "\n",
    "        new_unweighted_scores = (new_unweighted_scores - new_unweighted_scores.min(axis=0)) / (new_unweighted_scores.max(axis=0) - new_unweighted_scores.min(axis=0)) + 0.0000000001\n",
    "            \n",
    "        #Temporary final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        temp_new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        #Calculated the weighted scores on the external validation set with the current\n",
    "        #value of w_old:\n",
    "        weighted_validation_scores = np.matmul(new_unweighted_validation_scores_AL,w_old)\n",
    "        active_LODA_AUC[r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        all_active_LODA_AUC[curr_trial,r] = roc_auc_score(Y_AUC,weighted_validation_scores)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,weighted_validation_scores)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        #We actually have to, at this point, attach the current versions of H_A\n",
    "        #and H_N to new_unweighted_scores, since in this batch framework, we do\n",
    "        #not have a fixed number of data points from the start to the finish, like\n",
    "        #they do in Das et al. (2016). If we do not do this, it will affect the\n",
    "        #calculation of q_tau over time (a kind of bias will be introduced, maybe\n",
    "        #not the end of the world, but still.)\n",
    "        \n",
    "        new_unweighted_scores = np.concatenate([new_unweighted_scores,H_A,H_N])\n",
    "        \n",
    "        #Actual final active-LODA scores are a linear combination over anomaly detectors:\n",
    "        new_active_LODA_scores = np.matmul(new_unweighted_scores,w_old)\n",
    "        \n",
    "        temp_Y = Y_new.tolist() + [1]*np.shape(H_A)[0] + [0]*np.shape(H_N)[0]\n",
    "        \n",
    "        #Following the methodology in Das et al. (2016), we should provide the \n",
    "        #highest scoring data-point to an expert for labeling. In order to be\n",
    "        #slightly more general, we shall instead provide the top_k scoring data\n",
    "        #points to the expert, where top_k has been pre-defined.\n",
    "        \n",
    "        #Sort new_active_LODA_scores from smallest to largest, whilst retaining the indices.\n",
    "        #Remember that these may include items from the previous loop or from the initialization.\n",
    "        #However, since we appended those on to the end, we know their indices will be B, B+1,...\n",
    "        #so we will be able to look out for them.\n",
    "        sorted_indices = np.argsort(new_active_LODA_scores)\n",
    "        sorted_scores = new_active_LODA_scores[sorted_indices]\n",
    "    \n",
    "        my_quantile_sorted_index = int(np.floor(len(new_active_LODA_scores)*(1-C_tau)))-1*(C_tau != 1)\n",
    "        \n",
    "        #We now have to go through the sorted_indices from the end back towards the \n",
    "        #beginning until we manage to gather top_k indices which are less than or equal to B-1.\n",
    "        top_k_indices = []\n",
    "        n_indices_so_far = 0\n",
    "        curr_index = len(sorted_indices)\n",
    "        while n_indices_so_far < top_k:\n",
    "            curr_index = curr_index - 1\n",
    "            next_top_index = sorted_indices[curr_index]\n",
    "            #print('next top index:',next_top_index)\n",
    "            if next_top_index < B:\n",
    "                top_k_indices.append(next_top_index)\n",
    "                n_indices_so_far = n_indices_so_far + 1\n",
    "                \n",
    "        #Now we go and get the labels in Y_new associated with these indices:\n",
    "        #top_k_indices = np.array(top_k_indices)\n",
    "        Y_expert = Y_new[top_k_indices]\n",
    "\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "        nY1 = Y_lab.count(1)\n",
    "        all_nY1_active_LODA[curr_trial,r] = nY1\n",
    "        \n",
    "        #Now is a good time to check whether we just found at least one real anomaly while\n",
    "        #up to now we only had one fake anomaly:\n",
    "        if sum(Y_expert==1) > 0 and fake_anomaly==1:\n",
    "            #We now reset H_A to be empty:\n",
    "            H_A = np.empty((0, len(models)))\n",
    "            #And we set fake_anomaly to 0 forever:\n",
    "            fake_anomaly = 0\n",
    "            \n",
    "        if sum(Y_expert==1) == 0 and fake_anomaly==1:\n",
    "            #We reset H_A back to empty again:\n",
    "            H_A = np.empty((0, len(models)))\n",
    "                \n",
    "        #We then need to append the relevant unweighted scores to the current H_A and H_N\n",
    "        for j in range(top_k):\n",
    "            if Y_expert[j]==1:\n",
    "                H_A = np.concatenate([H_A,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "            else:\n",
    "                H_N = np.concatenate([H_N,new_unweighted_scores[top_k_indices[j]:(top_k_indices[j]+1),:]])\n",
    "                \n",
    "        #Here we have to check whether this was the first time H_A actually contained\n",
    "        #a real anomaly or not, rather than a temporary one (if there were no labeled anomalies)    \n",
    "        \n",
    "        #Now since top_k > 0 we know that there is a positive number of labeled data for sure.\n",
    "        #We always have to retain however the possibility that all of the labeled data so far\n",
    "        #are nominals. \n",
    "        if np.shape(H_A)[0]==0:\n",
    "            #We take the data point corresponding to the score sorted into the \n",
    "            #np.floor(n_old*(1-C_tau))-th position as a proxy for an anomaly.\n",
    "            H_A = new_unweighted_scores[sorted_indices[my_quantile_sorted_index],:].reshape(1,-1)\n",
    "            #Update the linear combination coefficients:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "        else:\n",
    "            w_new = optimize_w_2(H_A, H_N, q_tau)\n",
    "    \n",
    "                \n",
    "    \n",
    "        #Update q_tau on this data:\n",
    "        q_tau = sorted_scores[my_quantile_sorted_index]\n",
    "        \n",
    "        #update w_old\n",
    "        w_old = w_new\n",
    "        w_old = w_old / np.linalg.norm(w_old)\n",
    "        temp_unweighted = np.concatenate([H_A,H_N])\n",
    "        temp_pred = np.matmul(temp_unweighted,w_old)\n",
    "        temp_YA = [1]*np.shape(H_A)[0]\n",
    "        temp_YN = [0]*np.shape(H_N)[0]\n",
    "        temp_Y = temp_YA + temp_YN\n",
    "\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # GLAD trials:\n",
    "    \n",
    "    #Initialize some parameters:\n",
    "    b = 0.5\n",
    "    mylambda = 1\n",
    "    top_k = n_send\n",
    "    q_tau_tm1 = -10e7\n",
    "    \n",
    "    # Build the model:\n",
    "    model_GLAD = build_neural_network(np.shape(X)[1], len(models))\n",
    "    \n",
    "    # Compile the model with the custom loss function:\n",
    "    model_GLAD.compile(optimizer='adam', loss=custom_binary_crossentropy_loss(b=b,mylambda=mylambda), metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    # We use the same n_min used earlier to find the number of LODA projections:\n",
    "    y_true = np.full((n_min, len(models)), b)\n",
    "    \n",
    "    model_GLAD.fit(X[:n_min,:], y_true, epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "    # Sanity check: the output for each data point should all be very close to b:\n",
    "    row = X[1:2] \n",
    "    output = model_GLAD.predict(row)\n",
    "    \n",
    "    #INITIALIZATION\n",
    "    #Dealing with edge cases:\n",
    "    if n_old == 0:\n",
    "        X_lab = np.empty([0,np.shape(X)[1]])\n",
    "        Y_lab = []\n",
    "        all_labeled_scores = np.empty([0,len(models)])\n",
    "        all_unweighted_scores = np.empty([0,len(models)])\n",
    "    \n",
    "    else:\n",
    "        X_old = X[:n_old,:]\n",
    "        Y_old = Y_muted  #We directly use Y_muted, as in all the other methods\n",
    "        which_lab = [i for i in range(len(Y_old)) if not np.isnan(Y_old[i])]\n",
    "    \n",
    "        if len(which_lab) == 0:\n",
    "            X_lab = np.empty([0,np.shape(X)[1]])\n",
    "            Y_lab = [] \n",
    "            all_unweighted_scores = np.empty([0,len(models)])\n",
    "            all_labeled_scores = np.empty([0,len(models)])\n",
    "        else:\n",
    "            X_lab = X_old[which_lab,:]\n",
    "            Y_lab = [Y_old[j] for j in which_lab]\n",
    "            all_unweighted_scores = np.empty([np.shape(X_old)[0],len(models)])\n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                model.fit(X_old)\n",
    "                y_score = model.score_samples(X_old)\n",
    "                y_score.dtype = np.float64\n",
    "                all_unweighted_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "            all_labeled_scores = all_unweighted_scores[which_lab,:]\n",
    "    \n",
    "    #LOOPING OVER BATCHES:\n",
    "    #Deal with indices of X:\n",
    "    curr_L_index = n_old\n",
    "    \n",
    "    #Initialize storage of AUC scores\n",
    "    GLAD_AUC = [0]*n_loops\n",
    "    \n",
    "    #Loop through batches of new data of size B:\n",
    "    for r in range(n_loops): \n",
    "        \n",
    "        curr_R_index = curr_L_index + B\n",
    "        \n",
    "        #Get next batch of data:\n",
    "        X_new = X[curr_L_index:curr_R_index,:] \n",
    "        Y_new = Y[curr_L_index:curr_R_index].tolist()\n",
    "        \n",
    "        nYnew1 = Y_new.count(1)\n",
    "    \n",
    "        # Calculate all of the scores for X_new:\n",
    "        all_scores = np.empty([np.shape(X_new)[0],len(models)])\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            model.fit(X_new)\n",
    "            y_score = model.score_samples(X_new)\n",
    "            y_score.dtype = np.float64\n",
    "            all_scores[:,i] = y_score.squeeze()\n",
    "    \n",
    "        #Update:\n",
    "        all_unweighted_scores = np.concatenate([all_unweighted_scores,all_scores])\n",
    "    \n",
    "        # Pass X_new through the current state of the neural network in order to get the weights out:\n",
    "        curr_w = model_GLAD.predict(X_new)\n",
    "\n",
    "        X_new_final_scores = np.sum(all_scores * curr_w, axis=1)\n",
    "    \n",
    "        # Sort these scores:\n",
    "        sorted_indices = np.argsort(X_new_final_scores)\n",
    "        sorted_scores = X_new_final_scores[sorted_indices]\n",
    "        top_k_indices = sorted_indices[(B-top_k):]\n",
    "        # Add the scores of the top_k data points to all_labeled_scores:\n",
    "        all_labeled_scores = np.concatenate([all_labeled_scores,all_scores[top_k_indices,:]])\n",
    "    \n",
    "        # Add the relevant data points to X_lab\n",
    "        X_lab = np.concatenate([X_lab,X_new[top_k_indices,:]])\n",
    "    \n",
    "        # Get the true labels of these data points and add them to Y_lab\n",
    "        expert_provided_labels = [Y_new[j] for j in top_k_indices]\n",
    "        Y_lab = Y_lab + expert_provided_labels\n",
    "    \n",
    "        #Update X_so_far (all X data so far):\n",
    "        X_so_far = X[:(n_old + (r+1)*B),:]\n",
    "        \n",
    "        nY0 = Y_lab.count(0)\n",
    "        nY1 = Y_lab.count(1)\n",
    "        print('There are ',nY0,' labeled non-anomalies and ',nY1,' labeled anomalies so far.')\n",
    "\n",
    "        all_nY1_GLAD[curr_trial, r] = nY1\n",
    "    \n",
    "        curr_w_ext = model_GLAD.predict(X_AUC)\n",
    "        X_new_final_scores_ext = np.zeros((np.shape(X_AUC)[0],))\n",
    "        for k in range(np.shape(X_AUC)[0]):\n",
    "            X_new_final_scores_ext[k] = np.matmul(new_unweighted_validation_scores[k,:],np.transpose(curr_w_ext[k,:]))\n",
    "        \n",
    "        GLAD_AUC[r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "        all_GLAD_AUC[curr_trial,r] = roc_auc_score(Y_AUC,X_new_final_scores_ext)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_AUC,X_new_final_scores_ext)\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        #Check if this is the first time through:\n",
    "        if q_tau_tm1 == -10e7:\n",
    "            #Dealing with q_tau_tm1\n",
    "            all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "            all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "            for k in range(np.shape(X_so_far)[0]):\n",
    "                all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "            # Calculate the quantile index without fully sorting\n",
    "            quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "            partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "            q_tau_tm1 = partitioned_array[quantile_index]    \n",
    "    \n",
    "        # Next, compile the model with the second custom loss function:\n",
    "        # 1. Identify the indices of anomalies (Y_lab == 1)\n",
    "        anomaly_indices = np.where(np.array(Y_lab) == 1)[0]\n",
    "        \n",
    "        # 2. Repeat the corresponding rows in X_lab and all_labeled_scores (4 copies for each anomaly)\n",
    "        X_lab_temp = np.vstack([X_lab] + [X_lab[anomaly_indices]] * 4)  # Repeat rows of X_lab for anomalies\n",
    "        all_labeled_scores_temp = np.vstack([all_labeled_scores] + [all_labeled_scores[anomaly_indices]] * 4)  # Repeat rows of all_labeled_scores for anomalies\n",
    "        X_so_far_temp = np.vstack([X_so_far] + [X_lab[anomaly_indices]] * 4)\n",
    "\n",
    "        # 3. Extend Y_lab_temp with four 1s for each anomaly (4 copies for each anomaly)\n",
    "        Y_lab_temp = np.concatenate([Y_lab, [1] * 4 * len(anomaly_indices)])\n",
    "        \n",
    "        model_GLAD.compile(optimizer='adam', loss=lambda y_true, y_pred: new_custom_loss_2(X_lab_temp, Y_lab_temp, q_tau_tm1, all_labeled_scores_temp,model_GLAD, X_so_far_temp, mylambda, b), metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model for further iterations (e.g., 10 epochs)\n",
    "        model_GLAD.fit(X_lab_temp, np.array(Y_lab_temp), epochs=10, batch_size=32,verbose=0)\n",
    "    \n",
    "        #Dealing with q_tau_tm1\n",
    "        all_weights_so_far = model_GLAD.predict(X_so_far)\n",
    "        all_final_scores_so_far = np.zeros((np.shape(X_so_far)[0],))\n",
    "        for k in range(np.shape(X_so_far)[0]):\n",
    "            all_final_scores_so_far[k] = np.matmul(all_unweighted_scores[k,:],np.transpose(all_weights_so_far[k,:]))\n",
    "\n",
    "        # Calculate the quantile index without fully sorting\n",
    "        quantile_index = int(np.floor(len(all_final_scores_so_far) * (1 - C_tau))) - 1 * (C_tau != 1)\n",
    "        partitioned_array = np.partition(all_final_scores_so_far, quantile_index)\n",
    "        q_tau_tm1 = partitioned_array[quantile_index]\n",
    "            \n",
    "        #Update indices:\n",
    "        curr_L_index = curr_L_index + B\n",
    "\n",
    "# Calculate column averages for each array\n",
    "avg_LODA_TS1 = np.mean(all_LODA_AUC, axis=0)\n",
    "avg_active_LODA_TS1 = np.mean(all_active_LODA_AUC, axis=0)\n",
    "avg_GLAD_TS1 = np.mean(all_GLAD_AUC, axis=0)\n",
    "\n",
    "avg_nY1_LODA_TS1 = np.mean(all_nY1_LODA, axis=0)\n",
    "avg_nY1_active_LODA_TS1 = np.mean(all_nY1_active_LODA, axis=0)\n",
    "avg_nY1_GLAD_TS1 = np.mean(all_nY1_GLAD, axis=0)\n",
    "\n",
    "#Save these results:\n",
    "np.savez(\"AUC_TS1.npz\",\n",
    "         avg_LODA_TS1 = avg_LODA_TS1,\n",
    "         avg_active_LODA_TS1 = avg_active_LODA_TS1,\n",
    "         avg_GLAD_TS1 = avg_GLAD_TS1,\n",
    "         avg_nY1_LODA_TS1 = avg_nY1_LODA_TS1,\n",
    "         avg_nY1_active_LODA_TS1 = avg_nY1_active_LODA_TS1,\n",
    "         avg_nY1_GLAD_TS1 = avg_nY1_GLAD_TS1,\n",
    "        )\n",
    "\n",
    "# Save also the score matrices for later (for the plot below)\n",
    "np.savez(\"scores_TS1.npz\",\n",
    "         weighted_scores_TS1 = weighted_scores,\n",
    "         weighted_validation_scores_TS1 = weighted_validation_scores,\n",
    "         X_new_final_scores_ext_TS1 = X_new_final_scores_ext,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667501e-8b91-4027-a6d0-50929947017d",
   "metadata": {},
   "source": [
    "## Load all data and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a8039-3dae-431d-a72e-516b41abe0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_TS1_onlyAAA = np.load(\"AUC_TS1_onlyAAA.npz\")\n",
    "avg_AAA_TS1 = data_TS1_onlyAAA[\"avg_AAA_TS1\"]\n",
    "avg_nY1_AAA_TS1 = data_TS1_onlyAAA[\"avg_nY1_AAA_TS1\"]\n",
    "\n",
    "data_TS1 = np.load(\"AUC_TS1.npz\")\n",
    "avg_LODA_TS1 = data_TS1[\"avg_LODA_TS1\"]\n",
    "avg_active_LODA_TS1 = data_TS1[\"avg_active_LODA_TS1\"]\n",
    "avg_GLAD_TS1 = data_TS1[\"avg_GLAD_TS1\"]\n",
    "avg_nY1_LODA_TS1 = data_TS1[\"avg_nY1_LODA_TS1\"]\n",
    "avg_nY1_active_LODA_TS1 = data_TS1[\"avg_nY1_active_LODA_TS1\"]\n",
    "avg_nY1_GLAD_TS1 = data_TS1[\"avg_nY1_GLAD_TS1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfffbe1-de0f-4bb9-975a-d155eae68370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating one of the 10 dimensions of the data for visualization purposes. We simply take the first \n",
    "#X_AUC generated in the two code blocks just above (it's the same) and we'll plot just the first dimension, \n",
    "#and in colors for #nominals and anomalies. And we will also plot the anomaly distribution(s)\n",
    "\n",
    "#Generate data and scores \n",
    "np.random.seed(123456789)\n",
    "\n",
    "# Generate the data\n",
    "X, Y = generate_time_series(my_tau=my_tau, d=my_d, n = n,mean_G1=mean_G1, mean_G2=mean_G2, mean_G3=mean_G3,c_anom = c_anom)\n",
    "X_AUC, Y_AUC = generate_time_series(my_tau=my_tau, d=my_d, n = n_AUC,mean_G1=mean_G1, mean_G2=mean_G2, mean_G3=mean_G3,c_anom = c_anom)   \n",
    "# Define additional models to include\n",
    "extra_models = {\n",
    "    \"IsolationForest\": IsolationForest(),\n",
    "    \"OneClassSVM\": OneClassSVM(),\n",
    "    \"EuclideanDifferenceAnomalyDetector\": EuclideanDifferenceAnomalyDetector(),\n",
    "    \"LocalOutlierFactor\": LocalOutlierFactor(novelty=True),\n",
    "    \"RandomScore\": RandomScore(),\n",
    "}\n",
    "# Create all models (including the LODA models and additional ones)\n",
    "models = Create_Anomaly_Models(my_d, n_LODA_models=n_LODA_models, additional_models=extra_models)\n",
    "#Calculate the unweighted scores on the massive external validation set. Here, to be more precise,\n",
    "#since we now have score functions that are contextual per batch, it makes sense to calculate raw\n",
    "#scores on the external validation set per batch of size B too.\n",
    "new_unweighted_validation_scores = np.empty((np.shape(X_AUC)[0],len(models)))\n",
    "for j in range(n_loops):\n",
    "    #left and right indices:\n",
    "    inlef = B*j \n",
    "    inrig = B*(j+1)\n",
    "    new_unweighted_validation_scores[inlef:inrig,:] = Compute_Model_Scores(X_AUC[inlef:inrig,:],models)\n",
    "\n",
    "\n",
    "xx = list(range(1, n_loops+1))\n",
    "xxactiveLODAv2 = list(range(1, int(n_loops/4)+1))\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(8, 16))  # 3 rows, 1 column\n",
    "\n",
    "# First subplot: Scatter plot of the first column of X_AUC vs Y_AUC\n",
    "scatter = axs[0].scatter(range(500), X_AUC[0:500, 0], \n",
    "                         c=['blue' if y == 0 else 'red' for y in Y_AUC[0:500]], s=4, marker='o')\n",
    "\n",
    "# Set axis labels and title\n",
    "axs[0].set_xlabel(\"Time\",fontsize=15)\n",
    "axs[0].set_ylabel(\"Data value\",fontsize=15)\n",
    "axs[0].set_title(\"First dimension of time series\",fontsize=17)\n",
    "\n",
    "# Create custom legend entries (without lines, just markers)\n",
    "blue_marker = mlines.Line2D([], [], color='blue', marker='o', markersize=5, label=\"Nominal (Y=0)\", linestyle='None')\n",
    "red_marker = mlines.Line2D([], [], color='red', marker='o', markersize=5, label=\"Anomaly (Y=1)\", linestyle='None')\n",
    "\n",
    "# Add the custom legend (only marker, no lines)\n",
    "axs[0].legend(handles=[blue_marker, red_marker], loc=\"best\",fontsize=13)\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "# Second subplot: KDE Plot for the 8th column of new_unweighted_validation_scores\n",
    "diffs = new_unweighted_validation_scores[:, 7]  # 8th column (index 7)\n",
    "\n",
    "# Step 2: Split into nominal and anomaly based on Y_AUC labels\n",
    "nominal_diffs = diffs[Y_AUC == 0]\n",
    "anomaly_diffs = diffs[Y_AUC == 1]\n",
    "\n",
    "# Define x-axis range for the plot\n",
    "x_min, x_max = np.min(diffs), np.max(diffs)\n",
    "x_vals = np.linspace(x_min, x_max, 500)\n",
    "\n",
    "# KDE for nominal (Y=0)\n",
    "if len(nominal_diffs) > 1:\n",
    "    kde_nominal = gaussian_kde(nominal_diffs)\n",
    "    nominal_density = kde_nominal(x_vals) * (1 - my_tau)  # Adjust tau as needed\n",
    "    axs[1].plot(x_vals, nominal_density, label=\"Nominal (Y=0)\", color=\"blue\")\n",
    "\n",
    "# KDE for anomaly (Y=1)\n",
    "if len(anomaly_diffs) > 1:\n",
    "    kde_anomaly = gaussian_kde(anomaly_diffs)\n",
    "    anomaly_density = kde_anomaly(x_vals) * my_tau  # Adjust tau as needed\n",
    "    axs[1].plot(x_vals, anomaly_density, label=\"Anomaly (Y=1)\", color=\"red\")\n",
    "\n",
    "# Formatting for the second plot\n",
    "axs[1].set_title(\" Weighted 'score density' function for the first anomaly detector\",fontsize=17)\n",
    "axs[1].set_xlabel(\"Absolute difference anomaly score\",fontsize=15)\n",
    "axs[1].set_ylabel(\"Density\",fontsize=15)\n",
    "axs[1].legend(fontsize=13)\n",
    "axs[1].grid(True)\n",
    "axs[1].set_ylim(0, 0.5)  # Set y-axis limits for consistency\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "# Third subplot: Plot column averages from avg_LODA_TS1, avg_active_LODA_TS1, avg_GLAD_TS1, avg_AAA_TS1\n",
    "axs[2].plot(xx, avg_LODA_TS1, label=\"LODA\", linewidth=1.5)\n",
    "axs[2].plot(xxactiveLODAv2, avg_active_LODA_TS1, label=\"Active-LODA\", linewidth=1.5)\n",
    "axs[2].plot(xx, avg_GLAD_TS1, label=\"GLAD\",  linewidth=1.5)\n",
    "axs[2].plot(xx, avg_AAA_TS1, label=\"AAA\", linewidth=1.5)\n",
    "\n",
    "# Add plot title and labels for the third subplot\n",
    "axs[2].set_xlabel(\"Batch\",fontsize=15)\n",
    "axs[2].set_ylabel(\"Average AUC\",fontsize=15)\n",
    "axs[2].set_title(\"Average AUC over time\",fontsize=17)\n",
    "\n",
    "# Customize x-ticks to show at intervals of 5\n",
    "#axs[2].set_xticks(range(5, n_loops + 1, 5))\n",
    "\n",
    "# Add legend to the third subplot\n",
    "axs[2].legend(loc=\"best\", fontsize=13)\n",
    "\n",
    "# Add grid for better readability in third subplot\n",
    "axs[2].grid(True)\n",
    "\n",
    "#############################\n",
    "# Fourth subplot (Cumulative anomalies detected)\n",
    "axs[3].plot(xx, avg_nY1_LODA_TS1, label=\"LODA\", linewidth=1.5)\n",
    "axs[3].plot(xxactiveLODAv2, avg_nY1_active_LODA_TS1, label=\"Active-LODA\", linewidth=1.5)\n",
    "axs[3].plot(xx, avg_nY1_GLAD_TS1, label=\"GLAD\",linewidth=1.5)\n",
    "axs[3].plot(xx, avg_nY1_AAA_TS1, label=\"AAA\", linewidth=1.5)\n",
    "\n",
    "axs[3].set_xlabel(\"Batch\",fontsize=15)\n",
    "axs[3].set_ylabel(\"Anomalies detected\",fontsize=15)\n",
    "axs[3].set_title(\"Cumulative anomalies detected\",fontsize=17)\n",
    "axs[3].legend(fontsize=13)\n",
    "axs[3].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure to PDF\n",
    "fig.savefig(\"ContextualScores2.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Show all plots at once\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "187px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
